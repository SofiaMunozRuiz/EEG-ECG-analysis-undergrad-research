# -*- coding: utf-8 -*-
"""BandasFrecuencia2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17BQ_OqSVa2BnafIJcAWKfmAtKoFLo1yV

# Importar librerias
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
!pip install vitaldb
import vitaldb
import seaborn as sns
import ast
!pip install fcbf
!pip install sklearn_relief
!pip install scikeras
!pip install scikit-feature
!pip install skfeature-chappers

"""#Cargar dataframe

##Cargar final
"""

#df_caract = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TrabajoGrado/Code_new/SerieTemporales/Dataframes/df_180_filt.csv')
df_caract =pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TrabajoGrado/Code_new/SerieTemporales/Dataframes2/df_caract_652.csv')
df_caract

df_limpio =pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TrabajoGrado/Code_new/SerieTemporales/Dataframes2/df_eeg1_clean.csv')

plt.figure(figsize=(5, 4))
sns.boxplot(y=df_limpio['eeg1_var'])

# Añadir título y etiquetas

plt.ylabel('Valores de varianza ((mV$)^2$)')
plt.xlabel('Varianza')
# Mostrar el gráfico
plt.show()

plt.figure(figsize=(5, 4))
sns.boxplot(y=df_caract['eeg1_var'])

# Añadir título y etiquetas
plt.ylabel('Valores de varianza ((mV$)^2$)')
plt.xlabel('Varianza')

# Mostrar el gráfico
plt.show()

df_caract['caseid'].unique()

exclude_ids =

# Excluir los IDs de caseids usando comprehension de listas
caseids = [caseid for caseid in caseids if caseid not in exclude_ids]

"""#Caracteristicas"""

import matplotlib.pyplot as plt
from collections import Counter

# La lista de características

caracteristicas = [
                    'eeg1_higu_10','eeg1_gamma_pfd','eeg1_alpha_tasa','eeg1_lenta_higu_100','eeg1_gamma_tasa',
                    'Pabs_eeg1_delta','Prel_eeg1_lenta','eeg1_spect_ent_f','eeg1_max','eeg1_delta_std',
                    'eeg1_beta_rms','eeg1_higu_100','eeg1_var','eeg1_beta_tasa','eeg1_beta_entropy_Samp',
                    'eeg1_higu_50','eeg1_beta_higu_10','eeg1_beta_mobility','eeg1_alpha_pfd','eeg1_delta_var',
                    'eeg1_gamma_h','eeg1_lenta_std','eeg1_lenta_rms','eeg1_pfd','eeg1_pow','Pabs_eeg1_lenta',
                    'eeg1_entropy_Samp','eeg1_svd_ent','Prel_eeg1_delta','eeg1_complexity',
                    'eeg1_tasa','eeg1_alpha_higu_10','eeg1_mobility','Pabs_eeg1_beta','eeg1_h','eeg1_lenta_pow',
                    'eeg1_lenta_var','eeg1_alpha_svd_ent','eeg1_katz','eeg1_beta_higu_50',
                    'Pabs_eeg1_theta','eeg1_beta_pfd','Prel_eeg1_beta','eeg1_theta_rms','Prel_eeg1_alpha',
                    'eeg1_std','eeg1_delta_rms','eeg1_lenta_max','eeg1_gamma_higu_10','Pabs_eeg1_gamma',
                    'Prel_eeg1_gamma','eeg1_gamma_rms','eeg1_beta_h','eeg1_alpha_higu_50','eeg1_gamma_spect_ent_f',
                    'eeg1_entropy_ApEn','esp_rms_eeg1_lenta','eeg1_lenta_higu_50','eeg1_gamma_mobility','eeg1_beta_std',
                    'eeg1_curt','eeg1_activity','Pabs_eeg1_alpha','eeg1_delta_pow','eeg1_beta_svd_ent','Prel_eeg1_theta',
                    'eeg1_gamma_curt','eeg1_alpha_mobility','eeg1_delta_activity','eeg1_gamma_var',
                    'eeg1_beta_entropy_ApEn','eeg1_beta_complexity','eeg1_alpha_entropy_Samp','eeg1_lenta_activity',
                    'eeg1_theta_std','eeg1_gamma_activity','eeg1_beta_var','eeg1_beta_curt','eeg1_alpha_complexity','eeg1_beta_higu_100',
                    'eeg1_gamma_std','eeg1_theta_pow','eeg1_gamma_svd_ent','eeg1_gamma_pow','eeg1_beta_activity',
                    'eeg1_beta_pow','eeg1_gamma_complexity','eeg1_alpha_rms','eeg1_lenta_pfd','eeg1_alpha_higu_100',
                    'esp_rms_eeg1_gamma','esp_rms_eeg1_delta','eeg1_theta_activity','esp_rms_eeg1_theta',
                    'eeg1_theta_var','esp_rms_eeg1_alpha','esp_rms_eeg1_beta','eeg1_delta_max','eeg1_alpha_pow','eeg1_alpha_activity'
]

# Contar cuántas veces aparece cada palabra clave
contador_palabras_clave = Counter()
for caracteristica in caracteristicas:
    if "theta" in caracteristica:
        contador_palabras_clave["theta"] += 1
    if "delta" in caracteristica:
        contador_palabras_clave["delta"] += 1
    if "alpha" in caracteristica:
        contador_palabras_clave["alpha"] += 1
    if "gamma" in caracteristica:
        contador_palabras_clave["gamma"] += 1
    if "lenta" in caracteristica:
        contador_palabras_clave["lenta"] += 1
    if "beta" in caracteristica:
        contador_palabras_clave["beta"] += 1
    if "hrv" in caracteristica:
        contador_palabras_clave["hrv"] += 1

# Graficar el conteo de palabras clave
azul_palette = ["#caddee", "#a5c6e2", "#80aed6", "#5b97ca", "#3b7fb9", '#2f6694']
plt.bar(contador_palabras_clave.keys(), contador_palabras_clave.values(), color=azul_palette)
plt.xlabel('Bandas de frecuencia')
plt.ylabel('Frecuencia')

plt.yticks(range(int(max(contador_palabras_clave.values())) + 1))
max_freq = max(contador_palabras_clave.values())
step = max(1, max_freq // 10)  # Ajusta el paso según el rango de valores de frecuencia
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.yticks(range(0, max_freq + step, step))

plt.show()

# Clasificar características según categorías
categorias = {
    "complexity": [],   "activity": [],'tasa': [], 'pow': [], 'max': [], 'var': [],
    'mobility': [], 'activity': [], 'complexity': [], 'std': [], 'Pabs': [], 'esp_rms': [],
    'Prel': [], 'median': [], 'ske': [], 'curt': [], 'higu_100': [], 'higu_50': [], 'higu_10': [], 'pfd': [],
    'svd_ent': [], 'entropy_Samp': [], 'entropy_ApEn': [], 'spect_ent_f': [], 'rms':[], 'katz':[], 'hurt':[]

    # Agrega más categorías según tus necesidades
}
palabras_clave = {
    "complexity": ["complexity"],
    "activity": ["activity"],
    "tasa": ["tasa"],
    "pow": ["pow"],
    "max": ["max"],
    "var": ["var"],
    "mobility": ["mobility"],
    "std": ["std"],
    "Pabs": ["Pabs"],
    "esp_rms": ["esp_rms"],
    "Prel": ["Prel"],
    "median": ["median"],
    "ske": ["ske"],
    "curt": ["curt"],
    "higu_100": ["higu_100"],
    "higu_50": ["higu_50"],
    "higu_10": ["higu_10"],
    "pfd": ["pfd"],
    "svd_ent": ["svd_ent"],
    "entropy_Samp": ["entropy_Samp"],
    "entropy_ApEn": ["entropy_ApEn"],
    "spect_ent_f": ["spect_ent_f"],
    "rms": ["rms"],
    "katz": ["katz"]
}

# Clasificar las características
for caracteristica in caracteristicas:
    clasificado = False
    for categoria, palabras in palabras_clave.items():
        for palabra in palabras:
            if palabra in caracteristica:
                categorias[categoria].append(caracteristica)
                clasificado = True
                break
        if clasificado:
            break
    if not clasificado:
        categorias['hurt'].append(caracteristica)

# Imprimir las características clasificadas por categorías
for categoria, palabras_clave in categorias.items():
    print(f"Categoría: {categoria}")
    for palabra_clave in palabras_clave:
        print(palabra_clave)
    print()

# Calcular el número de datos por categoría y filtrar las categorías sin datos
num_datos_por_categoria = {categoria: len(palabras_clave) for categoria, palabras_clave in categorias.items() if palabras_clave}

# Definir nuevos nombres para las categorías
nuevos_nombres = {
    "complexity": "Complejidad",
    "activity": "Actividad",
    'tasa': "Tasa",
    'pow': "Pow",
    'max': "Máximo",
    'var': "Varianza",
    'mobility': "Movilidad",
    'std': "std",
    'Pabs': "Pabs",
    'esp_rms': "RMS - espectro",
    'Prel': "Prel",
    'median': "Mediana",
    'ske': "Ske",
    'curt': "Curtosis",
    'higu_100': "HFD 100",
    'higu_50': "HFD 50",
    'higu_10': "HFD 10",
    'pfd': "PFD",
    'svd_ent': "SVD",
    'entropy_Samp': "Samp",
    'entropy_ApEn': "ApEn",
    'spect_ent_f': "SE",
    'rms':'RMS',
    'katz':'FKD',
    'hurt':'HE',

}

# Mapear los nuevos nombres a las categorías existentes
nuevos_nombres_categorias = {nuevos_nombres[categoria]: num_datos for categoria, num_datos in num_datos_por_categoria.items()}
azul_palette = ["#caddee", "#a5c6e2", "#80aed6", "#5b97ca", "#3b7fb9", '#2f6694']
# Crear el gráfico de barras
plt.figure(figsize=(10, 6))
#plt.grid()
plt.bar(nuevos_nombres_categorias.keys(), nuevos_nombres_categorias.values(), color='#4682B4')
plt.xlabel('Característica')
plt.ylabel('Frecuencia')

plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.7)
# Mostrar el gráfico
plt.show()

"""#Eliminar inf y nan"""

df_caract.replace([np.inf, -np.inf], np.nan, inplace=True)
df_caract.dropna(inplace=True)

df_caract.rename(columns={'eeg2_abeta_pow': 'eeg2_beta_pow'}, inplace=True)
df_caract.rename(columns={'ecg2_abeta_pow': 'ecg2_beta_pow'}, inplace=True)

"""#Grupos"""

columnas_a_excluir = [ 'casestart', 'caseend', 'anestart', 'aneend', 'opstart', 'opend', 'age', 'sex', 'height', 'weight', 'optype', 'dx']
df_new= df_caract.drop(columns=columnas_a_excluir)

grupo3  = ['eeg1_max', 'eeg1_min', 'eeg1_med','eeg1_median', 'eeg1_mod', 'eeg1_var','eeg1_std', 'eeg1_pow' , 'eeg1_tasa','eeg1_curt','eeg1_ske']
df_grupo3 = df_new[grupo3]
grupo4_lenta  = ['eeg1_lenta_max' ,'eeg1_lenta_min', 'eeg1_lenta_med','eeg1_lenta_median', 'eeg1_lenta_mod', 'eeg1_lenta_var','eeg1_lenta_std', 'eeg1_lenta_pow', 'eeg1_lenta_tasa','eeg1_lenta_curt','eeg1_lenta_ske']
df_grupo4 = df_new[grupo4_lenta]
grupo5_delta = [elemento.replace('lenta', 'delta') for elemento in grupo4_lenta]
df_grupo5 = df_new[grupo5_delta]
grupo6_theta = [elemento.replace('lenta', 'theta') for elemento in grupo4_lenta]
df_grupo6 = df_new[grupo6_theta]
grupo7_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo4_lenta]
df_grupo7 = df_new[grupo7_alpha]
grupo8_beta = [elemento.replace('lenta', 'beta') for elemento in grupo4_lenta]
df_grupo8 = df_new[grupo8_beta]
grupo9_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo4_lenta]
df_grupo9 = df_new[grupo9_gamma]


grupo10  = ['eeg2_max', 'eeg2_min', 'eeg2_med', 'eeg2_median', 'eeg2_mod', 'eeg2_var', 'eeg2_std', 'eeg2_pow', 'eeg2_tasa', 'eeg2_curt', 'eeg2_ske']
df_grupo10 = df_new[grupo10]
grupo11_lenta = ['eeg2_lenta_max', 'eeg2_lenta_min', 'eeg2_lenta_med', 'eeg2_lenta_median', 'eeg2_lenta_mod', 'eeg2_lenta_var', 'eeg2_lenta_std', 'eeg2_lenta_pow', 'eeg2_lenta_tasa', 'eeg2_lenta_curt', 'eeg2_lenta_ske']
df_grupo11 = df_new[grupo11_lenta]
grupo12_delta = [elemento.replace('lenta', 'delta') for elemento in grupo11_lenta]
df_grupo12 = df_new[grupo12_delta]
grupo13_theta = [elemento.replace('lenta', 'theta') for elemento in grupo11_lenta]
df_grupo13 = df_new[grupo13_theta]
grupo14_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo11_lenta]
df_grupo14 = df_new[grupo14_alpha]
grupo15_beta = [elemento.replace('lenta', 'beta') for elemento in grupo11_lenta]
df_grupo15 = df_new[grupo15_beta]
grupo16_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo11_lenta]
df_grupo16 = df_new[grupo16_gamma]


grupo17  = ['ecg2_max', 'ecg2_min', 'ecg2_med', 'ecg2_median', 'ecg2_mod', 'ecg2_var', 'ecg2_std', 'ecg2_pow', 'ecg2_tasa', 'ecg2_curt', 'ecg2_ske']
df_grupo17 = df_new[grupo17]
grupo18_lenta  = ['ecg2_lenta_max', 'ecg2_lenta_min', 'ecg2_lenta_med', 'ecg2_lenta_median', 'ecg2_lenta_mod', 'ecg2_lenta_var', 'ecg2_lenta_std', 'ecg2_lenta_pow', 'ecg2_lenta_tasa', 'ecg2_lenta_curt', 'ecg2_lenta_ske']
df_grupo18 = df_new[grupo18_lenta]
grupo19_delta = [elemento.replace('lenta', 'delta') for elemento in grupo18_lenta]
df_grupo19 = df_new[grupo19_delta]
grupo20_theta = [elemento.replace('lenta', 'theta') for elemento in grupo18_lenta]
df_grupo20 = df_new[grupo20_theta]
grupo21_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo18_lenta]
df_grupo21 = df_new[grupo21_alpha]
grupo22_beta = [elemento.replace('lenta', 'beta') for elemento in grupo18_lenta]
df_grupo22 = df_new[grupo22_beta]
grupo23_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo18_lenta]
df_grupo23 = df_new[grupo23_gamma]

grupo24 = ['eeg1_entropy_shan', 'eeg1_entropy_ren', 'eeg1_entropy_ApEn','eeg1_entropy_Samp','eeg1_spect_ent_f', 'eeg1_svd_ent']
df_grupo24 = df_new[grupo24]
grupo25_lenta = ['eeg1_lenta_entropy_shan', 'eeg1_lenta_entropy_ren', 'eeg1_lenta_entropy_ApEn','eeg1_lenta_entropy_Samp','eeg1_lenta_spect_ent_f', 'eeg1_lenta_svd_ent']
df_grupo25 = df_new[grupo25_lenta]
grupo26_delta = [elemento.replace('lenta', 'delta') for elemento in grupo25_lenta]
df_grupo26 = df_new[grupo26_delta]
grupo27_theta = [elemento.replace('lenta', 'theta') for elemento in grupo25_lenta]
df_grupo27 = df_new[grupo27_theta]
grupo28_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo25_lenta]
df_grupo28 = df_new[grupo28_alpha]
grupo29_beta = [elemento.replace('lenta', 'beta') for elemento in grupo25_lenta]
df_grupo29 = df_new[grupo29_beta]
grupo30_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo25_lenta]
df_grupo30 = df_new[grupo30_gamma]

grupo31 = ['eeg2_entropy_shan', 'eeg2_entropy_ren', 'eeg2_entropy_ApEn', 'eeg2_entropy_Samp', 'eeg2_spect_ent_f', 'eeg2_svd_ent']
df_grupo31 = df_new[grupo31]
grupo32_lenta = ['eeg2_lenta_entropy_shan', 'eeg2_lenta_entropy_ren', 'eeg2_lenta_entropy_ApEn', 'eeg2_lenta_entropy_Samp', 'eeg2_lenta_spect_ent_f', 'eeg2_lenta_svd_ent']
df_grupo32 = df_new[grupo32_lenta]
grupo33_delta = [elemento.replace('lenta', 'delta') for elemento in grupo32_lenta]
df_grupo33 = df_new[grupo33_delta]
grupo34_theta = [elemento.replace('lenta', 'theta') for elemento in grupo32_lenta]
df_grupo34 = df_new[grupo34_theta]
grupo35_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo32_lenta]
df_grupo35 = df_new[grupo35_alpha]
grupo36_beta = [elemento.replace('lenta', 'beta') for elemento in grupo32_lenta]
df_grupo36 = df_new[grupo36_beta]
grupo37_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo32_lenta]
df_grupo37 = df_new[grupo37_gamma]

grupo38 = [ 'ecg2_entropy_ApEn', 'ecg2_entropy_Samp', 'ecg2_spect_ent_f', 'ecg2_svd_ent']
df_grupo38 = df_new[grupo38]
grupo39_lenta = ['ecg2_lenta_entropy_shan', 'ecg2_lenta_entropy_ren', 'ecg2_lenta_entropy_ApEn', 'ecg2_lenta_entropy_Samp', 'ecg2_lenta_spect_ent_f', 'ecg2_lenta_svd_ent']
df_grupo39 = df_new[grupo39_lenta]
grupo40_delta = [elemento.replace('lenta', 'delta') for elemento in grupo39_lenta]
df_grupo40 = df_new[grupo40_delta]
grupo41_theta = [elemento.replace('lenta', 'theta') for elemento in grupo39_lenta]
df_grupo41 = df_new[grupo41_theta]
grupo42_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo39_lenta]
df_grupo42 = df_new[grupo42_alpha]
grupo43_beta = [elemento.replace('lenta', 'beta') for elemento in grupo39_lenta]
df_grupo43 = df_new[grupo43_beta]
grupo44_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo39_lenta]
df_grupo44 = df_new[grupo44_gamma]


grupo45 = ['eeg1_pfd', 'eeg1_h','eeg1_complexity', 'eeg1_activity','eeg1_mobility','eeg1_katz', 'eeg1_higu_10', 'eeg1_higu_50', 'eeg1_higu_100']
df_grupo45 = df_new[grupo45]
grupo46_lenta = ['eeg1_lenta_pfd','eeg1_lenta_h', 'eeg1_lenta_complexity', 'eeg1_lenta_activity','eeg1_lenta_mobility','eeg1_lenta_katz', 'eeg1_lenta_higu_10', 'eeg1_lenta_higu_50', 'eeg1_lenta_higu_100']
df_grupo46 = df_new[grupo46_lenta]
grupo47_delta = [elemento.replace('lenta', 'delta') for elemento in grupo46_lenta]
df_grupo47 = df_new[grupo47_delta]
grupo48_theta = [elemento.replace('lenta', 'theta') for elemento in grupo46_lenta]
df_grupo48 = df_new[grupo48_theta]
grupo49_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo46_lenta]
df_grupo49 = df_new[grupo49_alpha]
grupo50_beta = [elemento.replace('lenta', 'beta') for elemento in grupo46_lenta]
df_grupo50 = df_new[grupo50_beta]
grupo51_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo46_lenta]
df_grupo51 = df_new[grupo51_gamma]

grupo52 = ['eeg2_pfd', 'eeg2_h', 'eeg2_complexity', 'eeg2_activity', 'eeg2_mobility', 'eeg2_katz', 'eeg2_higu_10', 'eeg2_higu_50', 'eeg2_higu_100']
df_grupo52 = df_new[grupo52]
grupo53_lenta = ['eeg2_lenta_pfd', 'eeg2_lenta_h', 'eeg2_lenta_complexity', 'eeg2_lenta_activity', 'eeg2_lenta_mobility', 'eeg2_lenta_katz', 'eeg2_lenta_higu_10', 'eeg2_lenta_higu_50', 'eeg2_lenta_higu_100']
df_grupo53 = df_new[grupo53_lenta]
grupo54_delta = [elemento.replace('lenta', 'delta') for elemento in grupo53_lenta]
df_grupo54 = df_new[grupo54_delta]
grupo55_theta = [elemento.replace('lenta', 'theta') for elemento in grupo53_lenta]
df_grupo55 = df_new[grupo55_theta]
grupo56_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo53_lenta]
df_grupo56 = df_new[grupo56_alpha]
grupo57_beta = [elemento.replace('lenta', 'beta') for elemento in grupo53_lenta]
df_grupo57 = df_new[grupo57_beta]
grupo58_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo53_lenta]
df_grupo58 = df_new[grupo58_gamma]

grupo59 = ['ecg2_pfd', 'ecg2_h', 'ecg2_complexity', 'ecg2_activity', 'ecg2_mobility', 'ecg2_katz', 'ecg2_higu_10', 'ecg2_higu_50', 'ecg2_higu_100']
df_grupo59 = df_new[grupo59]
grupo60_lenta = ['ecg2_lenta_pfd', 'ecg2_lenta_h', 'ecg2_lenta_complexity', 'ecg2_lenta_activity', 'ecg2_lenta_mobility', 'ecg2_lenta_katz', 'ecg2_lenta_higu_10', 'ecg2_lenta_higu_50', 'ecg2_lenta_higu_100']
df_grupo60 = df_new[grupo60_lenta]
grupo61_delta = [elemento.replace('lenta', 'delta') for elemento in grupo60_lenta]
df_grupo61 = df_new[grupo61_delta]
grupo62_theta = [elemento.replace('lenta', 'theta') for elemento in grupo60_lenta]
df_grupo62 = df_new[grupo62_theta]
grupo63_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo60_lenta]
df_grupo63 = df_new[grupo63_alpha]
grupo64_beta = [elemento.replace('lenta', 'beta') for elemento in grupo60_lenta]
df_grupo64 = df_new[grupo64_beta]
grupo65_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo60_lenta]
df_grupo65 = df_new[grupo65_gamma]

grupo66 = ['hrv_mean_nni','hrv_sdnn','hrv_sdsd','hrv_nni_50','hrv_pnni_50','hrv_nni_20','hrv_pnni_20','hrv_rmssd','hrv_median_nni','hrv_range_nni','hrv_cvsd','hrv_cvnni','hrv_mean_hr','hrv_max_hr','hrv_min_hr', 'hrv_std_hr']
df_grupo66 = df_caract[grupo66]
grupo67 = ['hrv_lf', 'hrv_hf', 'hrv_lf_hf_ratio', 'hrv_lfnu', 'hrv_hfnu','hrv_total_power', 'hrv_vlf', 'hrv_tri_idx','hrv_sd1','hrv_sd2','hrv_ratio_sd2_sd1','hrv_csi', 'hrv_cvi','hrv_Modified_csi', 'hrv_elipse']
#grupo67 = ['hrv_lf', 'hrv_hf', 'hrv_lf_hf_ratio', 'hrv_lfnu', 'hrv_hfnu','hrv_total_power', 'hrv_vlf', 'hrv_tri_idx','hrv_sd1','hrv_sd2','hrv_ratio_sd2_sd1','hrv_csi', 'hrv_cvi','hrv_Modified_csi']
df_grupo67 = df_caract[grupo67]

grupo68 = ['hf_max', 'hf_min', 'hf_med', 'hf_median', 'hf_mod', 'hf_var','hf_std', 'hf_pow', 'hf_tasa', 'hf_curt','hf_ske']
df_grupo68 = df_new[grupo68]

grupo69 = ['lf_max', 'lf_min', 'lf_med', 'lf_median', 'lf_mod', 'lf_var', 'lf_std', 'lf_pow', 'lf_tasa', 'lf_curt', 'lf_ske']
df_grupo69 = df_new[grupo69]

grupo70 = ['hf_entropy_shan', 'hf_entropy_ren', 'hf_entropy_ApEn', 'hf_entropy_Samp','hf_spect_ent_f', 'hf_svd_ent']
df_grupo70 = df_new[grupo70]

grupo71 =['lf_entropy_shan', 'lf_entropy_ren', 'lf_entropy_ApEn', 'lf_entropy_Samp', 'lf_spect_ent_f', 'lf_svd_ent']
df_grupo71 = df_new[grupo71]

grupo72 = ['hrv_entropy_shan', 'hrv_entropy_ren', 'hrv_entropy_ApEn', 'hrv_spect_ent_f', 'hrv_svd_ent']
df_grupo72 = df_caract[grupo72]

grupo73 = ['hf_pfd', 'hf_h', 'hf_complexity', 'hf_activity','hf_mobility','hf_katz', 'hf_higu_10', 'hf_higu_50', 'hf_higu_100']
df_grupo73 = df_new[grupo73]

grupo74 = ['lf_pfd', 'lf_h', 'lf_complexity', 'lf_activity', 'lf_mobility', 'lf_katz', 'lf_higu_10', 'lf_higu_50', 'lf_higu_100']
df_grupo74 = df_new[grupo74]

grupo75 = ['hrv_pfd', 'hrv_h', 'hrv_complexity', 'hrv_activity', 'hrv_mobility', 'hrv_katz', 'hrv_higu_10', 'hrv_higu_50', 'hrv_higu_100', 'hrv_dfa_a1', 'hrv_dfa_a2']
#grupo75 = ['hrv_pfd',  'hrv_complexity', 'hrv_activity', 'hrv_mobility', 'hrv_katz', 'hrv_higu_10']
df_grupo75 = df_caract[grupo75]

grupo76 = ['Pabs_eeg1_lenta', 'Pabs_eeg1_delta', 'Pabs_eeg1_theta', 'Pabs_eeg1_alpha', 'Pabs_eeg1_beta', 'Pabs_eeg1_gamma']
df_grupo76 = df_new[grupo76]

grupo77 = ['Prel_eeg1_lenta', 'Prel_eeg1_delta', 'Prel_eeg1_theta', 'Prel_eeg1_alpha', 'Prel_eeg1_beta', 'Prel_eeg1_gamma']
df_grupo77 = df_new[grupo77]

grupo78 = ['eeg1_lenta_rms', 'eeg1_delta_rms', 'eeg1_theta_rms', 'eeg1_alpha_rms', 'eeg1_beta_rms',  'eeg1_gamma_rms']
df_grupo78 = df_new[grupo78]

grupo79 = ['esp_rms_eeg1_lenta', 'esp_rms_eeg1_delta','esp_rms_eeg1_theta', 'esp_rms_eeg1_alpha' , 'esp_rms_eeg1_beta', 'esp_rms_eeg1_gamma']
df_grupo79 = df_new[grupo79]

grupo80_modificado = [elemento.replace('eeg1', 'eeg2') for elemento in grupo76]
grupo81_modificado = [elemento.replace('eeg1', 'eeg2') for elemento in grupo77]
grupo82_modificado = [elemento.replace('eeg1', 'eeg2') for elemento in grupo78]
grupo83_modificado = [elemento.replace('eeg1', 'eeg2') for elemento in grupo79]

# Cambiar nombres de columnas en el DataFrame
df_grupo80 = df_new[grupo80_modificado]
df_grupo81 = df_new[grupo81_modificado]
df_grupo82 = df_new[grupo82_modificado]
df_grupo83 = df_new[grupo83_modificado]

grupo84_modificado = [elemento.replace('eeg1', 'ecg2') for elemento in grupo76]
grupo85_modificado = [elemento.replace('eeg1', 'ecg2') for elemento in grupo77]
grupo86_modificado = [elemento.replace('eeg1', 'ecg2') for elemento in grupo78]
grupo87_modificado = [elemento.replace('eeg1', 'ecg2') for elemento in grupo79]

# Cambiar nombres de columnas en el DataFrame
df_grupo84 = df_new[grupo84_modificado]
df_grupo85 = df_new[grupo85_modificado]
df_grupo86 = df_new[grupo86_modificado]
df_grupo87 = df_new[grupo87_modificado]

grupo88 = ['eeg1_max_freq', 'eeg1_min_freq', 'eeg1_med_freq', 'eeg1_median_freq', 'eeg1_mod_freq', 'eeg1_var_freq', 'eeg1_std_freq', 'eeg1_pow_freq', 'eeg1_tasa_freq', 'eeg1_curt_freq', 'eeg1_ske_freq', 'eeg1_freq_max', 'eeg1_mean_freq', 'eeg1_freq_median', 'eeg1_freq_centroid', 'eeg1_freq_dispersion', 'eeg1_freq_flatness', 'eeg1_freq_slope', 'eeg1_freq_crest_factor']
df_grupo88 = df_new [grupo88]
grupo89_lenta = ['eeg1_lenta_max_freq', 'eeg1_lenta_min_freq', 'eeg1_lenta_med_freq', 'eeg1_lenta_median_freq', 'eeg1_lenta_mod_freq', 'eeg1_lenta_var_freq', 'eeg1_lenta_std_freq', 'eeg1_lenta_pow_freq', 'eeg1_lenta_tasa_freq', 'eeg1_lenta_curt_freq', 'eeg1_lenta_ske_freq', 'eeg1_lenta_freq_max', 'eeg1_lenta_mean_freq', 'eeg1_lenta_freq_median', 'eeg1_lenta_freq_centroid', 'eeg1_lenta_freq_dispersion', 'eeg1_lenta_freq_flatness', 'eeg1_lenta_freq_slope', 'eeg1_lenta_freq_crest_factor']
df_grupo89 = df_new[grupo89_lenta]
grupo90_delta = [elemento.replace('lenta', 'delta') for elemento in grupo89_lenta]
df_grupo90 = df_new[grupo90_delta]
grupo91_theta = [elemento.replace('lenta', 'theta') for elemento in grupo89_lenta]
df_grupo91 = df_new[grupo91_theta]
grupo92_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo89_lenta]
df_grupo92 = df_new[grupo92_alpha]
grupo93_beta = [elemento.replace('lenta', 'beta') for elemento in grupo89_lenta]
df_grupo93 = df_new[grupo93_beta]
grupo94_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo89_lenta]
df_grupo94 = df_new[grupo94_gamma]


grupo95 = ['eeg2_max_freq', 'eeg2_min_freq', 'eeg2_med_freq', 'eeg2_median_freq', 'eeg2_mod_freq', 'eeg2_var_freq', 'eeg2_std_freq', 'eeg2_pow_freq', 'eeg2_tasa_freq', 'eeg2_curt_freq', 'eeg2_ske_freq', 'eeg2_freq_max', 'eeg2_mean_freq', 'eeg2_freq_median', 'eeg2_freq_centroid', 'eeg2_freq_dispersion', 'eeg2_freq_flatness', 'eeg2_freq_slope', 'eeg2_freq_crest_factor']
df_grupo95 = df_new [grupo95]
grupo96_lenta = ['eeg2_lenta_max_freq', 'eeg2_lenta_min_freq', 'eeg2_lenta_med_freq', 'eeg2_lenta_median_freq', 'eeg2_lenta_mod_freq', 'eeg2_lenta_var_freq', 'eeg2_lenta_std_freq', 'eeg2_lenta_pow_freq', 'eeg2_lenta_tasa_freq', 'eeg2_lenta_curt_freq', 'eeg2_lenta_ske_freq', 'eeg2_lenta_freq_max', 'eeg2_lenta_mean_freq', 'eeg2_lenta_freq_median', 'eeg2_lenta_freq_centroid', 'eeg2_lenta_freq_dispersion', 'eeg2_lenta_freq_flatness', 'eeg2_lenta_freq_slope', 'eeg2_lenta_freq_crest_factor']
df_grupo96 = df_new[grupo96_lenta]
grupo97_delta = [elemento.replace('lenta', 'delta') for elemento in grupo96_lenta]
df_grupo97 = df_new[grupo97_delta]
grupo98_theta = [elemento.replace('lenta', 'theta') for elemento in grupo96_lenta]
df_grupo98 = df_new[grupo98_theta]
grupo99_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo96_lenta]
df_grupo99 = df_new[grupo99_alpha]
grupo100_beta = [elemento.replace('lenta', 'beta') for elemento in grupo96_lenta]
df_grupo100 = df_new[grupo100_beta]
grupo101_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo96_lenta]
df_grupo101 = df_new[grupo101_gamma]

grupo102 = ['ecg2_max_freq', 'ecg2_min_freq', 'ecg2_med_freq', 'ecg2_median_freq', 'ecg2_mod_freq', 'ecg2_var_freq', 'ecg2_std_freq', 'ecg2_pow_freq', 'ecg2_tasa_freq', 'ecg2_curt_freq', 'ecg2_ske_freq', 'ecg2_freq_max', 'ecg2_mean_freq', 'ecg2_freq_median', 'ecg2_freq_centroid', 'ecg2_freq_dispersion', 'ecg2_freq_flatness', 'ecg2_freq_slope', 'ecg2_freq_crest_factor']
df_grupo102 = df_new [grupo102]
grupo103_lenta = ['ecg2_lenta_max_freq', 'ecg2_lenta_min_freq', 'ecg2_lenta_med_freq', 'ecg2_lenta_median_freq', 'ecg2_lenta_mod_freq', 'ecg2_lenta_var_freq', 'ecg2_lenta_std_freq', 'ecg2_lenta_pow_freq', 'ecg2_lenta_tasa_freq', 'ecg2_lenta_curt_freq', 'ecg2_lenta_ske_freq', 'ecg2_lenta_freq_max', 'ecg2_lenta_mean_freq', 'ecg2_lenta_freq_median', 'ecg2_lenta_freq_centroid', 'ecg2_lenta_freq_dispersion', 'ecg2_lenta_freq_flatness', 'ecg2_lenta_freq_slope', 'ecg2_lenta_freq_crest_factor']
df_grupo103 = df_new[grupo103_lenta]
grupo104_delta = [elemento.replace('lenta', 'delta') for elemento in grupo103_lenta]
df_grupo104 = df_new[grupo104_delta]
grupo105_theta = [elemento.replace('lenta', 'theta') for elemento in grupo103_lenta]
df_grupo105 = df_new[grupo105_theta]
grupo106_alpha = [elemento.replace('lenta', 'alpha') for elemento in grupo103_lenta]
df_grupo106 = df_new[grupo106_alpha]
grupo107_beta = [elemento.replace('lenta', 'beta') for elemento in grupo103_lenta]
df_grupo107 = df_new[grupo107_beta]
grupo108_gamma = [elemento.replace('lenta', 'gamma') for elemento in grupo103_lenta]
df_grupo108 = df_new[grupo108_gamma]

#concaternar datos: tiempo, entropia, caoticas

df_eeg1= pd.concat([df_caract[['Estado']],df_grupo3, df_grupo24, df_grupo45], axis=1)
df_eeg1_lenta= pd.concat([df_new['Estado'], df_grupo4, df_grupo25, df_grupo46, df_grupo76['Pabs_eeg1_lenta'], df_grupo77['Prel_eeg1_lenta'],df_grupo78['eeg1_lenta_rms'],df_grupo79['esp_rms_eeg1_lenta']], axis=1)
df_eeg1_delta= pd.concat([df_new['Estado'],df_grupo5, df_grupo26, df_grupo47,df_grupo76['Pabs_eeg1_delta'], df_grupo77['Prel_eeg1_delta'],df_grupo78['eeg1_delta_rms'],df_grupo79['esp_rms_eeg1_delta']], axis=1)
df_eeg1_theta= pd.concat([df_new['Estado'],df_grupo6, df_grupo27, df_grupo48, df_grupo76['Pabs_eeg1_theta'], df_grupo77['Prel_eeg1_theta'],df_grupo78['eeg1_theta_rms'],df_grupo79['esp_rms_eeg1_theta']], axis=1)
df_eeg1_alpha= pd.concat([df_new['Estado'],df_grupo7, df_grupo28, df_grupo49, df_grupo76['Pabs_eeg1_alpha'], df_grupo77['Prel_eeg1_alpha'],df_grupo78['eeg1_alpha_rms'],df_grupo79['esp_rms_eeg1_alpha']], axis=1)
df_eeg1_beta= pd.concat([df_new['Estado'],df_grupo8, df_grupo29, df_grupo50, df_grupo76['Pabs_eeg1_beta'], df_grupo77['Prel_eeg1_beta'],df_grupo78['eeg1_beta_rms'],df_grupo79['esp_rms_eeg1_beta']], axis=1)
df_eeg1_gamma= pd.concat([df_new['Estado'],df_grupo9, df_grupo30, df_grupo51, df_grupo76['Pabs_eeg1_gamma'], df_grupo77['Prel_eeg1_gamma'],df_grupo78['eeg1_gamma_rms'],df_grupo79['esp_rms_eeg1_gamma']], axis=1)

df_eeg1_all= pd.concat([df_new[['Estado', 'caseid']], df_grupo3, df_grupo24, df_grupo45, df_grupo4, df_grupo25, df_grupo46,df_grupo5, df_grupo26, df_grupo47,df_grupo76, df_grupo77,df_grupo78,df_grupo79, df_grupo6,
                        df_grupo27, df_grupo48,df_grupo7, df_grupo28, df_grupo49,df_grupo8, df_grupo29, df_grupo50,df_grupo9, df_grupo30, df_grupo51, df_grupo88, df_grupo89,
                        df_grupo90, df_grupo91, df_grupo92, df_grupo93, df_grupo94], axis=1)

df_eeg2= pd.concat([df_new[['Estado', 'caseid']],df_grupo10, df_grupo31, df_grupo52], axis=1)
df_eeg2_lenta= pd.concat([df_new[['Estado', 'caseid']],df_grupo11, df_grupo32, df_grupo53, df_grupo80['Pabs_eeg2_lenta'], df_grupo81['Prel_eeg2_lenta'],df_grupo82['eeg2_lenta_rms'],df_grupo83['esp_rms_eeg2_lenta']], axis=1)
df_eeg2_delta= pd.concat([df_new[['Estado', 'caseid']],df_grupo12, df_grupo33, df_grupo54, df_grupo80['Pabs_eeg2_delta'], df_grupo81['Prel_eeg2_delta'],df_grupo82['eeg2_delta_rms'],df_grupo83['esp_rms_eeg2_delta']], axis=1)
df_eeg2_theta= pd.concat([df_new[['Estado', 'caseid']],df_grupo13, df_grupo34, df_grupo55, df_grupo80['Pabs_eeg2_theta'], df_grupo81['Prel_eeg2_theta'],df_grupo82['eeg2_theta_rms'],df_grupo83['esp_rms_eeg2_theta']], axis=1)
df_eeg2_alpha= pd.concat([df_new[['Estado', 'caseid']],df_grupo14, df_grupo35, df_grupo56, df_grupo80['Pabs_eeg2_alpha'], df_grupo81['Prel_eeg2_alpha'],df_grupo82['eeg2_alpha_rms'],df_grupo83['esp_rms_eeg2_alpha']], axis=1)
df_eeg2_beta= pd.concat([df_new[['Estado', 'caseid']],df_grupo15, df_grupo36, df_grupo57, df_grupo80['Pabs_eeg2_beta'], df_grupo81['Prel_eeg2_beta'],df_grupo82['eeg2_beta_rms'],df_grupo83['esp_rms_eeg2_beta']], axis=1)
df_eeg2_gamma= pd.concat([df_new[['Estado', 'caseid']],df_grupo16, df_grupo37, df_grupo58, df_grupo80['Pabs_eeg2_gamma'], df_grupo81['Prel_eeg2_gamma'],df_grupo82['eeg2_gamma_rms'],df_grupo83['esp_rms_eeg2_gamma']], axis=1)

df_eeg2_all = pd.concat([df_new[['Estado', 'caseid']],df_grupo11, df_grupo32, df_grupo53, df_grupo80, df_grupo81,df_grupo82,df_grupo83,df_grupo12, df_grupo33, df_grupo54,
                         df_grupo13, df_grupo34, df_grupo55,df_grupo14, df_grupo35, df_grupo56,df_grupo15, df_grupo36, df_grupo57,df_grupo16, df_grupo37, df_grupo58,
                         df_grupo95, df_grupo96, df_grupo97, df_grupo98, df_grupo99, df_grupo100, df_grupo101], axis=1)


df_ecg2= pd.concat([df_new[['Estado', 'caseid']],df_grupo17, df_grupo38, df_grupo59], axis=1)
df_ecg2_lenta= pd.concat([df_new[['Estado', 'caseid']],df_grupo18, df_grupo39, df_grupo60, df_grupo84['Pabs_ecg2_lenta'], df_grupo85['Prel_ecg2_lenta'],df_grupo86['ecg2_lenta_rms'],df_grupo87['esp_rms_ecg2_lenta']], axis=1)
df_ecg2_delta= pd.concat([df_new[['Estado', 'caseid']],df_grupo19, df_grupo40, df_grupo61, df_grupo84['Pabs_ecg2_delta'], df_grupo85['Prel_ecg2_delta'],df_grupo86['ecg2_delta_rms'],df_grupo87['esp_rms_ecg2_delta']], axis=1)
df_ecg2_theta= pd.concat([df_new[['Estado', 'caseid']],df_grupo20, df_grupo41, df_grupo62, df_grupo84['Pabs_ecg2_theta'], df_grupo85['Prel_ecg2_theta'],df_grupo86['ecg2_theta_rms'],df_grupo87['esp_rms_ecg2_theta']], axis=1)
df_ecg2_alpha= pd.concat([df_new[['Estado', 'caseid']],df_grupo21, df_grupo42, df_grupo63, df_grupo84['Pabs_ecg2_alpha'], df_grupo85['Prel_ecg2_alpha'],df_grupo86['ecg2_alpha_rms'],df_grupo87['esp_rms_ecg2_alpha']], axis=1)
df_ecg2_beta= pd.concat([df_new[['Estado', 'caseid']],df_grupo22, df_grupo43, df_grupo64, df_grupo84['Pabs_ecg2_beta'], df_grupo85['Prel_ecg2_beta'],df_grupo86['ecg2_beta_rms'],df_grupo87['esp_rms_ecg2_beta']], axis=1)
df_ecg2_gamma= pd.concat([df_new[['Estado', 'caseid']],df_grupo23, df_grupo44, df_grupo65, df_grupo84['Pabs_ecg2_gamma'], df_grupo85['Prel_ecg2_gamma'],df_grupo86['ecg2_gamma_rms'],df_grupo87['esp_rms_ecg2_gamma']], axis=1)

df_ecg2_all = pd.concat([df_new['Estado'],df_grupo17, df_grupo38, df_grupo59,df_grupo18, df_grupo39, df_grupo60, df_grupo84, df_grupo85,df_grupo86,df_grupo87,df_grupo19, df_grupo40, df_grupo61,df_grupo20, df_grupo41, df_grupo62,
                         df_grupo21, df_grupo42, df_grupo63,df_grupo22, df_grupo43, df_grupo64,df_grupo23, df_grupo44, df_grupo65, df_grupo102, df_grupo103, df_grupo104, df_grupo105, df_grupo106, df_grupo107, df_grupo108], axis=1)

df_hrv = pd.concat([df_new[['Estado', 'caseid']],df_grupo66, df_grupo67, df_grupo72, df_grupo75] , axis=1)
df_hf =  pd.concat([df_new['Estado'],df_grupo68, df_grupo70, df_grupo73] , axis=1)
df_lf =  pd.concat([df_new['Estado'],df_grupo69, df_grupo71, df_grupo74] , axis=1)
df_hrv_all = pd.concat([df_new['Estado'],df_grupo66, df_grupo67, df_grupo72, df_grupo75,df_grupo68, df_grupo70, df_grupo73, df_grupo69, df_grupo71, df_grupo74] , axis=1)

df_ecg2= pd.concat([df_new[['Estado', 'caseid']],df_grupo17, df_grupo38, df_grupo59, df_grupo66, df_grupo67, df_grupo72, df_grupo75], axis=1)
df_ecg2_lenta= pd.concat([df_new[['Estado', 'caseid']],df_grupo18, df_grupo39, df_grupo60, df_grupo84['Pabs_ecg2_lenta'], df_grupo85['Prel_ecg2_lenta'],df_grupo86['ecg2_lenta_rms'],df_grupo87['esp_rms_ecg2_lenta'], df_grupo66, df_grupo67, df_grupo72, df_grupo75], axis=1)
df_ecg2_delta= pd.concat([df_new[['Estado', 'caseid']],df_grupo19, df_grupo40, df_grupo61, df_grupo84['Pabs_ecg2_delta'], df_grupo85['Prel_ecg2_delta'],df_grupo86['ecg2_delta_rms'],df_grupo87['esp_rms_ecg2_delta'], df_grupo66, df_grupo67, df_grupo72, df_grupo75], axis=1)
df_ecg2_theta= pd.concat([df_new[['Estado', 'caseid']],df_grupo20, df_grupo41, df_grupo62, df_grupo84['Pabs_ecg2_theta'], df_grupo85['Prel_ecg2_theta'],df_grupo86['ecg2_theta_rms'],df_grupo87['esp_rms_ecg2_theta'], df_grupo66, df_grupo67, df_grupo72, df_grupo75], axis=1)
df_ecg2_alpha= pd.concat([df_new[['Estado', 'caseid']],df_grupo21, df_grupo42, df_grupo63, df_grupo84['Pabs_ecg2_alpha'], df_grupo85['Prel_ecg2_alpha'],df_grupo86['ecg2_alpha_rms'],df_grupo87['esp_rms_ecg2_alpha'], df_grupo66, df_grupo67, df_grupo72, df_grupo75], axis=1)
df_ecg2_beta= pd.concat([df_new[['Estado', 'caseid']],df_grupo22, df_grupo43, df_grupo64, df_grupo84['Pabs_ecg2_beta'], df_grupo85['Prel_ecg2_beta'],df_grupo86['ecg2_beta_rms'],df_grupo87['esp_rms_ecg2_beta'], df_grupo66, df_grupo67, df_grupo72, df_grupo75], axis=1)
df_ecg2_gamma= pd.concat([df_new[['Estado', 'caseid']],df_grupo23, df_grupo44, df_grupo65, df_grupo84['Pabs_ecg2_gamma'], df_grupo85['Prel_ecg2_gamma'],df_grupo86['ecg2_gamma_rms'],df_grupo87['esp_rms_ecg2_gamma'], df_grupo66, df_grupo67, df_grupo72, df_grupo75], axis=1)

df_ecg2_all = pd.concat([df_new[['Estado', 'caseid']],df_grupo17, df_grupo38, df_grupo59,df_grupo18, df_grupo39, df_grupo60, df_grupo84, df_grupo85,df_grupo86,df_grupo87,df_grupo19, df_grupo40, df_grupo61,df_grupo20, df_grupo41, df_grupo62,
                         df_grupo21, df_grupo42, df_grupo63,df_grupo22, df_grupo43, df_grupo64,df_grupo23, df_grupo44, df_grupo65, df_grupo66, df_grupo67, df_grupo72, df_grupo75,df_grupo68, df_grupo70, df_grupo73, df_grupo69, df_grupo71, df_grupo74,
                         df_grupo102, df_grupo103, df_grupo104, df_grupo105, df_grupo106, df_grupo107, df_grupo108], axis=1)

"""#ELiminar outliers

**Autoencoder**


---
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dropout
'''
columnas_a_excluir = ['Estado', 'caseid','casestart', 'caseend', 'anestart', 'aneend', 'opstart', 'opend',
                      'age', 'sex', 'height', 'weight', 'optype', 'dx', 'bis_max',	'bis_min',	'bis_med',
                      'bis_median',	'bis_mod',	'bis_var'	,'bis_std']
'''
columnas_a_excluir = ['Estado', 'caseid']
X = df_eeg1_all.drop(columns=columnas_a_excluir)
y = df_eeg1_all['Estado']
# Escala tus datos
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Define y entrena el autoencoder
input_dim = X_scaled.shape[1]
code_size = 32

input_layer = Input(shape=(input_dim,))
encoded = Dense(128, activation='relu')(input_layer)
decoded = Dropout(0.2)(encoded)
encoded = Dense(32, activation='relu')(encoded)
decoded = Dropout(0.2)(decoded)
decoded = Dense(64, activation='relu')(encoded)
decoded = Dropout(0.2)(decoded)
decoded = Dense(input_dim, activation='sigmoid')(decoded)

autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=256)

# Reconstruye los datos
reconstructions = autoencoder.predict(X_scaled)

# Calcula la pérdida de reconstrucción
mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)

# Establece un umbral de anomalía
threshold = np.percentile(mse, 95)

# Identifica y elimina los outliers
outliers_indices = np.where(mse > threshold)[0]
X_clean = np.delete(X_scaled, outliers_indices, axis=0)
y_clean = np.delete(y.values, outliers_indices)

data_clean = pd.DataFrame(data=X_clean, columns=X.columns)
df_reset_index = df_eeg1_all.reset_index(drop=True)
data_clean['Estado'] = y_clean
data_clean['caseid'] = df_reset_index['caseid']
data_clean

from google.colab import files
data_clean.to_csv('df_eeg1_clean.csv', index=False)
files.download('df_eeg1_clean.csv')

"""#CalculateVIF"""

from sklearn.linear_model import LinearRegression


def calculateVIF(var_predictoras_df):
    var_pred_labels = list(var_predictoras_df.columns)
    num_var_pred = len(var_pred_labels)

    lr_model = LinearRegression()

    result = pd.DataFrame(index = ['VIF'], columns = var_pred_labels)
    result = result.fillna(0)

    for ite in range(num_var_pred):
        x_features = var_pred_labels[:]
        y_feature = var_pred_labels[ite]
        x_features.remove(y_feature)

        x = var_predictoras_df[x_features]
        y = var_predictoras_df[y_feature]

        lr_model.fit(var_predictoras_df[x_features], var_predictoras_df[y_feature])

        result[y_feature] = 1/(1 - lr_model.score(var_predictoras_df[x_features], var_predictoras_df[y_feature]))

    return result
def selectDataUsingVIF(var_predictoras_df, max_VIF = 5):
    result = var_predictoras_df.copy(deep = True)

    VIF = calculateVIF(result)

    while VIF.values.max() > max_VIF:
        col_max = np.where(VIF == VIF.values.max())[1][0]
        features = list(result.columns)
        features.remove(features[col_max])
        result = result[features]

        VIF = calculateVIF(result)

    return result

columnas_a_excluir = ['Estado', 'caseid']

X = data_clean.drop(columns=columnas_a_excluir)
y = data_clean['Estado']

VIF = calculateVIF(selectDataUsingVIF(X)).T
VIF.index

new_caract =['Estado','eeg1_median', 'eeg1_ske', 'eeg1_lenta_med', 'eeg1_lenta_median',
       'eeg1_lenta_curt', 'eeg1_lenta_ske', 'eeg1_lenta_pfd',
       'eeg1_lenta_complexity', 'eeg1_lenta_katz', 'eeg1_lenta_higu_10',
       'eeg1_delta_med', 'eeg1_delta_median', 'eeg1_delta_curt',
       'eeg1_delta_ske', 'eeg1_delta_pfd', 'eeg1_delta_h',
       'eeg1_delta_complexity', 'eeg1_delta_katz', 'eeg1_delta_higu_10',
       'Pabs_eeg1_delta', 'Pabs_eeg1_beta', 'Prel_eeg1_theta',
       'Prel_eeg1_alpha', 'Prel_eeg1_beta', 'esp_rms_eeg1_gamma',
       'eeg1_theta_med', 'eeg1_theta_median', 'eeg1_theta_curt',
       'eeg1_theta_ske', 'eeg1_theta_spect_ent_f', 'eeg1_theta_pfd',
       'eeg1_theta_h', 'eeg1_theta_katz', 'eeg1_theta_higu_50',
       'eeg1_theta_higu_100', 'eeg1_alpha_med', 'eeg1_alpha_median',
       'eeg1_alpha_curt', 'eeg1_alpha_ske', 'eeg1_alpha_pfd',
       'eeg1_alpha_complexity', 'eeg1_alpha_katz', 'eeg1_alpha_higu_100',
       'eeg1_beta_med', 'eeg1_beta_median', 'eeg1_beta_curt', 'eeg1_beta_ske',
       'eeg1_beta_entropy_Samp', 'eeg1_beta_h', 'eeg1_beta_complexity',
       'eeg1_beta_katz', 'eeg1_beta_higu_100', 'eeg1_gamma_med',
       'eeg1_gamma_median', 'eeg1_gamma_curt', 'eeg1_gamma_ske',
       'eeg1_gamma_entropy_ren', 'eeg1_gamma_h', 'eeg1_gamma_complexity',
       'eeg1_gamma_katz', 'eeg1_gamma_higu_10', 'eeg1_mod_freq',
       'eeg1_tasa_freq', 'eeg1_freq_max', 'eeg1_freq_flatness',
       'eeg1_lenta_mod_freq', 'eeg1_lenta_tasa_freq', 'eeg1_lenta_freq_max',
       'eeg1_lenta_freq_dispersion', 'eeg1_lenta_freq_crest_factor',
       'eeg1_delta_tasa_freq', 'eeg1_delta_freq_max',
       'eeg1_delta_freq_dispersion', 'eeg1_delta_freq_crest_factor',
       'eeg1_theta_mod_freq', 'eeg1_theta_tasa_freq', 'eeg1_theta_freq_max',
       'eeg1_theta_freq_median', 'eeg1_theta_freq_dispersion',
       'eeg1_theta_freq_crest_factor', 'eeg1_alpha_mod_freq',
       'eeg1_alpha_tasa_freq', 'eeg1_alpha_freq_max',
       'eeg1_alpha_freq_flatness', 'eeg1_alpha_freq_crest_factor',
       'eeg1_beta_mod_freq', 'eeg1_beta_tasa_freq', 'eeg1_beta_freq_max',
       'eeg1_beta_freq_flatness', 'eeg1_beta_freq_crest_factor',
       'eeg1_gamma_mod_freq', 'eeg1_gamma_tasa_freq', 'eeg1_gamma_curt_freq',
       'eeg1_gamma_freq_max', 'eeg1_gamma_freq_median',
       'eeg1_gamma_freq_flatness']

data_hrv = data_clean[new_caract]

columnas_a_excluir = ['Estado']

X = data_hrv.drop(columns=columnas_a_excluir)
y = data_hrv['Estado']

data_hrv

df_filtrado = data_hrv[data_hrv['Estado'] != 4]

# Define las columnas a excluir
columnas_a_excluir = ['Estado']

# Separa características y etiquetas
X_r = df_filtrado.drop(columns=columnas_a_excluir)
y_r = df_filtrado['Estado']

# Calcula el muestreo estratégico
sampling_strategy = {0: 1284}  # Define las proporciones deseadas

# Instancia el objeto SMOTE con el muestreo estratégico
smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)

# Sobremuestreo de datos
X_resampled, y_resampled = smote.fit_resample(X_r, y_r)

df_resampled = pd.concat([pd.DataFrame(X_resampled), pd.DataFrame(y_resampled, columns=['Estado'])], axis=1)
subset_df = df_resampled[df_resampled['Estado'].isin([0,1,2,3])]  # HRV_sin_outliers

# Función para realizar el muestreo de cada grupo
def sample_by_state(group):
    return group.sample(n=1200, random_state=42)

# Aplicar la función de muestreo por grupo
sampled_df = subset_df.groupby('Estado', group_keys=False).apply(sample_by_state)

sampled_df

"""#Comparar modelos para Clasificación

##Dependiente del paciente

###Desbalanceada
"""

data_clean =pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TrabajoGrado/Code_new/SerieTemporales/Dataframes2/df_eeg1_clean.csv')
data_clean

caract = ['Estado','caseid', 'eeg1_median', 'eeg1_ske', 'eeg1_lenta_med', 'eeg1_lenta_median',
       'eeg1_lenta_curt', 'eeg1_lenta_ske', 'eeg1_lenta_pfd',
       'eeg1_lenta_complexity', 'eeg1_lenta_katz', 'eeg1_lenta_higu_10',
       'eeg1_delta_med', 'eeg1_delta_median', 'eeg1_delta_curt',
       'eeg1_delta_ske', 'eeg1_delta_pfd', 'eeg1_delta_h',
       'eeg1_delta_complexity', 'eeg1_delta_katz', 'eeg1_delta_higu_10',
       'Pabs_eeg1_delta', 'Pabs_eeg1_beta', 'Prel_eeg1_theta',
       'Prel_eeg1_alpha', 'Prel_eeg1_beta', 'esp_rms_eeg1_gamma',
       'eeg1_theta_med', 'eeg1_theta_median', 'eeg1_theta_curt',
       'eeg1_theta_ske', 'eeg1_theta_spect_ent_f', 'eeg1_theta_pfd',
       'eeg1_theta_h', 'eeg1_theta_katz', 'eeg1_theta_higu_50',
       'eeg1_theta_higu_100', 'eeg1_alpha_med', 'eeg1_alpha_median',
       'eeg1_alpha_curt', 'eeg1_alpha_ske', 'eeg1_alpha_pfd',
       'eeg1_alpha_complexity', 'eeg1_alpha_katz', 'eeg1_alpha_higu_100',
       'eeg1_beta_med', 'eeg1_beta_median', 'eeg1_beta_curt', 'eeg1_beta_ske',
       'eeg1_beta_entropy_Samp', 'eeg1_beta_h', 'eeg1_beta_complexity',
       'eeg1_beta_katz', 'eeg1_beta_higu_100', 'eeg1_gamma_med',
       'eeg1_gamma_median', 'eeg1_gamma_curt', 'eeg1_gamma_ske',
       'eeg1_gamma_entropy_ren', 'eeg1_gamma_h', 'eeg1_gamma_complexity',
       'eeg1_gamma_katz', 'eeg1_gamma_higu_10', 'eeg1_mod_freq',
       'eeg1_tasa_freq', 'eeg1_freq_max', 'eeg1_freq_flatness',
       'eeg1_lenta_mod_freq', 'eeg1_lenta_tasa_freq', 'eeg1_lenta_freq_max',
       'eeg1_lenta_freq_dispersion', 'eeg1_lenta_freq_crest_factor',
       'eeg1_delta_tasa_freq', 'eeg1_delta_freq_max',
       'eeg1_delta_freq_dispersion', 'eeg1_delta_freq_crest_factor',
       'eeg1_theta_mod_freq', 'eeg1_theta_tasa_freq', 'eeg1_theta_freq_max',
       'eeg1_theta_freq_median', 'eeg1_theta_freq_dispersion',
       'eeg1_theta_freq_crest_factor', 'eeg1_alpha_mod_freq',
       'eeg1_alpha_tasa_freq', 'eeg1_alpha_freq_max',
       'eeg1_alpha_freq_flatness', 'eeg1_alpha_freq_crest_factor',
       'eeg1_beta_mod_freq', 'eeg1_beta_tasa_freq', 'eeg1_beta_freq_max',
       'eeg1_beta_freq_flatness', 'eeg1_beta_freq_crest_factor',
       'eeg1_gamma_mod_freq', 'eeg1_gamma_tasa_freq', 'eeg1_gamma_curt_freq',
       'eeg1_gamma_freq_max', 'eeg1_gamma_freq_median',
       'eeg1_gamma_freq_flatness']
data_clean = data_clean[caract]

data_clean

subset_df_desbalance = data_clean[data_clean['Estado'].isin([0,1,2,3])]

subset_df_desbalance.loc[subset_df_desbalance['Estado'] == 2, 'Estado'] = 0
subset_df_desbalance.loc[subset_df_desbalance['Estado'] == 3, 'Estado'] = 1

"""###Deinir X, y"""

columnas_a_excluir = ['Estado', 'caseid','tiempo_start','tiempo_end', 'casestart', 'caseend', 'anestart', 'aneend', 'opstart', 'opend', 'age', 'sex', 'height', 'weight', 'optype', 'dx']

columnas_a_excluir = ['Estado', 'caseid']

X = subset_df_desbalance.drop(columns=columnas_a_excluir)
y = subset_df_desbalance['Estado']

"""### Obtener caracteristicas"""

from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
import xgboost as xgb
from sklearn.feature_selection import mutual_info_classif
from collections import Counter
from skfeature.function.similarity_based import fisher_score
import sklearn_relief as sr

import pandas as pd
from sklearn.model_selection import GroupKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier

def print_top_features(name, top_features):
    print(f"{name}:")
    for feature in top_features:
        print(feature)

# Definir los grupos (en este caso, 'caseid')
#groups = sampled_df_balance['caseid']
groups = subset_df_desbalance['caseid']

# Inicializar el validador cruzado agrupado
gkf = GroupKFold(n_splits=5)


# Prepare models
models = []
#models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='linear')))
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))
models.append(('XGB', xgb.XGBClassifier()))
models.append(('MI', None))

all_features = []
for name, model in models:
# Iterar sobre las divisiones generadas por GroupKFold
    for train_index, test_index in gkf.split(X, y, groups=groups):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        scaler = StandardScaler()

        X_sc = scaler.fit_transform(X_train.values)


        if name in [ 'RFC', 'XGB', 'ETC']:
            model.fit(X_sc, y_train)
            importances = model.feature_importances_
            top_indices = np.argsort(importances)[::-1][:90]
            top_features = [X.columns[i] for i in top_indices]
            print_top_features(name, top_features)
            all_features.extend(top_features)

        elif name == 'MI':
            column_names = X.columns.tolist()
            mutual_info = mutual_info_classif(X_sc, y_train)
            mutual_info = pd.Series(mutual_info)
            mutual_info.index = column_names
            mutual_info_sorted = mutual_info.sort_values(ascending=False)
            feature_names_sorted = mutual_info_sorted.index
            print_top_features(name, feature_names_sorted[:90])
            all_features.extend(feature_names_sorted[:90])
        '''
        elif name == 'SVM':
            svm_classifier = SVC(kernel='linear', C=1)
            svm_classifier.fit(X_sc, y_train)
            features_importance = svm_classifier.coef_[0]
            feature_importance_tuples = [(feature, importance) for feature, importance in zip(X.columns, features_importance)]
            sorted_features = sorted(feature_importance_tuples, key=lambda x: abs(x[1]), reverse=True)
            top_10_features = [feature for feature, _ in sorted_features[:100]]
            print_top_features(name, top_10_features)
            all_features.extend(top_10_features)
        '''

feature_counter = Counter(all_features)
print("Características más comunes:")
for feature, count in feature_counter.most_common():
    print(f"{feature}")

feature_counter = Counter(all_features)

# Selecciona las primeras 10 características más comunes
selec_caract = [feature for feature, _ in feature_counter.most_common(90)]
selec_caract.append('Estado')  # Agrega la columna 'estado' a la lista

# Selecciona las columnas relevantes del DataFrame df_cD8_2 usando las características seleccionadas
#df_principal_features = sampled_df_balance[selec_caract]
df_principal_features = subset_df_desbalance[selec_caract]

"""### Hacer clasificacion"""

columnas_a_excluir = ['Estado']
X = df_principal_features.drop(columns=columnas_a_excluir)
y = df_principal_features['Estado']

caracteristicas = [
                    'Estado','caseid','eeg1_higu_10','eeg1_gamma_pfd','eeg1_alpha_tasa','eeg1_lenta_higu_100','eeg1_gamma_tasa',
                    'Pabs_eeg1_delta','Prel_eeg1_lenta','eeg1_spect_ent_f','eeg1_max','eeg1_delta_std',
                    'eeg1_beta_rms','eeg1_higu_100','eeg1_var','eeg1_beta_tasa','eeg1_beta_entropy_Samp',
                    'eeg1_higu_50','eeg1_beta_higu_10','eeg1_beta_mobility','eeg1_alpha_pfd','eeg1_delta_var',
                    'eeg1_gamma_h','eeg1_lenta_std','eeg1_lenta_rms','eeg1_pfd','eeg1_pow','Pabs_eeg1_lenta',
                    'eeg1_entropy_Samp','eeg1_svd_ent','Prel_eeg1_delta','eeg1_complexity',
                    'eeg1_tasa','eeg1_alpha_higu_10','eeg1_mobility','Pabs_eeg1_beta','eeg1_h','eeg1_lenta_pow',
                    'eeg1_lenta_var','eeg1_alpha_svd_ent','eeg1_katz','eeg1_beta_higu_50',
                    'Pabs_eeg1_theta','eeg1_beta_pfd','Prel_eeg1_beta','eeg1_theta_rms','Prel_eeg1_alpha',
                    'eeg1_std','eeg1_delta_rms','eeg1_lenta_max','eeg1_gamma_higu_10','Pabs_eeg1_gamma',
                    'Prel_eeg1_gamma','eeg1_gamma_rms','eeg1_beta_h','eeg1_alpha_higu_50','eeg1_gamma_spect_ent_f',
                    'eeg1_entropy_ApEn','esp_rms_eeg1_lenta','eeg1_lenta_higu_50','eeg1_gamma_mobility','eeg1_beta_std',
                    'eeg1_curt','eeg1_activity','Pabs_eeg1_alpha','eeg1_delta_pow','eeg1_beta_svd_ent','Prel_eeg1_theta',
                    'eeg1_gamma_curt','eeg1_alpha_mobility','eeg1_delta_activity','eeg1_gamma_var',
                    'eeg1_beta_entropy_ApEn','eeg1_beta_complexity','eeg1_alpha_entropy_Samp','eeg1_lenta_activity',
                    'eeg1_theta_std','eeg1_gamma_activity','eeg1_beta_var','eeg1_beta_curt','eeg1_alpha_complexity','eeg1_beta_higu_100',
                    'eeg1_gamma_std','eeg1_theta_pow','eeg1_gamma_svd_ent','eeg1_gamma_pow','eeg1_beta_activity',
                    'eeg1_beta_pow','eeg1_gamma_complexity','eeg1_alpha_rms','eeg1_lenta_pfd','eeg1_alpha_higu_100',
                    'esp_rms_eeg1_gamma','esp_rms_eeg1_delta','eeg1_theta_activity','esp_rms_eeg1_theta',
                    'eeg1_theta_var','esp_rms_eeg1_alpha','esp_rms_eeg1_beta','eeg1_delta_max','eeg1_alpha_pow','eeg1_alpha_activity'
]
df_principal_features = data_clean[caracteristicas]

"""Caracteristicas Nuevas"""

caracteristicas =['Estado', 'caseid','eeg1_higu_10', 'Prel_eeg1_gamma', 'Prel_eeg1_beta', 'eeg1_entropy_ApEn', 'eeg1_higu_50', 'eeg1_higu_100', 'eeg1_entropy_Samp', 'eeg1_pfd', 'eeg1_alpha_mobility', 'eeg1_alpha_higu_50',
    'eeg1_std', 'eeg1_alpha_higu_10', 'eeg1_var', 'eeg1_beta_entropy_Samp', 'eeg1_alpha_svd_ent', 'eeg1_delta_med_freq', 'eeg1_alpha_tasa', 'eeg1_lenta_rms', 'eeg1_lenta_freq_slope', 'eeg1_delta_var',
    'eeg1_lenta_var_freq', 'eeg1_delta_var_freq', 'eeg1_delta_freq_slope', 'eeg1_lenta_med_freq', 'Pabs_eeg1_delta', 'eeg1_spect_ent_f', 'eeg1_lenta_pow', 'eeg1_lenta_pow_freq', 'eeg1_gamma_pfd', 'Prel_eeg1_lenta',
    'eeg1_h', 'Prel_eeg1_delta', 'eeg1_theta_med_freq', 'eeg1_lenta_max_freq', 'eeg1_beta_h', 'eeg1_theta_freq_slope', 'eeg1_gamma_h', 'eeg1_theta_rms', 'eeg1_lenta_var', 'eeg1_beta_freq_slope',
    'Pabs_eeg1_theta', 'eeg1_theta_var_freq', 'eeg1_freq_median', 'eeg1_alpha_freq_median', 'eeg1_freq_centroid', 'eeg1_lenta_std_freq', 'eeg1_delta_std_freq', 'eeg1_lenta_std', 'eeg1_gamma_entropy_Samp', 'eeg1_beta_med_freq',
    'eeg1_lenta_higu_100', 'eeg1_beta_var_freq', 'eeg1_ske_freq', 'eeg1_complexity', 'eeg1_mobility', 'eeg1_gamma_tasa', 'Prel_eeg1_alpha', 'eeg1_beta_higu_10', 'eeg1_freq_slope', 'eeg1_alpha_pfd',
    'esp_rms_eeg1_lenta', 'eeg1_gamma_mobility', 'eeg1_delta_pow', 'Pabs_eeg1_lenta', 'eeg1_beta_pfd', 'eeg1_theta_var', 'eeg1_delta_max_freq', 'Pabs_eeg1_beta', 'eeg1_beta_mobility', 'eeg1_delta_pow_freq',
    'eeg1_delta_rms', 'eeg1_gamma_svd_ent', 'eeg1_svd_ent', 'eeg1_curt_freq', 'eeg1_beta_tasa', 'eeg1_activity', 'eeg1_lenta_activity', 'esp_rms_eeg1_alpha', 'eeg1_gamma_freq_median', 'eeg1_delta_std',
    'eeg1_theta_pow', 'eeg1_delta_activity', 'eeg1_theta_std', 'eeg1_theta_pow_freq', 'eeg1_beta_rms', 'esp_rms_eeg1_beta', 'eeg1_med_freq', 'eeg1_beta_var', 'eeg1_beta_std_freq', 'eeg1_theta_activity',
    'eeg1_gamma_rms', 'eeg1_beta_pow', 'esp_rms_eeg1_delta', 'eeg1_theta_std_freq', 'eeg1_beta_std', 'eeg1_max', 'eeg1_gamma_freq_slope', 'eeg1_gamma_var_freq', 'eeg1_beta_activity', 'esp_rms_eeg1_gamma',
    'eeg1_beta_entropy_ApEn', 'esp_rms_eeg1_theta', 'eeg1_beta_pow_freq', 'eeg1_gamma_std_freq', 'eeg1_beta_svd_ent', 'eeg1_gamma_freq_centroid', 'eeg1_katz', 'eeg1_beta_freq_median', 'eeg1_alpha_entropy_Samp', 'Pabs_eeg1_gamma',
    'eeg1_gamma_std', 'eeg1_alpha_higu_100', 'eeg1_gamma_med_freq', 'eeg1_delta_max', 'eeg1_alpha_rms', 'eeg1_curt', 'eeg1_beta_higu_50', 'eeg1_lenta_max', 'eeg1_gamma_curt', 'eeg1_gamma_higu_10'             ]
df_principal_features = df_caract[caracteristicas]

df_principal_features

subset_df_desbalance = df_principal_features[df_principal_features['Estado'].isin([0,1,2,3])]

columnas_a_excluir = ['Estado', 'caseid']
X = subset_df_desbalance.drop(columns=columnas_a_excluir)
y = subset_df_desbalance['Estado']

"""####Multiclase"""

from sklearn.model_selection import GroupKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix, roc_auc_score
from sklearn.metrics import cohen_kappa_score

# Definir los grupos (en este caso, 'caseid')
#groups = sampled_df_balance['caseid']
groups = subset_df_desbalance['caseid']

# Inicializar el validador cruzado agrupado
gkf = GroupKFold(n_splits=5)


# Prepare models
models = []
models.append(('k-NN', KNeighborsClassifier(n_neighbors=19)))
models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='linear', probability=True)))  # Set probability=True for SVC
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))  # Add ExtraTreesClassifier
models.append(('SGD', SGDClassifier(max_iter=1000, tol=1e-3)))
models.append(('XGB', xgb.XGBClassifier()))

for name, model in models:
# Iterar sobre las divisiones generadas por GroupKFold
    for train_index, test_index in gkf.split(X, y, groups=groups):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        scaler = StandardScaler()

        X_train_sc = scaler.fit_transform(X_train.values)
        X_test_sc = scaler.transform(X_test.values)

        # Entrenar el clasificador
        model.fit(X_train_sc, y_train)

        # Realizar predicciones en el conjunto de prueba
        y_pred = model.predict(X_test_sc)

        # Calcular métricas de evaluación
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        kappa = cohen_kappa_score(y_test, y_pred)
        matthews_coef = matthews_corrcoef(y_test, y_pred)


        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test_sc)
            auc = roc_auc_score(y_test, y_prob, multi_class='ovr')
        else:
            auc = None

    print(f"{name}")
    print(f"  {accuracy:.2f}", end='    ')
    print(f"  {precision:.2f}", end='    ')
    print(f"  {f1:.2f}", end='    ')
    print(f"  {recall:.2f}", end='    ')
    print(f"  {kappa:.2f}", end='    ')
    print(f"  {matthews_coef:.2f}", end='    ')

    if auc is not None:
        print(f"  {auc:.2f}")
    else:
        print("  AUC: Not applicable (model does not support predict_proba)")

    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each model
    print()

"""####GridSearchCV"""

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, cohen_kappa_score, matthews_corrcoef, classification_report, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GroupKFold

# Definir los grupos (en este caso, 'caseid')
#groups = sampled_df_balance['caseid']
groups = subset_df_desbalance['caseid']

# Inicializar el validador cruzado agrupado
gkf = GroupKFold(n_splits=5)

# Prepare models
models = []
models.append(('k-NN', KNeighborsClassifier()))
models.append(('SVM', SVC(probability=True)))  # Set probability=True for SVC
models.append(('RFC', RandomForestClassifier(random_state=42)))
models.append(('ETC', ExtraTreesClassifier(random_state=42)))  # Add ExtraTreesClassifier
models.append(('SGD', SGDClassifier(max_iter=1000, tol=1e-3)))
models.append(('XGB', xgb.XGBClassifier()))

# Define hyperparameter grids for each model
param_grids = {
    'k-NN': {'n_neighbors': [3, 5, 10, 15, 20]},
    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly']},
    'RFC': {'n_estimators': [50, 100, 200]},
    'ETC': {'n_estimators': [50, 100, 200]},
    'SGD': {'alpha': [0.0001, 0.001, 0.01]},
    'XGB': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7]}
}

for name, model in models:
    param_grid = param_grids.get(name, {})  # Get the hyperparameter grid for the current model
    clf = GridSearchCV(model, param_grid, cv=gkf, scoring='accuracy')

    # Iterar sobre las divisiones generadas por GroupKFold
    for train_index, test_index in gkf.split(X, y, groups=groups):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        scaler = StandardScaler()

        X_train_sc = scaler.fit_transform(X_train.values)
        X_test_sc = scaler.transform(X_test.values)

        # Entrenar el clasificador con búsqueda en cuadrícula
        clf.fit(X_train_sc, y_train)

        # Obtener los mejores hiperparámetros y el mejor modelo
        best_params = clf.best_params_
        best_model = clf.best_estimator_

        # Realizar predicciones en el conjunto de prueba
        y_pred = best_model.predict(X_test_sc)

        # Calcular métricas de evaluación
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        kappa = cohen_kappa_score(y_test, y_pred)
        matthews_coef = matthews_corrcoef(y_test, y_pred)

        if hasattr(best_model, "predict_proba"):
            y_prob = best_model.predict_proba(X_test_sc)
            auc = roc_auc_score(y_test, y_prob, multi_class='ovr')
        else:
            auc = None

        print(f"{name} - Best Parameters: {best_params}")
        print(f"  {accuracy:.2f}", end='    ')
        print(f"  {precision:.2f}", end='    ')
        print(f"  {f1:.2f}", end='    ')
        print(f"  {recall:.2f}", end='    ')
        print(f"  {kappa:.2f}", end='    ')
        print(f"  {matthews_coef:.2f}", end='    ')

        if auc is not None:
            print(f"  {auc:.2f}")
        else:
            print("  AUC: Not applicable (model does not support predict_proba)")

        print("Classification Report:")
        print(classification_report(y_test, y_pred))  # Print classification report for each model
        print()

"""####Voting"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import GroupKFold
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedGroupKFold
# Definir los grupos (en este caso, 'caseid')
groups = subset_df_desbalance['caseid']

# Inicializar el validador cruzado agrupado
gkf = StratifiedGroupKFold(n_splits=5)

# Prepare models
clf1 = KNeighborsClassifier(n_neighbors=20)
clf2 = RandomForestClassifier(n_estimators=10, random_state=42)
clf3 = ExtraTreesClassifier(n_estimators=150, random_state=42)
clf4 = xgb.XGBClassifier()

accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
sensitivity_scores = []
specificity_scores = []
auc_scores = []
matthews_scores = []
kappa_scores = []

for train_index, test_index in gkf.split(X, y, groups=groups):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    scaler = StandardScaler()

    X_train_sc = scaler.fit_transform(X_train.values)
    X_test_sc = scaler.transform(X_test.values)

    # Entrenar el clasificador
    eclf1 = VotingClassifier(estimators=[
        ('KNN', clf1), ('rf', clf2), ('ETR', clf3), ('XGB', clf4)], voting='soft', weights=[1,1,4,1], flatten_transform=True)

    eclf1 = eclf1.fit(X_train_sc, y_train)

    # Realizar predicciones en el conjunto de prueba
    y_pred = eclf1.predict(X_test_sc)

    # Calcular métricas de evaluación
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)
    precision = precision_score(y_test, y_pred, average='weighted')
    precision_scores.append(precision)
    recall = recall_score(y_test, y_pred, average='weighted')
    recall_scores.append(recall)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_scores.append(f1)
    kappa = cohen_kappa_score(y_test, y_pred)
    kappa_scores.append(kappa)
    matthews_coef = matthews_corrcoef(y_test, y_pred)
    matthews_scores.append(matthews_coef)

    y_pred_prob = eclf1.predict_proba(X_test_sc)
    auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')
    auc_scores.append(auc)

    print("Métricas de evaluación:")
    print(f"{accuracy:.2f}")
    print(f"{precision:.2f}")
    print(f"{recall:.2f}")
    print(f"{f1:.2f}")
    print(f"{kappa:.2f}")
    print(f"{matthews_coef:.2f}")
    print(f"{auc:.2f}")
    print()

    # Mostrar la matriz de confusión
    conf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicciones')
    plt.ylabel('Clases reales')
    plt.title('Matriz de Confusión')
    plt.show()

    # Mostrar el classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each fold
    print()

# Print average metrics
print('Average Metrics:')
print(f'{np.mean(accuracy_scores):.2f}')
print(f'{np.mean(precision_scores):.2f}')
print(f'{np.mean(f1_scores):.2f}')
print(f'{np.mean(recall_scores):.2f}')
print(f'{np.mean(kappa_scores):.2f}')
print(f'{np.mean(matthews_scores):.2f}')
print(f'{np.mean(auc_scores):.2f}')

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
import numpy as np
import pandas as pd

# Definir los grupos (en este caso, 'caseid')
groups = subset_df_desbalance['caseid']

# Inicializar el validador cruzado agrupado
gkf = StratifiedGroupKFold(n_splits=5)

# Prepare models
clf1 = KNeighborsClassifier(n_neighbors=20)
clf2 = RandomForestClassifier(n_estimators=10, random_state=42)
clf3 = ExtraTreesClassifier(n_estimators=150, random_state=42)
clf4 = xgb.XGBClassifier()

accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
sensitivity_scores = []
specificity_scores = []
auc_scores = []
matthews_scores = []
kappa_scores = []

conf_matrices = []

for train_index, test_index in gkf.split(X, y, groups=groups):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    scaler = StandardScaler()

    X_train_sc = scaler.fit_transform(X_train.values)
    X_test_sc = scaler.transform(X_test.values)

    # Entrenar el clasificador
    eclf1 = VotingClassifier(estimators=[
        ('KNN', clf1), ('rf', clf2), ('ETR', clf3), ('XGB', clf4)],
        voting='soft', weights=[1, 1, 4, 1], flatten_transform=True)

    eclf1 = eclf1.fit(X_train_sc, y_train)

    # Realizar predicciones en el conjunto de prueba
    y_pred = eclf1.predict(X_test_sc)

    # Calcular métricas de evaluación
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)
    precision = precision_score(y_test, y_pred, average='weighted')
    precision_scores.append(precision)
    recall = recall_score(y_test, y_pred, average='weighted')
    recall_scores.append(recall)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_scores.append(f1)
    kappa = cohen_kappa_score(y_test, y_pred)
    kappa_scores.append(kappa)
    matthews_coef = matthews_corrcoef(y_test, y_pred)
    matthews_scores.append(matthews_coef)

    y_pred_prob = eclf1.predict_proba(X_test_sc)[:, 1]
    auc = roc_auc_score(y_test, y_pred_prob)
    auc_scores.append(auc)

    # Mostrar la matriz de confusión
    conf_matrix = confusion_matrix(y_test, y_pred)
    conf_matrices.append(conf_matrix)

    print("Métricas de evaluación:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1:.2f}")
    print(f"Cohen's Kappa: {kappa:.2f}")
    print(f"Matthews Correlation Coefficient: {matthews_coef:.2f}")
    print(f"ROC AUC: {auc:.2f}")
    print()

    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicciones')
    plt.ylabel('Clases reales')
    plt.title('Matriz de Confusión')
    plt.show()

    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each fold
    print()

# Sumar todas las matrices de confusión para obtener la matriz de confusión total
total_conf_matrix = np.sum(conf_matrices, axis=0)

print("Overall Confusion Matrix:")
plt.figure(figsize=(8, 6))
sns.heatmap(total_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicciones')
plt.ylabel('Clases reales')
plt.title('Matriz de Confusión Acumulada')
plt.show()

print('Average Metrics:')
print(f'Accuracy: {np.mean(accuracy_scores):.2f}')
print(f'Precision: {np.mean(precision_scores):.2f}')
print(f'Recall: {np.mean(recall_scores):.2f}')
print(f'F1-Score: {np.mean(f1_scores):.2f}')
print(f"Cohen's Kappa: {np.mean(kappa_scores):.2f}")
print(f'Matthews Correlation Coefficient: {np.mean(matthews_scores):.2f}')
print(f'ROC AUC: {np.mean(auc_scores):.2f}')

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
import numpy as np
import pandas as pd

# Definir los grupos (en este caso, 'caseid')
groups = subset_df_desbalance['caseid']

# Inicializar el validador cruzado agrupado
gkf = StratifiedGroupKFold(n_splits=5)

# Prepare models
clf1 = KNeighborsClassifier(n_neighbors=20)
clf2 = RandomForestClassifier(n_estimators=10, random_state=42)
clf3 = ExtraTreesClassifier(n_estimators=150, random_state=42)
clf4 = xgb.XGBClassifier()

accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
sensitivity_scores = []
specificity_scores = []
auc_scores = []
matthews_scores = []
kappa_scores = []

conf_matrices = []

for train_index, test_index in gkf.split(X, y, groups=groups):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    scaler = StandardScaler()

    X_train_sc = scaler.fit_transform(X_train.values)
    X_test_sc = scaler.transform(X_test.values)

    # Entrenar el clasificador
    eclf1 = VotingClassifier(estimators=[
        ('KNN', clf1), ('rf', clf2), ('ETR', clf3), ('XGB', clf4)],
        voting='soft', weights=[1, 1, 4, 1], flatten_transform=True)

    eclf1 = eclf1.fit(X_train_sc, y_train)

    # Realizar predicciones en el conjunto de prueba
    y_pred = eclf1.predict(X_test_sc)

    # Calcular métricas de evaluación
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)
    precision = precision_score(y_test, y_pred, average='weighted')
    precision_scores.append(precision)
    recall = recall_score(y_test, y_pred, average='weighted')
    recall_scores.append(recall)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_scores.append(f1)
    kappa = cohen_kappa_score(y_test, y_pred)
    kappa_scores.append(kappa)
    matthews_coef = matthews_corrcoef(y_test, y_pred)
    matthews_scores.append(matthews_coef)

    y_pred_prob = eclf1.predict_proba(X_test_sc)[:, 1]
    auc = roc_auc_score(y_test, y_pred_prob)
    auc_scores.append(auc)

    # Mostrar la matriz de confusión
    conf_matrix = confusion_matrix(y_test, y_pred)
    conf_matrices.append(conf_matrix)

    print("Métricas de evaluación:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1:.2f}")
    print(f"Cohen's Kappa: {kappa:.2f}")
    print(f"Matthews Correlation Coefficient: {matthews_coef:.2f}")
    print(f"ROC AUC: {auc:.2f}")
    print()

    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicciones')
    plt.ylabel('Clases reales')
    plt.title('Matriz de Confusión')
    plt.show()

    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each fold
    print()

# Sumar todas las matrices de confusión para obtener la matriz de confusión total
total_conf_matrix = np.sum(conf_matrices, axis=0)

print("Overall Confusion Matrix:")
plt.figure(figsize=(8, 6))
sns.heatmap(total_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicciones')
plt.ylabel('Clases reales')
plt.title('Matriz de Confusión Acumulada')
plt.show()

print('Average Metrics:')
print(f'Accuracy: {np.mean(accuracy_scores):.2f}')
print(f'Precision: {np.mean(precision_scores):.2f}')
print(f'Recall: {np.mean(recall_scores):.2f}')
print(f'F1-Score: {np.mean(f1_scores):.2f}')
print(f"Cohen's Kappa: {np.mean(kappa_scores):.2f}")
print(f'Matthews Correlation Coefficient: {np.mean(matthews_scores):.2f}')
print(f'ROC AUC: {np.mean(auc_scores):.2f}')

"""####Red Neuronal"""

from sklearn.model_selection import StratifiedGroupKFold
from keras.utils import to_categorical
from keras.models import Model
# Prepare models
sgkf = StratifiedGroupKFold(n_splits=5)
groups = subset_df_desbalance['caseid']

accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
sensitivity_scores = []
specificity_scores = []
auc_scores = []
matthews_scores = []
kappa_scores = []

# Model training and evaluation
for train_index, test_index in sgkf.split(X, y, groups=groups):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train_one_hot = to_categorical(y.iloc[train_index], num_classes=4)
    y_test_one_hot = to_categorical(y.iloc[test_index], num_classes=4)

    # Prepare data
    scaler = MinMaxScaler()
    X_train_sc = scaler.fit_transform(X_train)
    X_test_sc = scaler.transform(X_test)

    '''
    # Compilar el modelo 1
    model = Sequential()
    model.add(Dense(128, input_dim=X_train_sc.shape[1], activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(4, activation='softmax'))  # Para problemas de clasificación con 3 clases


    # Compilar el modelo 2
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train_sc, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2)

    input_layer = Input(shape=(X_train_sc.shape[1],))
    dense_layer_1 = Dense(128, activation='relu')(input_layer)
    dropout_layer_1 = Dropout(0.2)(dense_layer_1)
    dense_layer_2 = Dense(64, activation='relu')(dropout_layer_1)
    dropout_layer_2 = Dropout(0.2)(dense_layer_2)
    output_layer = Dense(4, activation='softmax')(dropout_layer_2)
    '''
    input_layer = Input(shape=(X_train_sc.shape[1],))
    dense_layer_1 = Dense(128, activation='relu')(input_layer)
    dense_layer_2 = Dense(64, activation='relu')(dense_layer_1)
    dense_layer_3 = Dense(32, activation='relu')(dense_layer_2)
    dense_layer_4 = Dense(64, activation='relu')(dense_layer_3)
    output_layer = Dense(4, activation='softmax')(dense_layer_4)
    # Define the model
    model = Model(inputs=input_layer, outputs=output_layer)

    # Compile and fit the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train_sc, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2)

    # Evaluate the model on the test set
    y_pred_prob = model.predict(X_test_sc)
    y_pred = np.argmax(y_pred_prob, axis=1)

    # Calculate evaluation metrics
    accuracy = accuracy_score(np.argmax(y_test_one_hot, axis=1), y_pred)
    precision = precision_score(np.argmax(y_test_one_hot, axis=1), y_pred, average='weighted')
    recall = recall_score(np.argmax(y_test_one_hot, axis=1), y_pred, average='weighted')
    f1 = f1_score(np.argmax(y_test_one_hot, axis=1), y_pred, average='weighted')
    mcc = matthews_corrcoef(np.argmax(y_test_one_hot, axis=1), y_pred)
    kappa = cohen_kappa_score(np.argmax(y_test_one_hot, axis=1), y_pred)
    auc = roc_auc_score(y_test_one_hot, y_pred_prob, average='weighted', multi_class='ovo')

    accuracy_scores.append(accuracy)
    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)
    matthews_scores.append(mcc)
    kappa_scores.append(kappa)
    auc_scores.append(auc)

    print(f'{accuracy:.2f}')
    print(f'{precision:.2f}')
    print(f'{recall:.2f}')
    print(f'{f1:.2f}')
    print(f'{mcc:.2f}')
    print(f'{kappa:.2f}')
    print(f'{auc:.2f}')
    print(classification_report(np.argmax(y_test_one_hot, axis=1), np.argmax(y_pred_prob, axis=1)))
# Print average metrics
print('Average Metrics:')
print(f'{np.mean(accuracy_scores):.2f}')
print(f'{np.mean(precision_scores):.2f}')
print(f'{np.mean(f1_scores):.2f}')
print(f'{np.mean(recall_scores):.2f}')
print(f'{np.mean(kappa_scores):.2f}')
print(f'{np.mean(matthews_scores):.2f}')
print(f'{np.mean(auc_scores):.2f}')

from keras.utils import to_categorical
from keras.models import Model
from sklearn.model_selection import GroupKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix, roc_auc_score
from sklearn.metrics import cohen_kappa_score
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Convertir las etiquetas a representación one-hot
y_train_one_hot = to_categorical(y_train)
y_test_one_hot = to_categorical(y_test)

scaler = MinMaxScaler()
X_train_sc = scaler.fit_transform(X_train)
X_test_sc = scaler.transform(X_test)

input_layer = Input(shape=(X_train_sc.shape[1],))
dense_layer_1 = Dense(128, activation='relu')(input_layer)
dense_layer_2 = Dense(64, activation='relu')(dense_layer_1)
dense_layer_3 = Dense(32, activation='relu')(dense_layer_2)
dense_layer_4 = Dense(64, activation='relu')(dense_layer_3)
output_layer = Dense(4, activation='softmax')(dense_layer_4)
# Define the model
model = Model(inputs=input_layer, outputs=output_layer)

# Compile and fit the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train_sc, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.3)

# Evaluate the model on the test set
y_pred_prob = model.predict(X_test_sc)
y_pred = np.argmax(y_pred_prob, axis=1)

# Calculate evaluation metrics
accuracy = accuracy_score(np.argmax(y_test_one_hot, axis=1), y_pred)
precision = precision_score(np.argmax(y_test_one_hot, axis=1), y_pred, average='weighted')
recall = recall_score(np.argmax(y_test_one_hot, axis=1), y_pred, average='weighted')
f1 = f1_score(np.argmax(y_test_one_hot, axis=1), y_pred, average='weighted')
mcc = matthews_corrcoef(np.argmax(y_test_one_hot, axis=1), y_pred)
kappa = cohen_kappa_score(np.argmax(y_test_one_hot, axis=1), y_pred)
auc = roc_auc_score(y_test_one_hot, y_pred_prob, average='weighted', multi_class='ovo')

print(f'{accuracy:.2f}')
print(f'{precision:.2f}')
print(f'{recall:.2f}')
print(f'{f1:.2f}')
print(f'{mcc:.2f}')
print(f'{kappa:.2f}')
print(f'{auc:.2f}')
print(classification_report(np.argmax(y_test_one_hot, axis=1), np.argmax(y_pred_prob, axis=1)))



"""####Redes CNN"""

from math import sqrt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
from numpy import array
from numpy import hstack
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Conv1D
from keras.layers import MaxPooling1D

#%% split a multivariate sequence into samples
def split_sequences(sequences, n_steps_in, n_steps_out):
    X, y = list(), list()
    for i in range(len(sequences)):
        # find the end of this pattern
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out-1
        # check if we are beyond the dataset
        if out_end_ix > len(sequences):
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)

# split a univariate dataset into train/test sets
def train_test_split(data, n_test):
    return data[:-n_test], data[-n_test:]


in_seqs = df_principal_features.drop(columns=['Estado']).values
#in_seqs = in_seqs.astype(int)
# Aquí se toma la columna 'bis_median'
out_seq = df_principal_features['Estado'].values

# Convertir a estructura [filas, columnas]
in_seqs = np.reshape(in_seqs, (len(in_seqs), -1))
out_seq = np.reshape(out_seq, (len(out_seq), -1))

# Concatenar horizontalmente las columnas
dataset = np.hstack((in_seqs, out_seq))

n_test = int(0.3*len(dataset))
train, test = train_test_split(dataset, n_test)


# choose a number of time steps
n_steps_in, n_steps_out = 3, 1
# convert into input/output
X_train, y_train = split_sequences(train, n_steps_in, n_steps_out)
X_test, y_test   = split_sequences(test, n_steps_in, n_steps_out)
# the dataset knows the number of features, e.g. 2
n_features = X_train.shape[2]

print(X_test.shape, y_test.shape)
# summarize the data
for i in range(len(X_test)):
  print(X_test[i], y_test[i])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Número de clases
num_classes = 4

# Definir el modelo para clasificación
model = Sequential()
model.add(Input(shape=(n_steps_in, n_features)))
model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
# Cambiar la última capa para clasificación con softmax y ajustar el número de clases
model.add(Dense(num_classes, activation='softmax'))
# Usar 'categorical_crossentropy' para clasificación
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Convertir las etiquetas a formato categórico
y_train_cat = to_categorical(y_train, num_classes=num_classes)
y_test_cat = to_categorical(y_test, num_classes=num_classes)

# Entrenar el modelo
model.fit(X_train, y_train_cat, epochs=2000, verbose=0)

# Validación del modelo
predictions = []
history = []

for i in range(len(X_test)):
    x_input_ = X_test[i]
    x_input_ = x_input_.reshape((1, n_steps_in, n_features))
    history.append(x_input_)

for i in range(len(y_test)):
    yhat = model.predict(history[i], verbose=0)
    predictions.append(yhat)

# Convertir las predicciones a clases
predictions = np.argmax(predictions, axis=-1)
y_test = np.argmax(y_test_cat, axis=-1)

# Evaluar el modelo
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy}')

print('Classification Report:')
print(classification_report(y_test, predictions))



"""####Binaria"""

from sklearn.model_selection import GroupKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef, roc_auc_score, cohen_kappa_score

# Definir los grupos (en este caso, 'caseid')
#groups = sampled_df_balance['caseid']
groups = subset_df_desbalance['caseid']

# Inicializar el validador cruzado agrupado
gkf = GroupKFold(n_splits=5)

# Prepare models
models = []
models.append(('k-NN', KNeighborsClassifier(n_neighbors=19)))
models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='linear', probability=True)))  # Set probability=True for SVC
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))  # Add ExtraTreesClassifier
models.append(('SGD', SGDClassifier(max_iter=1000, tol=1e-3)))
models.append(('XGB', xgb.XGBClassifier()))

for name, model in models:
    accuracy_values = []
    precision_values = []
    recall_values = []
    f1_values = []
    kappa_values = []
    matthews_coef_values = []
    auc_values = []

    # Iterar sobre las divisiones generadas por GroupKFold
    for train_index, test_index in gkf.split(X, y, groups=groups):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        scaler = StandardScaler()
        X_train_sc = scaler.fit_transform(X_train.values)
        X_test_sc = scaler.transform(X_test.values)

        # Entrenar el clasificador
        model.fit(X_train_sc, y_train)

        # Realizar predicciones en el conjunto de prueba
        y_pred = model.predict(X_test_sc)

        # Calcular métricas de evaluación
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        kappa = cohen_kappa_score(y_test, y_pred)
        matthews_coef = matthews_corrcoef(y_test, y_pred)

        accuracy_values.append(accuracy)
        precision_values.append(precision)
        recall_values.append(recall)
        f1_values.append(f1)
        kappa_values.append(kappa)
        matthews_coef_values.append(matthews_coef)

        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test_sc)[:, 1]  # Use only the probability for the positive class
            auc = roc_auc_score(y_test, y_prob)
            auc_values.append(auc)

    print(f"{name}")
    print(f"  Accuracy: {sum(accuracy_values) / len(accuracy_values):.2f}")
    print(f"  Precision: {sum(precision_values) / len(precision_values):.2f}")
    print(f"  Recall: {sum(recall_values) / len(recall_values):.2f}")
    print(f"  F1-Score: {sum(f1_values) / len(f1_values):.2f}")
    print(f"  Cohen's Kappa: {sum(kappa_values) / len(kappa_values):.2f}")
    print(f"  Matthews Correlation Coefficient: {sum(matthews_coef_values) / len(matthews_coef_values):.2f}")

    if auc_values:
        print(f"  ROC AUC: {sum(auc_values) / len(auc_values):.2f}")
    else:
        print("  ROC AUC: Not applicable (model does not support predict_proba)")

    print()

    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each model
    print()

    conf_matrix = confusion_matrix(y_test, y_pred)
    print(conf_matrix)

data_clean.groupby('Estado').size()

"""###StratifiedGroupKFold para datos desbalanceados y balanceados"""

from sklearn.model_selection import StratifiedGroupKFold
from sklearn.model_selection import GroupKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix, roc_auc_score
from sklearn.metrics import cohen_kappa_score

# Prepare models
models = []
models.append(('k-NN', KNeighborsClassifier(n_neighbors=19)))
models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='linear', probability=True)))  # Set probability=True for SVC
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))  # Add ExtraTreesClassifier
#models.append(('SGD', SGDClassifier(max_iter=1000, tol=1e-3)))
models.append(('XGB', xgb.XGBClassifier()))

sgkf = StratifiedGroupKFold(n_splits=5)

#groups = sampled_df_balance['caseid']
groups = subset_df_desbalance['caseid']
for name, model in models:
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    f1_scores = []
    sensitivity_scores = []
    specificity_scores = []
    auc_scores = []
    matthews_scores = []
    kappa_scores = []

    for train, test in sgkf.split(X, y, groups=groups):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train = y.iloc[train_index].values.ravel()
        y_test = y.iloc[test_index].values.ravel()
        # Prepare data
        scaler = StandardScaler()
        X_train_sc = scaler.fit_transform(X_train)
        X_test_sc = scaler.transform(X_test)

        model.fit(X_train_sc, y_train)
        y_pred = model.predict(X_test_sc)

        # Calcular y almacenar la precisión
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

        # Calcular y almacenar la precisión, recall y f1-score
        precision = precision_score(y_test, y_pred, average='weighted')
        precision_scores.append(precision)

        recall = recall_score(y_test, y_pred, average='weighted')
        recall_scores.append(recall)

        f1 = f1_score(y_test, y_pred, average='weighted')
        f1_scores.append(f1)

        # Calcular y almacenar la matriz de confusión
        conf_matrix = confusion_matrix(y_test, y_pred)
        tp = conf_matrix[1, 1]
        fn = conf_matrix[1, 0]
        sensitivity = tp / (tp + fn)
        sensitivity_scores.append(sensitivity)

        # Calcular y almacenar la especificidad
        tn = conf_matrix[0, 0]
        fp = conf_matrix[0, 1]
        specificity = tn / (tn + fp)
        specificity_scores.append(specificity)

        # Calcular y almacenar el AUC (área bajo la curva)
        #auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], multi_class='ovr')
        # Calcular y almacenar el AUC
        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test_sc)
            auc = roc_auc_score(y_test, y_prob, multi_class='ovr')
            auc_scores.append(auc)
        else:
            auc_scores.append(None)

        # Calcular y almacenar el coeficiente de correlación de Matthews
        matthews_coef = matthews_corrcoef(y_test, y_pred)
        matthews_scores.append(matthews_coef)

        kappa = cohen_kappa_score(y_test, y_pred)
        kappa_scores.append(kappa)

    print(f'{name}')
    print(f'{np.mean(accuracy_scores):.2f}', end='    ')
    print(f'{np.mean(precision_scores):.2f}', end='    ')
    print(f'{np.mean(f1_scores):.2f}', end='    ')
    print(f'{np.mean(recall_scores):.2f}', end='    ')
    print(f'{np.mean(kappa_scores):.2f}', end='    ')
    print(f'{np.mean(matthews_scores):.2f}', end='    ')
    print(f'{np.mean(auc_scores):.2f}')
    print()
    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each model
    print()




    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicciones')
    plt.ylabel('Clases reales')
    plt.title('Matriz de Confusión')
    plt.show()

from sklearn.model_selection import StratifiedGroupKFold

# Prepare models
models = []
models.append(('k-NN', KNeighborsClassifier(n_neighbors=19)))
models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='linear', probability=True)))  # Set probability=True for SVC
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))  # Add ExtraTreesClassifier
models.append(('SGD', SGDClassifier(max_iter=1000, tol=1e-3)))
models.append(('XGB', xgb.XGBClassifier()))

sgkf = StratifiedGroupKFold(n_splits=5)

#groups = sampled_df_balance['caseid']
groups = subset_df_desbalance['caseid']

for name, model in models:
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    f1_scores = []
    sensitivity_scores = []
    specificity_scores = []
    auc_scores = []
    matthews_scores = []
    kappa_scores = []

    for train, test in sgkf.split(X, y, groups=groups):
        X_train, X_test = X.iloc[train], X.iloc[test]
        y_train = y.iloc[train].values.ravel()
        y_test = y.iloc[test].values.ravel()

        # Prepare data
        scaler = StandardScaler()
        X_train_sc = scaler.fit_transform(X_train)
        X_test_sc = scaler.transform(X_test)

        model.fit(X_train_sc, y_train)
        y_pred = model.predict(X_test_sc)

        # Calcular y almacenar la precisión
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

        # Calcular y almacenar la precisión, recall y f1-score
        precision = precision_score(y_test, y_pred, average='weighted')
        precision_scores.append(precision)

        recall = recall_score(y_test, y_pred, average='weighted')
        recall_scores.append(recall)

        f1 = f1_score(y_test, y_pred, average='weighted')
        f1_scores.append(f1)

        # Calcular y almacenar la matriz de confusión
        conf_matrix = confusion_matrix(y_test, y_pred)
        tp = conf_matrix[1, 1]
        fn = conf_matrix[1, 0]
        sensitivity = tp / (tp + fn)
        sensitivity_scores.append(sensitivity)

        # Calcular y almacenar la especificidad
        tn = conf_matrix[0, 0]
        fp = conf_matrix[0, 1]
        specificity = tn / (tn + fp)
        specificity_scores.append(specificity)

        # Calcular y almacenar el AUC
        y_prob = model.predict_proba(X_test_sc)
        auc = roc_auc_score(y_test, y_prob, multi_class='ovr')
        auc_scores.append(auc)

        # Calcular y almacenar el coeficiente de correlación de Matthews
        matthews_coef = matthews_corrcoef(y_test, y_pred)
        matthews_scores.append(matthews_coef)

        kappa = cohen_kappa_score(y_test, y_pred)
        kappa_scores.append(kappa)

    print(f'{name}')
    print(f'{np.mean(accuracy_scores):.2f}', end='    ')
    print(f'{np.mean(precision_scores):.2f}', end='    ')
    print(f'{np.mean(f1_scores):.2f}', end='    ')
    print(f'{np.mean(recall_scores):.2f}', end='    ')
    print(f'{np.mean(kappa_scores):.2f}', end='    ')
    print(f'{np.mean(matthews_scores):.2f}', end='    ')
    print(f'{np.mean(auc_scores):.2f}')
    print()



"""##Independiente del paciente

###Desbalanceada
"""

columnas_a_excluir = ['Estado', 'caseid']
subset_df_desbalance = data_clean[data_clean['Estado'].isin([0,1,2,3])]

"""###Deinir X, y"""

columnas_a_excluir = ['Estado', 'caseid']

X = subset_df_desbalance.drop(columns=columnas_a_excluir)
y = subset_df_desbalance['Estado']

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler



# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Calculate the explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_

# Create a 2x1 grid of subplots
fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))

# Plot the explained variance ratio in the first subplot
ax1.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)
ax1.set_xlabel("Principal Component")
ax1.set_ylabel("Explained Variance Ratio")
ax1.set_title("Explained Variance Ratio by Principal Component")

# Calculate the cumulative explained variance
cumulative_explained_variance = np.cumsum(explained_variance_ratio)

# Plot the cumulative explained variance in the second subplot
ax2.plot(
    range(1, len(cumulative_explained_variance) + 1),
    cumulative_explained_variance,
    marker="o",
)
ax2.set_xlabel("Number of Principal Components")
ax2.set_ylabel("Cumulative Explained Variance")
ax2.set_title("Cumulative Explained Variance by Principal Components")

# Display the figure
plt.tight_layout()
plt.show()

"""###Obtener caracteristicas"""

from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
import xgboost as xgb
from sklearn.feature_selection import mutual_info_classif
from collections import Counter
from skfeature.function.similarity_based import fisher_score
import sklearn_relief as sr

def print_top_features(name, top_features):
    print(f"{name}:")
    for feature in top_features:
        print(feature)

# Prepare data
scaler = StandardScaler()
X_sc = scaler.fit_transform(X.values)
#y_corrected = y - 1
y_corrected = y
all_features = []
# Prepare models
models = []
#models.append(('DTC', DecisionTreeClassifier()))
models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='linear')))
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))
models.append(('XGB', xgb.XGBClassifier()))
models.append(('MI', None))
#models.append(('FisherScore', None))
#models.append(('Relief', None))
# Evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
    #if name in ['DTC', 'RFC', 'XGB']:
    if name in [ 'RFC', 'XGB', 'ETC']:
        model.fit(X_sc, y_corrected)
        importances = model.feature_importances_
        top_indices = np.argsort(importances)[::-1][:100]
        top_features = [X.columns[i] for i in top_indices]
        print_top_features(name, top_features)
        all_features.extend(top_features)


    elif name == 'MI':
        column_names = X.columns.tolist()
        mutual_info = mutual_info_classif(X_sc, y_corrected)
        mutual_info = pd.Series(mutual_info)
        mutual_info.index = column_names
        mutual_info_sorted = mutual_info.sort_values(ascending=False)
        feature_names_sorted = mutual_info_sorted.index
        print_top_features(name, feature_names_sorted[:100])
        all_features.extend(feature_names_sorted[:100])


    elif name == 'SVM':
        svm_classifier = SVC(kernel='linear', C=1)
        svm_classifier.fit(X_sc, y_corrected)
        features_importance = svm_classifier.coef_[0]
        feature_importance_tuples = [(feature, importance) for feature, importance in zip(X.columns, features_importance)]
        sorted_features = sorted(feature_importance_tuples, key=lambda x: abs(x[1]), reverse=True)
        top_10_features = [feature for feature, _ in sorted_features[:100]]
        print_top_features(name, top_10_features)
        all_features.extend(top_10_features)

    '''

    elif name == 'FisherScore':
        ranks = fisher_score.fisher_score(X_sc, y_corrected.values)
        feature_importances = pd.Series(ranks, index=X.columns)
        feature_importances_sorted = feature_importances.sort_values(ascending=False)
        feature_names_sorted = feature_importances_sorted.index
        print_top_features(name, feature_names_sorted[:20])
        all_features.extend(feature_names_sorted[:20])
    elif name == 'Relief':
        r = sr.RReliefF(n_features=10)
        y_corrected_reset_index = y_corrected.reset_index(drop=True)
        r.fit(X_sc, y_corrected_reset_index)
        feature_importances = r.w_
        top_indices = np.argsort(feature_importances)[::-1][:20]
        top_features = [X.columns[i] for i in top_indices]
        print_top_features(name, top_features)
        all_features.extend(top_features)
    '''

from sklearn.model_selection import GridSearchCV
def print_top_features(name, top_features):
    print(f"{name}:")
    for feature in top_features:
        print(feature)

# Prepare data
scaler = StandardScaler()
X_sc = scaler.fit_transform(X.values)
#y_corrected = y - 1
y_corrected = y
all_features = []
# Prepare models
models = []
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))
models.append(('XGB', xgb.XGBClassifier()))
models.append(('MI', None))

# Evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
    #if name in ['DTC', 'RFC', 'XGB']:
    if name in [ 'RFC', 'XGB', 'ETC']:
        print(f"Grid search for {name}...")
        # Define parameter grid for the model
        if name == 'RFC':
            param_grid = {'n_estimators': [50, 100, 150]}
        elif name == 'XGB':
            param_grid = {'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.01, 0.001]}
        elif name == 'ETC':
            param_grid = {'n_estimators': [50, 100, 150]}

        # Perform grid search
        grid_search = GridSearchCV(model, param_grid, cv=5, scoring=scoring)
        grid_search.fit(X_sc, y_corrected)

        best_params = grid_search.best_params_
        best_model = grid_search.best_estimator_

        # Extract top features
        importances = best_model.feature_importances_
        top_indices = np.argsort(importances)[::-1][:100]
        top_features = [X.columns[i] for i in top_indices]
        print_top_features(name, top_features)
        all_features.extend(top_features)

    elif name == 'MI':
        column_names = X.columns.tolist()
        mutual_info = mutual_info_classif(X_sc, y_corrected)
        mutual_info = pd.Series(mutual_info)
        mutual_info.index = column_names
        mutual_info_sorted = mutual_info.sort_values(ascending=False)
        feature_names_sorted = mutual_info_sorted.index
        print_top_features(name, feature_names_sorted[:100])
        all_features.extend(feature_names_sorted[:100])

from collections import Counter

# Lista de elementos
lista = [
    "spect_ent_f", "higu_100", "pow", "higu_10", "tasa", "entropy_Samp", "svd_ent", "higu_50", "mobility",
    "entropy_ApEn", "max", "pfd", "complexity", "curt", "h", "std", "var", "pow", "Pabs", "higu_100", "pfd",
    "higu_50", "Prel", "max", "rms", "svd_ent", "activity", "entropy_Samp", "min", "median", "std", "pow", "var",
    "Pabs", "Prel", "rms", "median", "activity", "higu_50", "max", "entropy_Samp", "complexity", "esp_rms", "ske",
    "curt", "Pabs", "esp_rms", "entropy_ApEn", "Prel", "complexity", "curt", "entropy_Samp", "h", "rms", "max",
    "var", "activity", "pow", "higu_50", "svd_ent", "Pabs", "higu_10", "pfd", "tasa", "esp_rms", "h", "spect_ent_f",
    "entropy_Samp", "higu_50", "pow", "var", "entropy_ApEn", "complexity", "Prel", "svd_ent", "tasa", "entropy_Samp",
    "entropy_ApEn", "mobility", "higu_10", "svd_ent", "higu_50", "esp_rms", "Prel", "pfd", "max", "higu_100",
    "complexity", "h", "curt", "spect_ent_f", "pfd", "tasa", "mobility", "svd_ent", "complexity", "entropy_ApEn",
    "std", "rms", "entropy_Samp", "esp_rms", "Pabs", "var", "curt", "higu_50", "Prel"
]

# Contar la frecuencia de cada elemento
frecuencia = Counter(lista)

# Extraer elementos y frecuencias
elementos = list(frecuencia.keys())
frecuencias = list(frecuencia.values())

# Crear el gráfico de barras
plt.figure(figsize=(10, 6))
plt.bar(elementos, frecuencias, color='skyblue')
plt.xlabel('Elemento')
plt.ylabel('Frecuencia')
plt.title('Frecuencia de elementos')
plt.xticks(rotation=90)  # Rotar etiquetas del eje x para mejor visualización
plt.tight_layout()
plt.show()

feature_counter = Counter(all_features)
print("Características más comunes:")
for feature, count in feature_counter.most_common():
    print(f"{feature} : {count}")

feature_counter = Counter(all_features)

# Selecciona las primeras 10 características más comunes
selec_caract = [feature for feature, _ in feature_counter.most_common(100)]
selec_caract.append('Estado')  # Agrega la columna 'estado' a la lista

# Selecciona las columnas relevantes del DataFrame df_cD8_2 usando las características seleccionadas
#df_principal_features = sampled_df_balance[selec_caract]
df_principal_features = subset_df_desbalance[selec_caract]

df_principal_features

"""###Hacer clasificacion"""

columnas_a_excluir = ['Estado']
X = df_principal_features.drop(columns=columnas_a_excluir)
y = df_principal_features['Estado']

from sklearn.metrics import classification_report
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import xgboost as xgb  # Import XGBoost

#y_corrected = y - 1
# Prepare data
scaler = StandardScaler()
X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.3, random_state=42)
X_train_sc = scaler.fit_transform(X_train.values)
X_test_sc = scaler.transform(X_test.values)  # Only transform for test set

# Prepare models
models = []
models.append(('k-NN', KNeighborsClassifier(n_neighbors=19)))
#models.append(('DTC', DecisionTreeClassifier()))
#models.append(('NB', GaussianNB()))
#models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='linear', probability=True)))  # Set probability=True for SVC
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))  # Add ExtraTreesClassifier
models.append(('SGD', SGDClassifier(max_iter=1000, tol=1e-3)))
models.append(('XGB', xgb.XGBClassifier()))
# Evaluate each model
for name, model in models:
    model.fit(X_train_sc, y_train)
    y_pred = model.predict(X_test_sc)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    kappa = cohen_kappa_score(y_test, y_pred)
    matthews_coef = matthews_corrcoef(y_test, y_pred)


    if hasattr(model, "predict_proba"):  # Check if the model has predict_proba method
        y_prob = model.predict_proba(X_test_sc)
        auc = roc_auc_score(y_test, y_prob, multi_class='ovr')  # Compute AUC using predict_proba
    else:
        auc = None

    print(f"{name}")
    print(f"  {accuracy:.2f}", end='    ')
    print(f"  {precision:.2f}", end='    ')
    print(f"  {f1:.2f}", end='    ')
    print(f"  {recall:.2f}", end='    ')
    print(f"  {kappa:.2f}", end='    ')
    print(f"  {matthews_coef:.2f}", end='    ')

    if auc is not None:
        print(f"  {auc:.2f}")
    else:
        print("  AUC: Not applicable (model does not support predict_proba)")

    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each model
    print()

"""###GridSearchCV"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import xgboost as xgb  # Import XGBoost

#y_corrected = y - 1
# Prepare data
scaler = StandardScaler()
X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.3, random_state=42)
X_train_sc = scaler.fit_transform(X_train.values)
X_test_sc = scaler.transform(X_test.values)  # Only transform for test set
# Define parameter grids for each model
param_grid_knn = {'n_neighbors': [5, 10, 15, 20]}
param_grid_rfc = {'n_estimators': [50, 100, 150]}
param_grid_etc = {'n_estimators': [50, 100, 150]}
param_grid_xgb = {'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.01, 0.001]}

# Define grid search objects for each model
grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='accuracy')
grid_search_rfc = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rfc, cv=5, scoring='accuracy')
grid_search_etc = GridSearchCV(ExtraTreesClassifier(random_state=42), param_grid_etc, cv=5, scoring='accuracy')
grid_search_xgb = GridSearchCV(xgb.XGBClassifier(), param_grid_xgb, cv=5, scoring='accuracy')

# List of grid search objects
grid_searches = [('k-NN', grid_search_knn),
                 ('RFC', grid_search_rfc),
                 ('ETC', grid_search_etc),
                 ('XGB', grid_search_xgb)]

# Iterate over each grid search object
for name, grid_search in grid_searches:
    print(f"Grid search for {name}...")
    grid_search.fit(X_train_sc, y_train)
    best_params = grid_search.best_params_
    best_model = grid_search.best_estimator_

    # Evaluate best model
    y_pred = best_model.predict(X_test_sc)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    kappa = cohen_kappa_score(y_test, y_pred)
    matthews_coef = matthews_corrcoef(y_test, y_pred)

    if hasattr(best_model, "predict_proba"):
        y_prob = best_model.predict_proba(X_test_sc)
        auc = roc_auc_score(y_test, y_prob, multi_class='ovr')
    else:
        auc = None

    print(f"Best parameters: {best_params}")
    print(f"  {accuracy:.2f}", end='    ')
    print(f"  {precision:.2f}", end='    ')
    print(f"  {f1:.2f}", end='    ')
    print(f"  {recall:.2f}", end='    ')
    print(f"  {kappa:.2f}", end='    ')
    print(f"  {matthews_coef:.2f}", end='    ')

    if auc is not None:
        print(f"  {auc:.2f}")
    else:
        print("  AUC: Not applicable (model does not support predict_proba)")

    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print()



"""###Voting"""

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
# Prepare models
clf1 = KNeighborsClassifier(n_neighbors=5)
clf2 = RandomForestClassifier(n_estimators=150, random_state=42)
clf3 = ExtraTreesClassifier(n_estimators=150, random_state=42)
clf4 = xgb.XGBClassifier()


eclf1 = VotingClassifier(estimators=[
        ('KNN', clf1), ('rf', clf2), ('ETR', clf3), ('XGB', clf4)], voting='soft', weights=[1,3,4,4], flatten_transform=True)


#y_corrected = y - 1
y_corrected = y
y_corrected_df = pd.DataFrame(y_corrected)

# Cross Validation
skf = StratifiedKFold(n_splits=5)

for train_index, test_index in skf.split(X, y_corrected):
  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train = y_corrected_df.iloc[train_index].values.ravel()
  y_test = y_corrected_df.iloc[test_index].values.ravel()
  # Prepare data
  scaler = StandardScaler()
  X_train_sc = scaler.fit_transform(X_train)
  X_test_sc = scaler.transform(X_test)

  eclf1 = eclf1.fit(X_train_sc, y_train)
  #print(eclf1.predict(X_test_sc))
  y_pred = eclf1.predict(X_test_sc)
  # Calcular y almacenar la precisión
  accuracy = accuracy_score(y_test, y_pred)

  # Calcular y almacenar la precisión, recall y f1-score
  precision = precision_score(y_test, y_pred, average='weighted')


  recall = recall_score(y_test, y_pred, average='weighted')


  f1 = f1_score(y_test, y_pred, average='weighted')


  # Calcular y almacenar la matriz de confusión
  conf_matrix = confusion_matrix(y_test, y_pred)
  tp = conf_matrix[1, 1]
  fn = conf_matrix[1, 0]
  sensitivity = tp / (tp + fn)


  # Calcular y almacenar la especificidad
  tn = conf_matrix[0, 0]
  fp = conf_matrix[0, 1]
  specificity = tn / (tn + fp)


  # Calcular y almacenar el AUC (área bajo la curva)
  #auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], multi_class='ovr')
  y_pred_prob = eclf1.predict_proba(X_test_sc)
  auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')

  # Calcular y almacenar el coeficiente de correlación de Matthews
  matthews_coef = matthews_corrcoef(y_test, y_pred)


  kappa = cohen_kappa_score(y_test, y_pred)


  print(f"  {accuracy:.2f}", end='    ')
  print(f"  {precision:.2f}", end='    ')
  print(f"  {f1:.2f}", end='    ')
  print(f"  {recall:.2f}", end='    ')
  print(f"  {kappa:.2f}", end='    ')
  print(f"  {matthews_coef:.2f}", end='    ')
  print(f"  {auc:.2f}")
  print()
  print("Classification Report:")
  print(classification_report(y_test, y_pred))  # Print classification report for each model
  print()




  plt.figure(figsize=(8, 6))
  sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
  plt.xlabel('Predicciones')
  plt.ylabel('Clases reales')
  plt.title('Matriz de Confusión')
  plt.show()

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix, roc_auc_score, classification_report, cohen_kappa_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare models
clf1 = KNeighborsClassifier(n_neighbors=5)
clf2 = RandomForestClassifier(n_estimators=150, random_state=42)
clf3 = ExtraTreesClassifier(n_estimators=150, random_state=42)
clf4 = xgb.XGBClassifier()

eclf1 = VotingClassifier(estimators=[
    ('KNN', clf1), ('rf', clf2), ('ETR', clf3), ('XGB', clf4)],
    voting='soft', weights=[1, 3, 4, 4], flatten_transform=True)

# y_corrected = y - 1
y_corrected = y
y_corrected_df = pd.DataFrame(y_corrected)

# Cross Validation
skf = StratifiedKFold(n_splits=5)

accuracy_values = []
precision_values = []
recall_values = []
f1_values = []
kappa_values = []
matthews_coef_values = []
auc_values = []

conf_matrices = []

for train_index, test_index in skf.split(X, y_corrected):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train = y_corrected_df.iloc[train_index].values.ravel()
    y_test = y_corrected_df.iloc[test_index].values.ravel()

    # Prepare data
    scaler = StandardScaler()
    X_train_sc = scaler.fit_transform(X_train)
    X_test_sc = scaler.transform(X_test)

    eclf1 = eclf1.fit(X_train_sc, y_train)
    y_pred = eclf1.predict(X_test_sc)

    # Calculate and store metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    kappa = cohen_kappa_score(y_test, y_pred)
    matthews_coef = matthews_corrcoef(y_test, y_pred)

    accuracy_values.append(accuracy)
    precision_values.append(precision)
    recall_values.append(recall)
    f1_values.append(f1)
    kappa_values.append(kappa)
    matthews_coef_values.append(matthews_coef)

    # Calculate and store AUC
    y_pred_prob = eclf1.predict_proba(X_test_sc)[:, 1]
    auc = roc_auc_score(y_test, y_pred_prob)
    auc_values.append(auc)

    # Calculate and store confusion matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    conf_matrices.append(conf_matrix)

    print(f"  Accuracy: {accuracy:.2f}")
    print(f"  Precision: {precision:.2f}")
    print(f"  Recall: {recall:.2f}")
    print(f"  F1-Score: {f1:.2f}")
    print(f"  Cohen's Kappa: {kappa:.2f}")
    print(f"  Matthews Correlation Coefficient: {matthews_coef:.2f}")
    print(f"  ROC AUC: {auc:.2f}")
    print()
    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each fold
    print()

    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicciones')
    plt.ylabel('Clases reales')
    plt.title('Matriz de Confusión')
    plt.show()

# Sum all confusion matrices to get the overall confusion matrix
total_conf_matrix = np.sum(conf_matrices, axis=0)
print("Overall Confusion Matrix:")
print(total_conf_matrix)

plt.figure(figsize=(8, 6))
sns.heatmap(total_conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicciones')
plt.ylabel('Clases reales')
plt.title('Matriz de Confusión Acumulada')
plt.show()

print(f"Overall Accuracy: {sum(accuracy_values) / len(accuracy_values):.2f}")
print(f"Overall Precision: {sum(precision_values) / len(precision_values):.2f}")
print(f"Overall Recall: {sum(recall_values) / len(recall_values):.2f}")
print(f"Overall F1-Score: {sum(f1_values) / len(f1_values):.2f}")
print(f"Overall Cohen's Kappa: {sum(kappa_values) / len(kappa_values):.2f}")
print(f"Overall Matthews Correlation Coefficient: {sum(matthews_coef_values) / len(matthews_coef_values):.2f}")
print(f"Overall ROC AUC: {sum(auc_values) / len(auc_values):.2f}")

"""###StratifiedKFold  - Para desbalance de clases"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import GroupKFold, StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import roc_auc_score

# Definir los grupos (en este caso, 'caseid')
groups = subset_df_desbalance['caseid']

# Inicializar el validador cruzado agrupado
skf = StratifiedKFold(n_splits=5)

# Prepare models
clf1 = KNeighborsClassifier(n_neighbors=20)
clf2 = RandomForestClassifier(n_estimators=10, random_state=42)
clf3 = ExtraTreesClassifier(n_estimators=150, random_state=42)
clf4 = xgb.XGBClassifier()

accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
sensitivity_scores = []
specificity_scores = []
auc_scores = []
matthews_scores = []
kappa_scores = []

y_corrected = y
y_corrected_df = pd.DataFrame(y_corrected)

for train_index, test_index in gkf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train = y_corrected_df.iloc[train_index].values.ravel()
    y_test = y_corrected_df.iloc[test_index].values.ravel()

    scaler = StandardScaler()

    X_train_sc = scaler.fit_transform(X_train.values)
    X_test_sc = scaler.transform(X_test.values)

    # Entrenar el clasificador
    eclf1 = VotingClassifier(estimators=[
        ('KNN', clf1), ('rf', clf2), ('ETR', clf3), ('XGB', clf4)], voting='soft', weights=[3,1,3,1], flatten_transform=True)

    eclf1 = eclf1.fit(X_train_sc, y_train)

    # Realizar predicciones en el conjunto de prueba
    y_pred = eclf1.predict(X_test_sc)

    # Calcular métricas de evaluación
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)
    precision = precision_score(y_test, y_pred, average='weighted')
    precision_scores.append(precision)
    recall = recall_score(y_test, y_pred, average='weighted')
    recall_scores.append(recall)
    f1 = f1_score(y_test, y_pred, average='weighted')
    f1_scores.append(f1)
    kappa = cohen_kappa_score(y_test, y_pred)
    kappa_scores.append(kappa)
    matthews_coef = matthews_corrcoef(y_test, y_pred)
    matthews_scores.append(matthews_coef)

    y_pred_prob = eclf1.predict_proba(X_test_sc)
    auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')
    auc_scores.append(auc)

    print("Métricas de evaluación:")
    print(f"{accuracy:.2f}")
    print(f"{precision:.2f}")
    print(f"{recall:.2f}")
    print(f"{f1:.2f}")
    print(f"{kappa:.2f}")
    print(f"{matthews_coef:.2f}")
    print(f"{auc:.2f}")
    print()

    # Mostrar la matriz de confusión
    conf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicciones')
    plt.ylabel('Clases reales')
    plt.title('Matriz de Confusión')
    plt.show()

    # Mostrar el classification report
    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each fold
    print()

# Print average metrics
print('Average Metrics:')
print(f'{np.mean(accuracy_scores):.2f}')
print(f'{np.mean(precision_scores):.2f}')
print(f'{np.mean(f1_scores):.2f}')
print(f'{np.mean(recall_scores):.2f}')
print(f'{np.mean(kappa_scores):.2f}')
print(f'{np.mean(matthews_scores):.2f}')
print(f'{np.mean(auc_scores):.2f}')

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
import xgboost as xgb

# Prepare models
models = []
models.append(('k-NN', KNeighborsClassifier(n_neighbors=19)))
#models.append(('DTC', DecisionTreeClassifier()))
#models.append(('NB', GaussianNB()))
models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='rbf', probability=True)))  # Set probability=True for SVC
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))  # Add ExtraTreesClassifier
#models.append(('SGD', SGDClassifier(max_iter=1000, tol=1e-3)))
models.append(('XGB', xgb.XGBClassifier()))

#y_corrected = y - 1
y_corrected = y
y_corrected_df = pd.DataFrame(y_corrected)

# Cross Validation
skf = StratifiedKFold(n_splits=5)

for name, model in models:
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    f1_scores = []
    sensitivity_scores = []
    specificity_scores = []
    auc_scores = []
    matthews_scores = []
    kappa_scores = []

    for train_index, test_index in skf.split(X, y_corrected):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train = y_corrected_df.iloc[train_index].values.ravel()
        y_test = y_corrected_df.iloc[test_index].values.ravel()
        # Prepare data
        scaler = StandardScaler()
        X_train_sc = scaler.fit_transform(X_train)
        X_test_sc = scaler.transform(X_test)

        model.fit(X_train_sc, y_train)
        y_pred = model.predict(X_test_sc)

        # Calcular y almacenar la precisión
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

        # Calcular y almacenar la precisión, recall y f1-score
        precision = precision_score(y_test, y_pred, average='weighted')
        precision_scores.append(precision)

        recall = recall_score(y_test, y_pred, average='weighted')
        recall_scores.append(recall)

        f1 = f1_score(y_test, y_pred, average='weighted')
        f1_scores.append(f1)

        # Calcular y almacenar la matriz de confusión
        conf_matrix = confusion_matrix(y_test, y_pred)
        tp = conf_matrix[1, 1]
        fn = conf_matrix[1, 0]
        sensitivity = tp / (tp + fn)
        sensitivity_scores.append(sensitivity)

        # Calcular y almacenar la especificidad
        tn = conf_matrix[0, 0]
        fp = conf_matrix[0, 1]
        specificity = tn / (tn + fp)
        specificity_scores.append(specificity)

        # Calcular y almacenar el AUC (área bajo la curva)
        #auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], multi_class='ovr')
        y_prob = model.predict_proba(X_test_sc)
        auc = roc_auc_score(y_test,y_prob, multi_class='ovr')
        auc_scores.append(auc)

        # Calcular y almacenar el coeficiente de correlación de Matthews
        matthews_coef = matthews_corrcoef(y_test, y_pred)
        matthews_scores.append(matthews_coef)

        kappa = cohen_kappa_score(y_test, y_pred)
        kappa_scores.append(kappa)
    print(f'Modelo: {name}')


    # Imprimir precision, recall y f1-score
    #print('Precision  Recall  F1-Score  Sensitivity  Matthews')
    print(f'{np.mean(accuracy_scores):.2f}', end='    ')
    print(f'{np.mean(precision_scores):.2f}', end='    ')
    print(f'{np.mean(f1_scores):.2f}', end='    ')
    print(f'{np.mean(recall_scores):.2f}', end='    ')
    print(f'{np.mean(kappa_scores):.2f}', end='    ')
    print(f'{np.mean(matthews_scores):.2f}', end='    ')
    print(f'{np.mean(auc_scores):.2f}')
    print()
    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each model
    print()




    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicciones')
    plt.ylabel('Clases reales')
    plt.title('Matriz de Confusión')
    plt.show()

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, matthews_corrcoef, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import SGDClassifier
import xgboost as xgb

# Prepare models
models = []
models.append(('k-NN', KNeighborsClassifier(n_neighbors=19)))
#models.append(('DTC', DecisionTreeClassifier()))
#models.append(('NB', GaussianNB()))
models.append(('SVM', SVC(gamma='auto', C=2.0, kernel='rbf', probability=True)))  # Set probability=True for SVC
models.append(('RFC', RandomForestClassifier(n_estimators=100, random_state=42)))
models.append(('ETC', ExtraTreesClassifier(n_estimators=100, random_state=42)))  # Add ExtraTreesClassifier
#models.append(('SGD', SGDClassifier(max_iter=1000, tol=1e-3)))
models.append(('XGB', xgb.XGBClassifier()))

#y_corrected = y - 1
y_corrected = y
y_corrected_df = pd.DataFrame(y_corrected)

# Cross Validation
skf = StratifiedKFold(n_splits=5)

for name, model in models:
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    f1_scores = []
    sensitivity_scores = []
    specificity_scores = []
    auc_scores = []
    matthews_scores = []
    kappa_scores = []

    for train_index, test_index in skf.split(X, y_corrected):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train = y_corrected_df.iloc[train_index].values.ravel()
        y_test = y_corrected_df.iloc[test_index].values.ravel()
        # Prepare data
        scaler = StandardScaler()
        X_train_sc = scaler.fit_transform(X_train)
        X_test_sc = scaler.transform(X_test)

        model.fit(X_train_sc, y_train)
        y_pred = model.predict(X_test_sc)

        # Calcular y almacenar la precisión
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

        # Calcular y almacenar la precisión, recall y f1-score
        precision = precision_score(y_test, y_pred, average='weighted')
        precision_scores.append(precision)

        recall = recall_score(y_test, y_pred, average='weighted')
        recall_scores.append(recall)

        f1 = f1_score(y_test, y_pred, average='weighted')
        f1_scores.append(f1)

        # Calcular y almacenar la matriz de confusión
        conf_matrix = confusion_matrix(y_test, y_pred)
        tp = conf_matrix[1, 1]
        fn = conf_matrix[1, 0]
        sensitivity = tp / (tp + fn)
        sensitivity_scores.append(sensitivity)

        # Calcular y almacenar la especificidad
        tn = conf_matrix[0, 0]
        fp = conf_matrix[0, 1]
        specificity = tn / (tn + fp)
        specificity_scores.append(specificity)

        # Calcular y almacenar el AUC (área bajo la curva)
        #auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1], multi_class='ovr')
        y_prob = model.predict_proba(X_test_sc)
        auc = roc_auc_score(y_test,y_prob, multi_class='ovr')
        auc_scores.append(auc)

        # Calcular y almacenar el coeficiente de correlación de Matthews
        matthews_coef = matthews_corrcoef(y_test, y_pred)
        matthews_scores.append(matthews_coef)

        kappa = cohen_kappa_score(y_test, y_pred)
        kappa_scores.append(kappa)
    print(f'Modelo: {name}')


    # Imprimir precision, recall y f1-score
    #print('Precision  Recall  F1-Score  Sensitivity  Matthews')
    print(f'{np.mean(accuracy_scores):.2f}', end='    ')
    print(f'{np.mean(precision_scores):.2f}', end='    ')
    print(f'{np.mean(f1_scores):.2f}', end='    ')
    print(f'{np.mean(recall_scores):.2f}', end='    ')
    print(f'{np.mean(kappa_scores):.2f}', end='    ')
    print(f'{np.mean(matthews_scores):.2f}', end='    ')
    print(f'{np.mean(auc_scores):.2f}')
    print()
    print("Classification Report:")
    print(classification_report(y_test, y_pred))  # Print classification report for each model
    print()




    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicciones')
    plt.ylabel('Clases reales')
    plt.title('Matriz de Confusión')
    plt.show()
# -*- coding: utf-8 -*-
"""Code_functions
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VKm_svlfcVh_cBYtjkB0bt-PfyuDfGg4
"""

import pandas as pd
import numpy as np
import librosa
from pythresh.thresholds.iqr import IQR
from scipy import signal
import math
from collections import Counter
import mne
from scipy import signal
from sklearn.preprocessing import normalize
from ecgdetectors import Detectors
import pywt
from pythresh.thresholds.iqr import IQR
from scipy import signal
import math
from collections import Counter
import neurokit2 as nk
from hrvanalysis import remove_outliers, remove_ectopic_beats, interpolate_nan_values, get_nn_intervals
from hrvanalysis import get_time_domain_features, get_frequency_domain_features, get_geometrical_features
from hrvanalysis import get_poincare_plot_features, get_csi_cvi_features, get_sampen
from hrvanalysis import plot_psd, plot_distrib
from hrvanalysis import plot_poincare
import pywt
from scipy.signal import find_peaks
from scipy.interpolate import CubicSpline
from scipy import interpolate
from scipy.interpolate import interp1d
from scipy.signal import welch
import seaborn as sns
import pyhrv
import pyhrv.frequency_domain as fd
import pyhrv.nonlinear as nl
from scipy import stats
import statistics as stat
import antropy as ant
from scipy.signal import welch
from scipy.integrate import simps
import pyhrv
from biosppy import utils
import pyhrv
import pyhrv.time_domain as td
import matplotlib.pyplot as plt

# %%
# https://stackoverflow.com/questions/58920673/return-the-index-range-of-the-longest-run-of-a-boolean-list
from itertools import groupby
from operator import itemgetter

def longest_false_run(lst):
    """Finds the longest false run in a list of boolean"""

    # find False runs only
    groups = [[i for i, _ in group] for key, group in groupby(enumerate(lst), key=itemgetter(1)) if not key]

    # get the one of maximum length
    group = max(groups, key=len, default=[-1, -1])

    start, end = group[0], group[-1]

    return start, end



# %% Entropy

def signal_shannon_entropy(signal):
    # Count the occurrences of each value in the signal
    symbol_counts = Counter(signal)

    # Total number of values in the signal
    total_symbols = len(signal)

    # Calculate the probabilities of each values
    symbol_probabilities = [count / total_symbols for count in symbol_counts.values()]

    # Calculate the Shannon entropy using the probabilities
    entropy = -sum(p * math.log2(p) for p in symbol_probabilities if p > 0)

    return entropy

# %%
def signal_detrend(signal_, mother_wavelet, decomp_level):
    '''
    Function for removing baseline of a signal using discrete wavelet transform.

    Parameters
    ----------
    signal_ : TYPE 1D array of float
        Signal to be processed.
    mother_wavelet : TYPE str
        Name of the mother wavelet to be employed
    decomp_level : TYPE int
        The number of levels to make the decomposition

    Returns
    -------
    signal_flat : TYPE array of float
        DESCRIPTION. Signal processed.


        Example:
            import pywt

            mother_wavelet = "bior3.7"
            decomp_level = 10
            signal_flat = signal_detrend(signal_, mother_wavelet, decomp_level)

            plt.figure()
            plt.plot(signal_)
            plt.plot(signal_flat)

    '''
    # Mother wavelet
    wl = pywt.Wavelet(mother_wavelet)
    # Maximum possible number of decomposition levels
    filter_len = wl.dec_len
    max_l = pywt.dwt_max_level(len(signal_), filter_len)

    # Verifying the decomposition levels
    if decomp_level > max_l:
        raise ValueError('decomp_level should be minor than ' + str(max_l) )

    if decomp_level <= max_l:
        coeff_all = pywt.wavedec(signal_, wl, level=decomp_level)

        # Detail coefficients equal to 0
        for coeff_indx in range(1, decomp_level+1, 1):
            coeff_all[coeff_indx][:] = 0

        baseline = pywt.waverec(coeff_all, wl)
        # Correcting length of reconstructed signal
        baseline = baseline[0:len(signal_)]

        # Removing baseline
        signal_flat = signal_ - baseline

        return signal_flat


#%% Signal quality
def quality_signal(signal_, Fs, segm_time, det_segm, window_time,overlap):
    '''
    This function computes a score vector of the signal quality "labels_q". 0 is assigned
    to good quality segments and 1 to bad segments. The output of the funcion
    is a list of quality labels values in which their position in the vector represents
    the window of the signal from which the quality value was computed.

    The methodology is based on the variation of the signal values with time.
    A corrupted signal thus would present more changes in its values than a
    "good-quality" signal. For this purpose, the standard deviation of maximum
    and minimum values of the signal are taken into account. In addition,
    zero crossing rate is computed for detecting noise.

    Parameters
    ----------
    signal_ : TYPE array or list
        signal to be analysed.
    Fs : TYPE int
        sampling rate.
    segm_time : TYPE int
        time in seconds of the smallest windows in which the signal would be firstly
        split for computing scores.
    det_segm : TYPE bool
        Variable for making decision of detendring or not the segment of analysis
        True = detrend segment
    window_time : TYPE int
        window time in seconds in which the signal would be analysed.

    Returns
    -------
    labels_q : TYPE array binary
        quality scores.
        labels_q = 1 -> Bad quality
        labels_q = 0 -> Good quality
    ind_start_good_L : TYPE list
        list with the indexes of the signal in which good segments start
    ind_end_good_L : TYPE list
        list with the indexes of the signal in which good segments end
    ind_start_bad_L : TYPE list
        list with the indexes of the signal in which bad segments start
    ind_end_bad_L : TYPE list
        list with the indexes of the signal in which bad segments end


    Example of application:

        signal_ = ecg_signal
        Fs = 500
        segm_time = 5
        det_segm = False
        window_time = 60

        labels_q, ind_start_good_L, ind_end_good_L, ind_start_bad_L, ind_end_bad_L = quality_signal(signal_, Fs, segm_time, det_segm, window_time)


    '''

    # Verifying the type of variables
    if isinstance(Fs, int) == False:
        raise ValueError('Variable "Fs" must be of type int')

    if isinstance(segm_time, int) == False:
        raise ValueError('Variable "segm_time" must be of type int')

    if isinstance(window_time, int) == False:
        raise ValueError('Variable "window_time" must be of type int')



    if isinstance(Fs, int) and isinstance(segm_time, int) and isinstance(window_time, int):


        # Replacing nan values with zeros
        signal_[np.isnan(signal_)] = 0

        # Detrending the signal
        # signal_seg_detrended = signal_detrend(signal_, mother_wavelet = "bior3.7",
        #                                       decomp_level = 10)
        signal_seg_detrended = signal_


        # -------------------------------------------------------------------------
        # I: Signal is analysed in windows of 10-seconds. Maximum, minimum, and zcr
        # are computed and stored in a vector

        # Envelope: maximum and minimum values, and zcr are computed in 10s windows
        # segm_time = 10              # segments of 10 seconds
        win_len = Fs*segm_time      # Samples=Fs*time(s)
        num_win = int(len(signal_seg_detrended)/win_len)

        # List for storing values
        max_ = []
        max_peak = []
        min_ = []
        min_peak = []
        constant_ = []
        zcr_ = []

        # Variables for constant-value analysis
        win_len_s = int(Fs*0.75)      # Samples=Fs_*time(s)
        num_win_s = int(win_len - win_len_s) + 1
        constant  = False

        for ind in range(num_win):

            segm = signal_seg_detrended[win_len*ind:(win_len*ind)+(win_len)]
            # maxElement = np.amax(segm)
            max_.append(np.amax(segm))
            # result = segm.argmax(axis=0)
            max_peak.append((win_len*ind) + segm.argmax())

            min_.append(np.amin(segm))
            min_peak.append((win_len*ind) + segm.argmin())

            # -------------------------------------------------------------------
            # Constant-value analysis
            for ind_ in range(num_win_s):
                window = segm[ind_ : ind_+win_len_s]
                constant = np.all(window == window[0])
                if constant == True:
                    break
            constant_.append(int(constant))
            # -------------------------------------------------------------------

            # Detrending segment ---------------------------------------------------
            if det_segm == True:
                # Mother wavelet
                wl = pywt.Wavelet("bior3.7")
                # Maximum possible number of decomposition levels
                filter_len = wl.dec_len
                max_l = pywt.dwt_max_level(len(segm), filter_len)

                max_l = max_l - 1
                if max_l > 0:
                    try:
                        segm_det = signal_detrend(segm, mother_wavelet = "bior3.7", decomp_level = max_l)
                    except:
                        segm_det = segm
                if max_l <= 0:
                    segm_det = segm

            if det_segm == False:
                segm_det = segm
            # ---------------------------------------------------------------------
            zero_crossings = librosa.zero_crossings(segm_det, pad=False)
            zero_crossings = sum(zero_crossings)
            zcr_.append(zero_crossings)

        max_ = np.asarray(max_)
        max_peak = np.asarray(max_peak)
        min_ = np.asarray(min_)
        min_peak = np.asarray(min_peak)
        constant_ = np.asarray(constant_)
        zcr_ = np.asarray(zcr_)

        # plt.figure()
        # plt.subplot(311)
        # plt.plot(signal_, 'g')
        # plt.subplot(312)
        # plt.plot(zcr_, 'b')
        # plt.subplot(313)
        # plt.plot(constant_, 'k')
        # plt.subplot(514)
        # plt.plot(eeg2_signal, 'k')
        # plt.subplot(515)
        # plt.plot(labels_q_L, 'k')

        # Plotting
        # plt.figure()
        # plt.plot(constant_, '-or')

        # # Plotting
        # plt.figure()
        # plt.plot(signal_seg_detrended)
        # plt.plot(max_peak, signal_seg_detrended[max_peak], '-or')
        # plt.plot(min_peak, signal_seg_detrended[min_peak], '-ok')


        # --------------------------------------------------------------------------
        # II: Signal is analysed in windows of window_time in seconds
        # This analysis is base on step I, and not directly on the signals.

        # Maximum and minimum values, standard deviation of maximum and minimum values,
        # and zcr are saved
        # q_max = []
        # q_min = []
        q_max_std  = []
        q_min_std  = []
        q_constant = []
        q_zcr      = []


        # window_time = 60                        # time in seconds
        # segm_time = 10
        win_len = int(window_time/segm_time)        # How many segments of segm_time(s) there are in the window of analysis
        win_len_half = int(win_len/2)
        num_segm = int(len(max_)/win_len)           # How many windows of analysis there are in the signal
        for ind in range(num_segm):
            # Maximum
            segm = max_[win_len*ind:(win_len*ind)+(win_len)]
            # max_max = np.nanmean(segm)
            max_std = np.nanstd(segm)
            # Minimum
            segm = min_[win_len*ind:(win_len*ind)+(win_len)]
            # min_min = np.nanmean(segm)
            min_std = np.nanstd(segm)
            # Constant
            segm = constant_[win_len*ind:(win_len*ind)+(win_len)]
            constant_seg = int(any(segm))
            # ZCR
            segm = zcr_[win_len*ind:(win_len*ind)+(win_len)]
            zcr_max = np.amax(segm)

            # Saving values: Quality of the signal
            # q_max.append(max_max)
            # q_min.append(min_min)
            q_max_std.append(max_std)
            q_min_std.append(min_std)
            q_constant.append(constant_seg)
            q_zcr.append(zcr_max)

            if overlap == True:
                # Overlapping window
                if (win_len*ind)+(win_len)+win_len_half <= len(max_):
                    # Maximum
                    segm = max_[(win_len*ind)+win_len_half:(win_len*ind)+(win_len)+win_len_half]
                    # max_max = np.nanmean(segm)
                    max_std = np.nanstd(segm)
                    # Minimum
                    segm = min_[(win_len*ind)+win_len_half:(win_len*ind)+(win_len)+win_len_half]
                    # min_min = np.nanmean(segm)
                    min_std = np.nanstd(segm)
                    # Constant
                    segm = constant_[win_len*ind:(win_len*ind)+(win_len)]
                    constant_seg = int(any(segm))
                    # ZCR
                    segm = zcr_[(win_len*ind)+win_len_half:(win_len*ind)+(win_len)+win_len_half]
                    zcr_max = np.amax(segm)

                    # Saving values: Quality of the signal
                    # q_max.append(max_max)
                    # q_min.append(min_min)
                    q_max_std.append(max_std)
                    q_min_std.append(min_std)
                    q_constant.append(constant_seg)
                    q_zcr.append(zcr_max)


        # Converting data to array
        # q_max = np.asarray(q_max)
        # q_min = np.asarray(q_min)
        q_max_std = np.asarray(q_max_std)
        q_min_std = np.asarray(q_min_std)
        q_constant = np.asarray(q_constant)
        q_zcr = np.asarray(q_zcr)

        # Inverting q_min
        # q_min = -1*q_min


        # --------------------------------------------------------------------------
        # III: Applying IQ thresholding to the quality signal metrics.
        # This analysis is base on step II, applied to the quality metrics,
        # and not directly on the signals.

        # - Threshold based on histogram
        # from pythresh.thresholds.hist import HIST
        # thres_q_zcr = HIST(method='otsu')
        # labels_thres_q_zcr = thres_q_zcr.eval(q_zcr)
        # threshold_thres_q_zcr = thres_q_zcr.thresh_

        # Defining the Threshold method
        thres_iqr = IQR()

        # Applying thres_iqr to q_zcr, q_min_std, and q_max_std
        # labels = 1 -> Bad quality
        # labels = 0 -> Good quality

        labels_q_zcr = thres_iqr.eval(q_zcr)
        # labels_q_constant = thres_iqr.eval(q_constant)
        labels_q_constant = q_constant
        labels_q_min_std = thres_iqr.eval(q_min_std)
        labels_q_max_std = thres_iqr.eval(q_max_std)

        # Mixing labels_q_zcr, labels_q_min_std, and labels_q_max_std in labels_q
        # Applying AND operator in such a way bad quality (labels = 1) are kept
        # List for saving labels
        labels_q = []
        # Saving values in lists
        for ind in range(len(labels_q_zcr)):
            labels_q.append(int(any([labels_q_zcr[ind], labels_q_constant[ind], labels_q_min_std[ind], labels_q_max_std[ind]]) ))

        labels_q = np.asarray(labels_q)

        # Indexes of good segments
        labels_good_q = np.where(labels_q == 0)
        labels_good_q = np.asarray(labels_good_q[0])

        # Lists for saving indexes
        ind_start_good_L = []
        ind_end_good_L   = []

        # Saving values in lists
        for ind in range(len(labels_good_q)):
        # for ind in range(10):
        #     print(ind* win_len * segm_time * Fs)
        #     print((ind* win_len * segm_time * Fs) + (2 * win_len_half * segm_time * Fs) )
        #     print('---')
            # ind * win_len_half * segm_time * Fs
            # ind * win_len_half * segm_time * Fs    +  (win_len_half * 2 * segm_time * Fs)


            if overlap == False:
                 ind_start_good = labels_good_q[ind] * win_len * segm_time * Fs
                 ind_end_good   = ind_start_good + (win_len * segm_time * Fs)

            if overlap == True:
                ind_start_good = labels_good_q[ind] * win_len_half * segm_time * Fs
                ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * Fs)

            ind_start_good_L.append(ind_start_good)
            ind_end_good_L.append(ind_end_good)

        # try_segment = signal_[ind_start_good:ind_end_good]

        # Indexes of bad segments
        labels_bad_q = np.where(labels_q == 1)
        labels_bad_q = np.asarray(labels_bad_q[0])

        # Lists for saving indexes
        ind_start_bad_L = []
        ind_end_bad_L   = []

        # Saving values in lists
        for ind in range(len(labels_bad_q)):

            if overlap == False:
                ind_start_bad = labels_bad_q[ind] * win_len * segm_time * Fs
                ind_end_bad   = ind_start_bad + (win_len * segm_time * Fs)

            if overlap == True:
                ind_start_bad  = labels_bad_q[ind] * win_len_half * segm_time * Fs
                ind_end_bad    = ind_start_bad + (2 * win_len_half * segm_time * Fs)

            ind_start_bad_L.append(ind_start_bad)
            ind_end_bad_L.append(ind_end_bad)

        # try_segment = signal_[ind_start_bad:ind_end_bad]


        return labels_q, ind_start_good_L, ind_end_good_L, ind_start_bad_L, ind_end_bad_L



# %% Multisignal quality assessment
def quality_multi_signal(samp_rates, signals, det_segms, segm_time, window_time,overlap):
    '''
    This function is based on the function "quality_signal"

    This function computes a score vector "labels_q_L" with the score vectors of
    the signal quality of each signal in the list "signals". 0 is assigned
    to good quality segments and 1 to bad segments. The output of the funcion
    is a list of starting and ending indexes of good and bad quality segments
    for each signal.

    The methodology is based on the variation of the signal values with time.
    A corrupted signal thus would present more changes in its values than a
    "good-quality" signal. For this purpose, the standard deviation of maximum
    and minimum values of the signal are taken into account. In addition,
    zero crossing rate is computed for detecting noise.

    This function can analyse until five signals at once. For analysis of PPG and
    BP signals, the variable det_segms must be True. For ECG and EEG signals it is
    suggested det_segms must be False.


    Parameters
    ----------
    samp_rates : TYPE list of values
        List with the sample rates of the signals to be analysed.
    signals : TYPE list of arrays
        List with the signals to be analysed.
    det_segms : TYPE list of booleans
        List with the instruction for detrending procedures, True or False.
    segm_time : TYPE int
        Duration in time of the shortest window for internal analysis.
    window_time : TYPE int
        Duration in time of the desired segments.

    Raises
    ------
    ValueError
        - The ratio "window_time/segm_time" must be divisible by 2.
        - The lengths of the lists samp_rates, signals, and det_segms must be the same,
        and they must be in the same order.
        - The lengths of the lists samp_rates, signals, and det_segms must be major or
        equal than 2 and less than 6

    Returns
    -------
    signal_0,1,..._ind : TYPE dict
        Dictionary that cotains the indexes of the end of band and good segments
        (ind_end_bad_L and ind_end_good_L) and of the starting of bad and good segments
        (ind_start_bad_L and ind_start_good_L).


    Example
    -------
    # Sampling frequencies, signals, instructions for detrending signals must be
    # put in lists. Moreover, it is important to provided the smallest segm_time for
    # analyse the signals, and the window_time for selecting good segments

    samp_rates  = [Fs_eeg_v, Fs_v, Fs_v, Fs_eeg_v]
    signals     = [eeg1_signal, ecg_signal, ppg_signal, eeg2_signal]
    det_segms   = [False, False, True, False]
    segm_time   = 3
    window_time = 30

    labels_q_L, signal_0_ind, signal_1_ind, signal_2_ind, signal_3_ind = quality_multi_signal(samp_rates, signals, det_segms, segm_time, window_time)

    '''

    # *****************************************************************************************

    win_len = int(window_time/segm_time)        # How many segments of segm_time(s) there are in the window of analysis
    win_len_half = int(win_len/2)

    if win_len%2 != 0:
        raise ValueError('The ratio "window_time/segm_time" must be divisible by 2')

    if len(samp_rates) != len(signals) or len(samp_rates)!= len(det_segms) or len(signals)!= len(det_segms):
        raise ValueError('The lengths of the lists samp_rates, signals, and det_segms must be the same')

    if len(samp_rates) >= 6 or len(samp_rates)<= 1:
        raise ValueError('The lengths of the lists samp_rates, signals, and det_segms must be major or equal than 2 and less than 6')

    ind_min_samp_rates = np.argmin(samp_rates)      # Index of the shortes signal
    min_len = len(signals[ind_min_samp_rates])      # Minimum number of samples of the signals


    # Replacing NaN values in the signals
    for ind in range(len(signals)):
        signals[ind][np.isnan(signals[ind])] = 0


    # *****************************************************************************************

    samp_rates_equal = all(map(lambda x: x == samp_rates[0], samp_rates))

    if samp_rates_equal == True:
        # Verifying the length of the signals
        signals_resampled = []
        ind = 0
        for s in signals:
            if len(s) != min_len:
                s_resampled = s[0:min_len]

            if len(s) == min_len:
                s_resampled = signals[ind]

            signals_resampled.append(s_resampled)
            ind = ind + 1


    if samp_rates_equal == False:
        # Resampling signals to the lowest sampling rate
        # Resampled signals are stored in the array signals_resampled
        signals_resampled = []
        ind = 0
        for s in signals:

            if len(s) != min_len:
                s_resampled = signal.resample(s, min_len)

            if len(s) == min_len:
                s_resampled = signals[ind]

            signals_resampled.append(s_resampled)
            ind = ind + 1



    # Computing signal quality indicator for all (resampled) signals
    labels_q_resampled = []
    for ind in range(len(signals)):
        labels_q_, ind_start_good_L_0, ind_end_good_L_0, ind_start_bad_L_0, ind_end_bad_L_0 = quality_signal(signals_resampled[ind], samp_rates[ind_min_samp_rates], segm_time, det_segms[ind], window_time, overlap)
        labels_q_resampled.append(labels_q_)

    del ind_start_good_L_0, ind_end_good_L_0, ind_start_bad_L_0, ind_end_bad_L_0

    labels_q_resampled = np.array(labels_q_resampled)


    # *****************************************************************************************
    # Mixing q_labels in the array labels_q_resampled
    # Applying AND operator in such a way bad quality (labels = 1) are kept.
    # This operator is applied by columns

    # List for saving labels
    labels_q_L = []
    # Saving values in lists
    for ind in range(len(labels_q_resampled[0])):

        column = labels_q_resampled[:,ind]
        labels_q_L.append( int(any(column)) )

    labels_q_L = np.asarray(labels_q_L)

    # plt.figure()
    # plt.subplot(411)
    # plt.plot(signal_0, '-g')
    # plt.subplot(412)
    # plt.plot(signal_1, '-b')
    # plt.subplot(413)
    # plt.plot(signal_2, '-k')
    # plt.subplot(414)
    # plt.plot(signal_3, '-k')

    # plt.figure()
    # plt.subplot(511)
    # plt.plot(labels_q_resampled[0], '-g')
    # plt.subplot(512)
    # plt.plot(labels_q_resampled[1], '-b')
    # plt.subplot(513)
    # plt.plot(labels_q_resampled[2], '-k')
    # plt.subplot(514)
    # plt.plot(labels_q_resampled[3], '-k')
    # plt.subplot(515)
    # plt.plot(labels_q_L, '-k')


    # Indexes of good segments
    ind_start_good_q = np.where(labels_q_L == 0)
    ind_start_good_q = np.asarray(ind_start_good_q[0])

    # Indexes of bad segments
    ind_start_bad_q = np.where(labels_q_L == 1)
    ind_start_bad_q = np.asarray(ind_start_bad_q[0])

    # Percentage of good and bad segments
    # len(ind_start_good_q) * 100 / len(labels_q_L)
    # len(ind_start_bad_q) * 100 / len(labels_q_L)

    # *****************************************************************************************
    # Lists for saving indexes
    # signal_0
    ind_start_good_L0 = []
    ind_end_good_L0   = []
    ind_start_bad_L0  = []
    ind_end_bad_L0    = []

    # signal_1
    ind_start_good_L1 = []
    ind_end_good_L1   = []
    ind_start_bad_L1  = []
    ind_end_bad_L1    = []

    if len(signals) == 3:
        # signal_2
        ind_start_good_L2 = []
        ind_end_good_L2   = []
        ind_start_bad_L2  = []
        ind_end_bad_L2    = []

    if len(signals) == 4:
        # signal_2
        ind_start_good_L2 = []
        ind_end_good_L2   = []
        ind_start_bad_L2  = []
        ind_end_bad_L2    = []

        # signal_3
        ind_start_good_L3 = []
        ind_end_good_L3   = []
        ind_start_bad_L3  = []
        ind_end_bad_L3    = []

    if len(signals) == 5:
        # signal_2
        ind_start_good_L2 = []
        ind_end_good_L2   = []
        ind_start_bad_L2  = []
        ind_end_bad_L2    = []

        # signal_3
        ind_start_good_L3 = []
        ind_end_good_L3   = []
        ind_start_bad_L3  = []
        ind_end_bad_L3    = []

        # signal_4
        ind_start_good_L4 = []
        ind_end_good_L4   = []
        ind_start_bad_L4  = []
        ind_end_bad_L4    = []


    # win_len = int(window_time/segm_time)        # How many segments of segm_time(s) there are in the window of analysis
    # win_len_half = int(win_len/2)


    # Saving good values in lists
    for ind in range(len(ind_start_good_q)):

        # signal_0
        if overlap == False:
            ind_start_good = ind_start_good_q[ind] * win_len * segm_time * samp_rates[0]
            ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[0])
        if overlap == True:
            ind_start_good = ind_start_good_q[ind] * win_len_half * segm_time * samp_rates[0]
            ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[0])

        ind_start_good_L0.append(ind_start_good)
        ind_end_good_L0.append(ind_end_good)

        # signal_1
        if overlap == False:
            ind_start_good = ind_start_good_q[ind] * win_len * segm_time * samp_rates[1]
            ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[1])
        if overlap == True:
            ind_start_good = ind_start_good_q[ind] * win_len_half * segm_time * samp_rates[1]
            ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[1])

        ind_start_good_L1.append(ind_start_good)
        ind_end_good_L1.append(ind_end_good)

        if len(signals) == 3:
            # signal_2
            if overlap == False:
                 ind_start_good = ind_start_good_q[ind] * win_len * segm_time * samp_rates[2]
                 ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[2])
            if overlap == True:
                ind_start_good = ind_start_good_q[ind] * win_len_half * segm_time * samp_rates[2]
                ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[2])

            ind_start_good_L2.append(ind_start_good)
            ind_end_good_L2.append(ind_end_good)

        if len(signals) == 4:
            # signal_2
            if overlap == False:
                 ind_start_good = ind_start_good_q[ind] * win_len * segm_time * samp_rates[2]
                 ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[2])
            if overlap == True:
                ind_start_good = ind_start_good_q[ind] * win_len_half * segm_time * samp_rates[2]
                ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[2])

            ind_start_good_L2.append(ind_start_good)
            ind_end_good_L2.append(ind_end_good)

            # signal_3
            if overlap == False:
                ind_start_good = ind_start_good_q[ind] * win_len * segm_time * samp_rates[3]
                ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[3])
            if overlap == True:
                ind_start_good = ind_start_good_q[ind] * win_len_half * segm_time * samp_rates[3]
                ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[3])

            ind_start_good_L3.append(ind_start_good)
            ind_end_good_L3.append(ind_end_good)

        if len(signals) == 5:
            # signal_2
            if overlap == False:
               ind_start_good = ind_start_good_q[ind] * win_len * segm_time * samp_rates[2]
               ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[2])
            if overlap == True:
                ind_start_good = ind_start_good_q[ind] * win_len_half * segm_time * samp_rates[2]
                ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[2])

            ind_start_good_L2.append(ind_start_good)
            ind_end_good_L2.append(ind_end_good)

            # signal_3
            if overlap == False:
               ind_start_good = ind_start_good_q[ind] * win_len * segm_time * samp_rates[3]
               ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[3])
            if overlap == True:
                ind_start_good = ind_start_good_q[ind] * win_len_half * segm_time * samp_rates[3]
                ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[3])

            ind_start_good_L3.append(ind_start_good)
            ind_end_good_L3.append(ind_end_good)

            # signal_4
            if overlap == False:
               ind_start_good = ind_start_good_q[ind] * win_len * segm_time * samp_rates[4]
               ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[4])
            if overlap == True:
                ind_start_good = ind_start_good_q[ind] * win_len_half * segm_time * samp_rates[4]
                ind_end_good   = ind_start_good + (2 * win_len_half * segm_time * samp_rates[4])

            ind_start_good_L4.append(ind_start_good)
            ind_end_good_L4.append(ind_end_good)


    # Saving bad values in lists
    for ind in range(len(ind_start_bad_q)):

        # signal_0
        if overlap == False:
            ind_start_bad = ind_start_bad_q[ind] * win_len * segm_time * samp_rates[0]
            ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[0])
        if overlap == True:
            ind_start_bad = ind_start_bad_q[ind] * win_len_half * segm_time * samp_rates[0]
            ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[0])

        ind_start_bad_L0.append(ind_start_bad)
        ind_end_bad_L0.append(ind_end_bad)

        # signal_1
        if overlap == False:
            ind_start_bad = ind_start_bad_q[ind] * win_len * segm_time * samp_rates[1]
            ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[1])
        if overlap == True:
            ind_start_bad = ind_start_bad_q[ind] * win_len_half * segm_time * samp_rates[1]
            ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[1])

        ind_start_bad_L1.append(ind_start_bad)
        ind_end_bad_L1.append(ind_end_bad)

        if len(signals) == 3:
            # signal_2
            if overlap == False:
               ind_start_bad = ind_start_bad_q[ind] * win_len * segm_time * samp_rates[2]
               ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[2])
            if overlap == True:
                ind_start_bad = ind_start_bad_q[ind] * win_len_half * segm_time * samp_rates[2]
                ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[2])

            ind_start_bad_L2.append(ind_start_bad)
            ind_end_bad_L2.append(ind_end_bad)

        if len(signals) == 4:
            # signal_2
            if overlap == False:
                ind_start_bad = ind_start_bad_q[ind] * win_len * segm_time * samp_rates[2]
                ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[2])
            if overlap == True:
                ind_start_bad = ind_start_bad_q[ind] * win_len_half * segm_time * samp_rates[2]
                ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[2])

            ind_start_bad_L2.append(ind_start_bad)
            ind_end_bad_L2.append(ind_end_bad)

            # signal_3
            if overlap == False:
                ind_start_bad = ind_start_bad_q[ind] * win_len * segm_time * samp_rates[3]
                ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[3])
            if overlap == True:
                ind_start_bad = ind_start_bad_q[ind] * win_len_half * segm_time * samp_rates[3]
                ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[3])

            ind_start_bad_L3.append(ind_start_bad)
            ind_end_bad_L3.append(ind_end_bad)

        if len(signals) == 5:
            # signal_2
            if overlap == False:
               ind_start_bad = ind_start_bad_q[ind] * win_len * segm_time * samp_rates[2]
               ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[2])
            if overlap == True:
                ind_start_bad = ind_start_bad_q[ind] * win_len_half * segm_time * samp_rates[2]
                ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[2])

            ind_start_bad_L2.append(ind_start_bad)
            ind_end_bad_L2.append(ind_end_bad)

            # signal_3
            if overlap == False:
               ind_start_bad = ind_start_bad_q[ind] * win_len * segm_time * samp_rates[3]
               ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[3])
            if overlap == True:
                ind_start_bad = ind_start_bad_q[ind] * win_len_half * segm_time * samp_rates[3]
                ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[3])

            ind_start_bad_L3.append(ind_start_bad)
            ind_end_bad_L3.append(ind_end_bad)

            # signal_4
            if overlap == False:
               ind_start_bad = ind_start_bad_q[ind] * win_len * segm_time * samp_rates[4]
               ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[4])
            if overlap == True:
                ind_start_bad = ind_start_bad_q[ind] * win_len_half * segm_time * samp_rates[4]
                ind_end_bad   = ind_start_bad + (2 * win_len_half * segm_time * samp_rates[4])

            ind_start_bad_L4.append(ind_start_bad)
            ind_end_bad_L4.append(ind_end_bad)


    # *****************************************************************************************
    # Returning values
    signal_0_ind = {}
    signal_0_ind["ind_start_good_L0"] = ind_start_good_L0
    signal_0_ind["ind_end_good_L0"]   = ind_end_good_L0
    signal_0_ind["ind_start_bad_L0"]  = ind_start_bad_L0
    signal_0_ind["ind_end_bad_L0"]    = ind_end_bad_L0

    signal_1_ind = {}
    signal_1_ind["ind_start_good_L1"] = ind_start_good_L1
    signal_1_ind["ind_end_good_L1"]   = ind_end_good_L1
    signal_1_ind["ind_start_bad_L1"]  = ind_start_bad_L1
    signal_1_ind["ind_end_bad_L1"]    = ind_end_bad_L1


    if len(signals) == 2:
        return labels_q_L, signal_0_ind, signal_1_ind

    if len(signals) == 3:
        signal_2_ind = {}
        signal_2_ind["ind_start_good_L2"] = ind_start_good_L2
        signal_2_ind["ind_end_good_L2"]   = ind_end_good_L2
        signal_2_ind["ind_start_bad_L2"]  = ind_start_bad_L2
        signal_2_ind["ind_end_bad_L2"]    = ind_end_bad_L2

        return labels_q_L, signal_0_ind, signal_1_ind, signal_2_ind

    if len(signals) == 4:
        signal_2_ind = {}
        signal_2_ind["ind_start_good_L2"] = ind_start_good_L2
        signal_2_ind["ind_end_good_L2"]   = ind_end_good_L2
        signal_2_ind["ind_start_bad_L2"]  = ind_start_bad_L2
        signal_2_ind["ind_end_bad_L2"]    = ind_end_bad_L2

        signal_3_ind = {}
        signal_3_ind["ind_start_good_L3"] = ind_start_good_L3
        signal_3_ind["ind_end_good_L3"]   = ind_end_good_L3
        signal_3_ind["ind_start_bad_L3"]  = ind_start_bad_L3
        signal_3_ind["ind_end_bad_L3"]    = ind_end_bad_L3

        return labels_q_L, signal_0_ind, signal_1_ind, signal_2_ind, signal_3_ind

    if len(signals) == 5:
        signal_2_ind = {}
        signal_2_ind["ind_start_good_L2"] = ind_start_good_L2
        signal_2_ind["ind_end_good_L2"]   = ind_end_good_L2
        signal_2_ind["ind_start_bad_L2"]  = ind_start_bad_L2
        signal_2_ind["ind_end_bad_L2"]    = ind_end_bad_L2

        signal_3_ind = {}
        signal_3_ind["ind_start_good_L3"] = ind_start_good_L3
        signal_3_ind["ind_end_good_L3"]   = ind_end_good_L3
        signal_3_ind["ind_start_bad_L3"]  = ind_start_bad_L3
        signal_3_ind["ind_end_bad_L3"]    = ind_end_bad_L3

        signal_4_ind = {}
        signal_4_ind["ind_start_good_L4"] = ind_start_good_L4
        signal_4_ind["ind_end_good_L4"]   = ind_end_good_L4
        signal_4_ind["ind_start_bad_L4"]  = ind_start_bad_L4
        signal_4_ind["ind_end_bad_L4"]    = ind_end_bad_L4

        return labels_q_L, signal_0_ind, signal_1_ind, signal_2_ind, signal_3_ind, signal_4_ind

    # return labels_q_L, signal_0_ind, signal_1_ind, signal_2_ind, signal_3_ind, signal_4_ind
    # *****************************************************************************************

#Definir Filtros

#%% Filtro Notch
from scipy.fftpack import fft
import scipy.signal
def notch_filter(senal_seg):
  bandas_de_rechazo = [[59.98,60.01],[89.98,90.01],[119,121]]
  filtros_rechaza_banda  = [scipy.signal.butter(3, bandas_de_rechazo[p], btype='bandstop', fs=250) for p in range(len(bandas_de_rechazo))]
  señal_filtrada = senal_seg  # Inicializa la señal filtrada con la señal original
  for b, a in filtros_rechaza_banda:
      señal_filtrada = scipy.signal.filtfilt(b, a, señal_filtrada)

  samplingFreq=250
  fftData = np.abs( fft(señal_filtrada) )
  fftLen = int(len(fftData) / 2)
  freqs = np.linspace(0,samplingFreq/2, fftLen )
  return señal_filtrada

#%% Savitzky-Golay filter

def golay_filter(ecgg,wavelet_type, nivel):

    ecg=ecgg
    index = []
    data = []
    for i in range (len(ecg)):
        X = float(i)
        Y = float(ecg[i])
        index.append(X)
        data.append(Y)
    w = pywt.Wavelet(wavelet_type)
   # maxlev = pywt.dwt_max_level(len(data), w.dec_len)  # Maximo nivel usado para la descomposición
    maxlev=nivel
    coeffs = pywt.wavedec(data, wavelet_type, level=maxlev) # Descomposición wavelet de la señal con los coeficientes
    #print(pywt.dwt_max_level(len(data)))


    for i in range(1, len(coeffs)):
      M = len(coeffs[i])
      lambda_val = math.sqrt(2*math.log(M)) # Umbral para filtrar, Metodo SureShrink
      coeffs[i] = pywt.threshold(coeffs[i], lambda_val, 'soft') # Filtar el ruido una desicion de filtro 'soft'
    datarec = pywt.waverec(coeffs, wavelet_type) # Reconstruccion wavelet de la señal
    #datarec = denoise_wavelet(ecgg, method='BayesShrink', mode='soft', wavelet_levels=3, wavelet='sym8', rescale_sigma='True')
    return datarec

#%%Filtro para eliminar la linea de base - mejor que con wavelet
def low_pass_filter(input_signal, cutoff):
    extra=int((250*(len(input_signal)/250))/15)
    wn = 2*cutoff/250 # Frecuencia de corte normalizad
    [a,b] = scipy.signal.butter(5,wn,'high');
    filtered_signal= scipy.signal.filtfilt(a,b,input_signal,padtype='even', padlen=extra, method='pad')
    return filtered_signal
#END FUNCTION

#%%Funcion para imputar outliers
def outliers(input_signal,n): # dataframe,tamaño de la muestra,frecuencia de muestreo
    media=np.mean(input_signal)
    des=np.std(input_signal) # desviacion estandar
    signal_size=len(input_signal)
    for i in range(signal_size):
        if input_signal[i]>(media+(n*des)):
          input_signal[i]=media
        #end if
        if input_signal[i]<(media-(n*des)): # -1.5 porque la grafica los picos s estan antes de este valor, los que este por encima es outlier
          input_signal[i]=media
        #end if
    #end for
    return input_signal
 #END FUNCTION


#%%Algortimo para detectar los picos
def detect_peaks(ecg_signal, threshold=0.3, qrs_filter=None):
    '''
    Peak detection algorithm using cross corrrelation and threshold
    '''
    if qrs_filter is None:
        # create default qrs filter, which is just a part of the sine function
        t = np.linspace(1.5 * np.pi, 3.5 * np.pi, 15)
        qrs_filter = np.sin(t)

    # normalize data
    ecg_signal = (ecg_signal - ecg_signal.mean()) / ecg_signal.std()

    # calculate cross correlation
    similarity = np.correlate(ecg_signal, qrs_filter, mode="same")
    similarity = similarity / np.max(similarity)
    ecg_signal=pd.DataFrame(ecg_signal)
    # return peaks (values in ms) using threshold
    return ecg_signal[similarity > threshold].index, similarity

#%% Algortimo para detectar los picos JD
def saavedra(signal):
  peaks_ind,similarity=detect_peaks(signal, 0.4)
  peaks_correc=R_correction(signal,peaks_ind)
  peaks_correct=[int(element) for element in peaks_correc]
  peaks_corrected=list(set(peaks_correc))
  peaks_corrected.sort()
  prom=np.mean(signal[peaks_corrected])
  desv=np.std(signal[peaks_corrected])
  umbral=prom - (desv/2)
  if len(peaks_corrected)>100 or desv>0.085:
    ind_delete= []
    for indice in peaks_corrected:
      if signal[indice] <= umbral:
        ind_delete.append(indice)
      #end if
    #end for
    peaks_def = [x for x in peaks_corrected if x not in ind_delete]
  else:
    peaks_def=peaks_corrected
  #end if
  return peaks_def
#END FUNCTION


#%% Funcion para corregir los picos

def R_correction(signal, peaks):
    c=int(len(signal)/60) #Definir la cantidad de ceros que serán agregadas al inicio y final de la señal
    new_signal=np.concatenate((np.zeros(c), signal, np.zeros(c))) #Agregar ceros al inicio y final
    peaks=peaks+ c*np.ones(len(peaks)) #correr la cantidad de picos una cantidad la misma cantidad desplazada de ceros al inicio
    peaks=[int(element) for element in peaks]
    peaks=np.asarray(peaks)
    num_peak=peaks.shape[0]
    peaks_corrected_list=list()
    for index in range(num_peak):
        i=peaks[index]
        cnt=i
        if cnt-1<0:
            break
        #end if
        if new_signal[cnt]<new_signal[cnt-1]:
            while new_signal[cnt]<new_signal[cnt-1]:
                cnt-=1
                if cnt<0:
                    break
                #end if
            #end while
        elif new_signal[cnt]<new_signal[cnt+1]:
            while new_signal[cnt]<new_signal[cnt+1]:
                cnt+=1
                if cnt<0:
                    break
                #end if
            #end while
        #end if
        peaks_corrected_list.append(cnt)
    #end for
    peaks_corrected=np.asarray(peaks_corrected_list)
    peaks_corrected= [x - c for x in peaks_corrected_list]
    peaks_corrected=[int(element) for element in peaks_corrected]
    return peaks_corrected
#end for


#%%Eliminar otras recuencias y dejar solo el complejo QRS
def bandpass(seg_signal):
  b, a = signal.butter(3, [5,20], btype='bandpass', fs=250)
  y = signal.filtfilt(b, a,seg_signal, padtype='even', padlen=1000, method='pad')
  return y

#%%Detectar el mejor algoritmo
def multiples_algs(ecg_segment):
    Fs = 250 # Sampling frequency
    r_pk_algs = ['saavedra','christov_detector','engzee_detector', 'wqrs_detector']
    detectors = Detectors(Fs)
    results = {}
    a,b=signal.butter(3, [5,20], btype='bandpass', fs=Fs)
    e=1000 #Definir la cantidad de elementos a agregar al inicio y al final
    ultimos_200 = ecg_segment[-e:]
    primeros_200 = ecg_segment[:e]
    senal_add = np.concatenate((primeros_200,ecg_segment,ultimos_200 ))
    #ecg_segment=signal.filtfilt(d,c,ecg_segment)
    for idx, r_pk_algs in enumerate(r_pk_algs):
      if r_pk_algs == 'saavedra':
        #ecg_segment_f=signal.filtfilt(a,b,ecg_segment)
        corrected_r_peaks = saavedra(senal_add)
      else:
        #ecg_segment=signal_detrend(ecg_segment, 'bior3.7', 3)
        ecg_segment_f=signal.filtfilt(a,b,senal_add)
        r_peaks = getattr(detectors, r_pk_algs)(ecg_segment_f)
        corrected_r_peaks = R_correction(ecg_segment_f, r_peaks)
      #end if
      if corrected_r_peaks[len(corrected_r_peaks)-1]==len(senal_add):
        corrected_r_peaks.pop()
      #end if
      peaks_def_f = [indice for indice in corrected_r_peaks if not (0 <= indice <= e or (len(senal_add) - e) <= indice <= len(senal_add))]
      peaks_def_f = [numero - e for numero in peaks_def_f]
      if len(peaks_def_f)==0:
        results[r_pk_algs] = {
            "corrected_r_peaks": [0],
            "mean_r_peak": 0
        }
      else:
        mean_r_peak = np.mean(ecg_segment[peaks_def_f[0:len(peaks_def_f)-1]])
        results[r_pk_algs] = {
            "corrected_r_peaks": peaks_def_f,
            "mean_r_peak": mean_r_peak
        }
    #end for
    best_algorithm = max(results, key=lambda x: results[x]["mean_r_peak"])
    r_pks = results[best_algorithm]["corrected_r_peaks"]
    r_pks=[int(element) for element in r_pks]
    r_pks = correccion_picuda(ecg_segment, r_pks)
    r_pks_ms = [(element / Fs) * 1000 for element in r_pks]
    return best_algorithm,r_pks,r_pks_ms


#end function

#%%
def detect_bestAlgorithm(ecg_segment):
    Fs = 250 # Sampling frequency
    r_pk_algs = ['saavedra','christov_detector','engzee_detector','wqrs_detector']
    detectors = Detectors(Fs)
    results = {}
    segment_length = len(ecg_segment)
    padding_length = 2 * 250
    ecg_segment_with_padding = np.concatenate((ecg_segment[:padding_length], ecg_segment, ecg_segment[-padding_length:]))
    for idx, r_pk_algs in enumerate(r_pk_algs):
      if r_pk_algs == 'saavedra':
        corrected_r_peaks = saavedra(ecg_segment_with_padding)
      else:
        r_peaks = getattr(detectors, r_pk_algs)(ecg_segment_with_padding)
        corrected_r_peaks = R_correction(ecg_segment_with_padding, r_peaks)
      #end if
      mean_r_peak = np.mean(ecg_segment_with_padding[corrected_r_peaks[0:len(corrected_r_peaks)-1]])
      results[r_pk_algs] = {
          "corrected_r_peaks": corrected_r_peaks,
           "mean_r_peak": mean_r_peak
      }
    #end for

    best_algorithm = max(results, key=lambda x: results[x]["mean_r_peak"])
    r_pks = results[best_algorithm]["corrected_r_peaks"]
    r_pks=[int(element) for element in r_pks]

    peaks_def_f = [indice for indice in r_pks if not (0 <= indice <= padding_length or (len(ecg_segment_with_padding) - padding_length) <= indice <= len(ecg_segment_with_padding))]
    peaks_def_f = [numero - padding_length for numero in peaks_def_f]
    r_pks_c = correccion_picuda(ecg_segment, peaks_def_f)
    r_pks_ms = [(element / Fs) * 1000 for element in r_pks_c]

    return best_algorithm,r_pks_c,r_pks_ms
#%% Corregir picos que se detectan un poquito abajo
import wfdb.processing
def correccion_picuda(ecg_segment, r_pks):
    peaks_corr_0 = wfdb.processing.correct_peaks(ecg_segment, r_pks, search_radius=5, smooth_window_size=50, peak_dir="up")
    peaks_corr_0.sort()
    indices_a_eliminar = [indice for indice, valor in enumerate(ecg_segment) if valor <= 0]
    peaks_corr_1 = [x for x in peaks_corr_0 if x not in indices_a_eliminar]
    return peaks_corr_1
#end function



#%% funcion de  Detectar outliers de NATA


#Codigo nuevo
from sklearn.tree import DecisionTreeRegressor

def spline_cubic(RR):
  indices_nan = np.where(np.isnan(RR))[0]
  RR2 = RR.copy()
  RR_no_nan = RR[~np.isnan(RR2)]
  xRR_no_nan = np.arange(len(RR2))[~np.isnan(RR2)].reshape(-1, 1)
  dt = DecisionTreeRegressor().fit(xRR_no_nan, RR_no_nan)
  RR_interpolated_zeros = dt.predict(indices_nan.reshape(-1, 1))
  RR[indices_nan] = RR_interpolated_zeros
  return RR



def correction_outliers(peaks_corrected,n_picos,time,x):
  #peaks_corrected: Posiciones de los picos
  #Ventana de picos: se recomienda cada 45 picos
  #time: vector de tiempo del segmento ecg
  #x: es una constante por la cual se multiplican los umbrales, el valor que recomiendan en el artículo es 5.2
  #PONER X=5.2
  #los parámetros que son vectores los paso como arrays y NO como listas. La función devuelve un array con el vector RR arreglado.

  #CODIGO PARA RECORRER EL VECTOR DE PICOS POR VENTANAS
  sizee=n_picos
  list_peaks_good=[]
  wind_numero=(len(peaks_corrected))/sizee  #cantidad de ventadas de 30 picos en la señal de picos
  if wind_numero%1 !=0:
    window=int(wind_numero)  # para la antepenultima ventana
    a=1
  else:
    window=int(wind_numero)
    a=0
  for i in range(0,window,1):
    segm_picos=peaks_corrected[(sizee*i):((sizee*i))+(sizee+1)]
    if a==1:
      if window==1:
        c=0
      else:
        c=i
      if c==window-1:  #2
        segm_picos=peaks_corrected[(sizee*i):len(peaks_corrected)]

    # CODIGO PARA ETECTAR OUTLIERS CON TH, UN UMBRAL VARIABLE, SE BASA EN RANGO INTERCUARTIL
    rr_0=np.diff(time[segm_picos])

    picos_olvidados = detect_picos_olvidados(rr_0)
    if any(valor == 1 for valor in picos_olvidados):  #esto es necesario hacerlo aqui y alla  :D ?
      new_rr = retornar_picos_perdido(rr_0)  #Cuando hay picos sin detectar --> mejorar
      rr2 = extra_beats(new_rr)   #Detectar extrabeats
    else:
      rr2 = extra_beats(rr_0)   #Detectar extrabeats
    #rr2=np.diff(time[segm_picos])
    prom=np.mean(rr2)
    drrs=(np.diff(rr2))
    iqr_q1=np.percentile(drrs,25)
    iqr_q3=np.percentile(drrs,75)
    iqr=iqr_q3-iqr_q1
    th=iqr*x
    drr=abs(drrs/th)
    #print(drr)
    RR=rr2.copy()

    outlier=any(numero > 1 for numero in drr)
    if outlier==True:
      indices_1 = np.where(drr>1)[0]
      for i in indices_1:
        RR[i+1]='nan'
      r_interp1=spline_cubic(RR)
    else:
      r_interp1=rr2

    #SEGUNDA PASADA PARA DETECTAR LOS OUTLIERS QUE HAYAN QUEDADO, SE BASA EN LA MEDIANA, se crea un segundo umbral variable th2
    mediana=np.median(r_interp1)
    mrrs=r_interp1-mediana
    mediana_new=[]
    for i in mrrs:
      if i<0:
        mediana_new.append(2*i)
      elif i>=0:
        mediana_new.append(i)

    th2=x*(np.percentile(mediana_new,75)-np.percentile(mediana_new,25))
    mrr=abs(mrrs/th2)
    indices_mrr1= np.where(mrr>1)[0]

    outlier_2=any(numero > 1 for numero in mrr)
    if outlier_2==True:
      RRmr=r_interp1.copy()
      RRmr[indices_mrr1] = 'nan'
      r_interp2=spline_cubic(RRmr)
    else:
      r_interp2=r_interp1

    list_peaks_good.extend(r_interp2)  #uno los valores de las ventanas de picos de un mismo segmento
  list_peaks_good=np.asarray(list_peaks_good)
  return list_peaks_good


#%% Funcion cuando no se han detectado picos
def retornar_picos_perdido(rr_0):
    rr_median = np.median(rr_0)
    #rr_std = np.std(rr_0)

    indices_to_remove = []
    picos_olvidados= detect_picos_olvidados(rr_0)  # devuelve una lista de 0 y 1
    indices = [i for i in range(len(picos_olvidados)) if picos_olvidados[i] == 1] #busco donde hay 1
    for j in reversed(range(len(rr_0)-1)):
      for l in range(len(indices)):
          if j == indices[l]:
          # Calcula cuántas veces se debe dividir el valor para que sea similar a la mediana
            divide_factor = int(round(rr_0[j] / rr_median))
            # Agrega nuevas posiciones con valores divididos
            new_values = np.full(divide_factor, rr_0[j] / divide_factor)
            rr_0 = np.insert(rr_0, j+1, new_values)
            #print(len(rr_0))
            picos_olvidados= detect_picos_olvidados(rr_0)  # devuelve una lista de 0 y 1
            indices_to_remove = [i for i in range(len(picos_olvidados)) if picos_olvidados[i] == 1] #busco donde hay 1

    rr_0 = np.delete(rr_0, indices_to_remove)
    return rr_0
#%%cambio_porcentaje

def cambio_porcentaje_nn(tacograma):
    
    val_beats = [0]*len(tacograma)
    cambio_porcentual_ = []
    cambio_porcentual2_ = []
    cambio_porcentual3_ = []
    for i in range(0,len(tacograma)-1,1):

        cambio_porcentual3 = ((tacograma[i - 1] - tacograma[len(tacograma)-1]) / tacograma[len(tacograma)-1]) * 100
        cambio_porcentual3_.append(cambio_porcentual3)
        median = np.median(tacograma)
        desviacion_estandar = np.std(tacograma)
        # Calcular la diferencia entre cada valor y la media
        diferencias = tacograma - median
        #NPN
        if i !=0:
          cambio_porcentual = ((tacograma[i + 1] - tacograma[i]) / tacograma[i]) * 100
          cambio_porcentual2 = ((tacograma[i - 1] - tacograma[i]) / tacograma[i]) * 100
          cambio_porcentual_.append(cambio_porcentual)
          cambio_porcentual2_.append(cambio_porcentual2)
          if (abs(cambio_porcentual2) >= 2 and abs(cambio_porcentual) >= 2) and abs(diferencias[i]) >= 2.75*desviacion_estandar:
              val_beats[i] = 1
          elif (abs(cambio_porcentual) >= 2 or abs(cambio_porcentual2) >= 2)  and abs(diferencias[i]) >= 2.75*desviacion_estandar:
              val_beats[i] = 1
        if i ==0:
          cambio_porcentual0 = ((tacograma[i + 1] - tacograma[0]) / tacograma[0]) * 100
          if abs(cambio_porcentual0) >= 2 and abs(diferencias[0]) >= 2.75*desviacion_estandar:
              val_beats[0] = 1


    if abs(cambio_porcentual3_[-1]) >= 2:
        val_beats[len(tacograma)-1] = 1
    return np.array(val_beats)

#%%
# funcion de  interpolacion

def interpolacion_vbeats(valid_beats, nn):
    # Encontrar índices de ceros y unos en valid_beats
    ceros = [i for i, valor in enumerate(valid_beats) if valor == 0]
    unos = [i for i, valor in enumerate(valid_beats) if valor == 1]

    # Realizar interpolación cúbica en los valores marcados como ceros
    spline = CubicSpline(ceros, [nn[i] for i in ceros])
    valores_interpolados = spline(unos)

    # Copiar los datos originales a una nueva lista
    copia_señal = nn.copy()

    # Reemplazar los valores interpolados en la copia_señal
    for indice, valor_interpolado in zip(unos, valores_interpolados):
        copia_señal[indice] = valor_interpolado

    return copia_señal

#%%
def interpolacion_final(val,nn):
  valid_beats= val
  # Encontrar índices de ceros y unos en valid_beats
  ceros = [i for i, valor in enumerate(valid_beats) if valor == 0]

  padding_length = min(10, len(ceros))
  padding_values = valid_beats[ceros[:padding_length]]

  # Agregar datos al principio y final de valid_beats, para tener ceros en ambos lados
  valid_beats_new = np.concatenate((padding_values, valid_beats, padding_values))

  seg =nn
  padding_values_nn = seg[ceros[:10]]
  # Agregar datos al principio y final que no son outleirs
  nn_new = np.concatenate((padding_values_nn, nn, padding_values_nn))

  nn_inter = interpolacion_vbeats(valid_beats_new,nn_new)
  nn_inter = nn_inter[10:-10]
  return nn_inter



#%%
def inter_tacograma(nn, window):
    interpolador = interp1d(np.arange(len(nn)), nn, kind='cubic')
    #nuevo_tamano = 4*60
    nuevo_tamano = 4*window
    tacograma_remuestreado = interpolador(np.linspace(0, len(nn)-1, nuevo_tamano))
    return tacograma_remuestreado

#%%

def band_pass(seg_signal, l,h, fs):
    b, a = signal.butter(3, [l,h], btype='bandpass', fs=fs)
    y = signal.filtfilt(b, a,seg_signal, padlen=150)
    return y

#%%

def detect_picos_olvidados(rr_0):
    picos_olvidados = [0] * len(rr_0)
    median = np.median(rr_0)
    for i in range(len(rr_0) - 1):
        if rr_0[i] > 1.5 * median:
            picos_olvidados[i] = 1
    return picos_olvidados

def retornar_indice_perdido(rr_0):

    picos_olvidados= detect_picos_olvidados(rr_0)  # devuelve una lista de 0 y 1
    indices = [i for i in range(len(picos_olvidados)) if picos_olvidados[i] == 1] #busco donde hay 1
    #valores = [rr_0[j] for j in indices if 0 <= j < len(rr_0)]

    for j in range(len(rr_0)-1):
        for l in range(len(indices)):
            if j == indices[l]:
                new_beat = rr_0[j] / 2
                rr_0[j] = new_beat
                rr_new = np.insert(rr_0, j+1, new_beat)

    return rr_new



#%%
#Funcion para detectar extra beats
def extra_beats(rr_0):
    rr_val = [0]*len(rr_0)

    for i in range(len(rr_0)-1):
      if rr_0[i] <= (np.median(rr_0)/2):
         rr_val[i] = 1
    indices = [i for i in range(len(rr_val)) if rr_val[i] == 1]
    valores = [rr_0[j] for j in indices if 0 <= j < len(rr_0)]
    indices_a_eliminar = []
    for j in range(len(rr_0)-1):
        for l in range(len(indices)):
            if j == indices[l]:
                dif1 = abs(rr_0[j] - rr_0[j+1])
                dif2 = abs(rr_0[j] - rr_0[j-1])
                if dif1 < dif2:
                  new_beat = rr_0[j]+rr_0[j+1]
                  rr_0[j] = new_beat
                  indices_a_eliminar.append(j+1)
                if dif1 >= dif2:
                  new_beat = rr_0[j]+rr_0[j-1]
                  rr_0[j] = new_beat
                  indices_a_eliminar.append(j-1)


    rr_0_new = np.delete(rr_0, indices_a_eliminar)

    return rr_0_new

#%%Interpolacion bis
def interpolacion_bis(signal_bis):
    signal_bis  = np.nan_to_num(signal_bis)
    bis_val = [1]*len(signal_bis)
    for i in range(len(signal_bis)):
      if signal_bis [i] != 0:
          bis_val[i] = 0
    valid_data = bis_val
    bis = np.array(signal_bis)
    ceros = [i for i, valor in enumerate(valid_data) if valor == 0]
    
    padding_length = min(10, len(ceros))
    padding_values = [valid_data[int(i)] for i in ceros[:padding_length]]
    valid_data_new = np.concatenate((padding_values, valid_data, padding_values))

    seg = bis
    padding_values_bis = seg[ceros[:10]]
    bis_new = np.concatenate((padding_values_bis, bis, padding_values_bis))

    indices_one = np.where(valid_data_new == 1)[0]
    indices_zero = np.where(valid_data_new == 0)[0]
    
    bis_new_known = bis_new[valid_data_new == 0].reshape(-1, 1)

    dt = DecisionTreeRegressor().fit(indices_zero.reshape(-1, 1), bis_new_known)
    bis_interpolated_ones = dt.predict(indices_one.reshape(-1, 1))
    bis_new[indices_one] = bis_interpolated_ones
    bis_inter = bis_new[10:-10]
    return bis_inter


#%%
def timedom_hrv(intervalos_nn):
    time_domain_features = get_time_domain_features(intervalos_nn)

    hrv_mean_nni = time_domain_features["mean_nni"]
    hrv_sdnn = time_domain_features["sdnn"]
    hrv_sdsd = time_domain_features["sdsd"]
    hrv_nni_50 = time_domain_features["nni_50"]
    hrv_pnni_50 = time_domain_features["pnni_50"]
    hrv_nni_20 = time_domain_features["nni_20"]
    hrv_pnni_20 = time_domain_features["pnni_20"]
    hrv_rmssd = time_domain_features["rmssd"]
    hrv_median_nni = time_domain_features["median_nni"]
    hrv_range_nni = time_domain_features["range_nni"]
    hrv_cvsd = time_domain_features["cvsd"]
    hrv_cvnni = time_domain_features["cvnni"]
    hrv_mean_hr = time_domain_features["mean_hr"]
    hrv_max_hr = time_domain_features["max_hr"]
    hrv_min_hr = time_domain_features["min_hr"]
    hrv_std_hr = time_domain_features["std_hr"]

    return hrv_mean_nni ,  hrv_sdnn, hrv_sdsd,  hrv_nni_50, hrv_pnni_50, hrv_nni_20,hrv_pnni_20,  hrv_rmssd, hrv_range_nni, hrv_median_nni,hrv_cvsd,hrv_cvnni,hrv_mean_hr, hrv_max_hr,hrv_min_hr, hrv_std_hr

#%%
def time_domain(eeg_signal):
    eeg_signal = np.array(eeg_signal)
    eeg_max = np.max(eeg_signal)
    eeg_min = np.min(eeg_signal)
    eeg_med = np.mean(eeg_signal)
    eeg_median = np.median(eeg_signal)
    #eeg_medianl = stats.median_low(eeg_signal)
    #eeg_medianh = stats.median_high(eeg_signal)
    #eeg_medianl = np.percentile(eeg_signal, 50.0, interpolation='lower')  # Equivalent to low median
    #eeg_medianh = np.percentile(eeg_signal, 50.0, interpolation='higher')  # Equivalent to high median
    eeg_mod_ = stats.mode(eeg_signal)
    eeg_mod = float(eeg_mod_[0])
    eeg_var = np.var(eeg_signal)
    eeg_std = np.std(eeg_signal)
    eeg_curt = stats.kurtosis(eeg_signal)
    eeg_ske = stats.skew(eeg_signal)
    eeg_pow = np.mean(eeg_signal**2)
    eeg_tasa = librosa.zero_crossings(eeg_signal, pad=False)
    return eeg_max, eeg_min, eeg_med, eeg_median, eeg_mod, eeg_var, eeg_std, eeg_pow , sum(eeg_tasa), eeg_curt,eeg_ske

#%% Extraccion de caracteristicas dominio de la frecuecia HRV
def frequency_hrv(nn):
  
  frequency_domain_features = get_frequency_domain_features(nn_intervals=nn, method='welch',
                                                           sampling_frequency=4,
                                                           interpolation_method='linear',
                                                           vlf_band=(0.003, 0.04),
                                                           lf_band=(0.04, 0.15),
                                                           hf_band=(0.15, 0.4))
  hrv_lf = frequency_domain_features["lf"]
  hrv_hf = frequency_domain_features["hf"]
  hrv_lf_hf_ratio = frequency_domain_features["lf_hf_ratio"]
  hrv_lfnu = frequency_domain_features["lfnu"]
  hrv_hfnu = frequency_domain_features["hfnu"]
  hrv_total_power = frequency_domain_features["total_power"]
  hrv_vlf = frequency_domain_features["vlf"]

  # Geometrical feature
  geometrical_features = get_geometrical_features(nn_intervals=nn)
  hrv_tri_idx = geometrical_features["triangular_index"]


  poin =pyhrv.nonlinear.poincare(nn, show=False,figsize=None, legend=False)
  plt.close('all')
  #Area elipse

  # Non-linear features
  non_linear_features = get_csi_cvi_features(nn_intervals=nn)
  hrv_csi = non_linear_features["csi"]
  hrv_cvi = non_linear_features["cvi"]
  hrv_Modified_csi = non_linear_features["Modified_csi"]
  
  
  
  #hrv_sampen = sample_entropy["sampen"]
  return hrv_lf, hrv_hf, hrv_lf_hf_ratio, hrv_lfnu, hrv_hfnu, hrv_total_power, hrv_vlf,hrv_tri_idx,poin['sd1'], poin['sd2'] ,poin['sd_ratio'], hrv_csi, hrv_cvi,hrv_Modified_csi, poin['ellipse_area']


#%%
def signal_shannon_entropy(signal):
    # Count the occurrences of each value in the signal
    symbol_counts = Counter(signal)

    # Total number of values in the signal
    total_symbols = len(signal)

    # Calculate the probabilities of each values
    symbol_probabilities = [count / total_symbols for count in symbol_counts.values()]

    # Calculate the Shannon entropy using the probabilities
    entropy = -sum(p * np.log2(p) for p in symbol_probabilities if p > 0)

    return entropy

def caract_entropia(signal):
    entropy_shan = nk.entropy_shannon(signal)
    entropy_ren = nk.entropy_renyi(signal)
    entropy_ApEn  = nk.entropy_approximate(signal)
    entropy_Samp = nk.entropy_sample(signal)
    spect_ent_f = ant.spectral_entropy(signal, sf=250, method='fft', normalize=True)
    svd_ent = ant.svd_entropy(signal, normalize=True)
    #appr_ent = ant.app_entropy(signal)
    #samp_ent = ant.sample_entropy(signal)
    #shannon_entropy = signal_shannon_entropy(signal)
    #return entropy_shan[0], entropy_ren[0], entropy_ApEn[0], entropy_Samp[0], spect_ent_f, svd_ent, appr_ent, samp_ent, shannon_entropy
    return entropy_shan[0], entropy_ren[0], entropy_ApEn[0], entropy_Samp[0], spect_ent_f, svd_ent
#%%
def caoticas(sig):
    sig = sig.astype(np.float64)
    
    try:
        higu_10 = ant.higuchi_fd(sig, kmax=10)
    except:
        higu_10 = np.nan
    try:
        higu_50 = ant.higuchi_fd(sig, kmax=50)
    except:
        higu_50 = np.nan
    try:
        higu_100 = ant.higuchi_fd(sig, kmax=100)
    except:
        higu_100 = np.nan
    try:
        pfd, info2 = nk.fractal_petrosian(sig, symbolize="C", show=False)
    except:
        pfd = np.nan
    try:
        h, info3 = nk.fractal_hurst(sig, corrected=True, show=False)
    except:
        h = np.nan
    try:
        complexity, info  = nk.complexity_hjorth(sig)
    except:
        complexity = np.nan
    try:
        katz =  ant.katz_fd(sig)
    except:
        katz = np.nan
        
  
    return  pfd,h, complexity, info['Activity'],info['Mobility'], katz, higu_10, higu_50, higu_100

#%%
def caoticas2(sig):
    sig = sig.astype(np.float64)
    
    try:
        higu_10 = ant.higuchi_fd(sig, kmax=10)
    except:
        higu_10 = np.nan
    try:
        higu_50 = ant.higuchi_fd(sig, kmax=50)
    except:
        higu_50 = np.nan
    try:
        higu_100 = ant.higuchi_fd(sig, kmax=100)
    except:
        higu_100 = np.nan
    try:
        pfd, info2 = nk.fractal_petrosian(sig, symbolize="C", show=False)
    except:
        pfd = np.nan
    try:
        h, info3 = nk.fractal_hurst(sig, corrected=True, show=False)
    except:
        h = np.nan
    try:
        complexity, info  = nk.complexity_hjorth(sig)
    except:
        complexity = np.nan
    try:
        katz =  ant.katz_fd(sig)
    except:
        katz = np.nan
        
    try:
        dfa= pyhrv.nonlinear.dfa(sig)
        plt.close('all')
        dfa_results = utils.ReturnTuple(dfa)
    except:
        dfa_results = np.nan
    return  pfd,h, complexity, info['Activity'],info['Mobility'], katz, higu_10, higu_50, higu_100, dfa_results[1], dfa_results[2]
#%%
def bandpower(data, sf, band, window_sec=None):
    """Compute the average power of the signal x in a specific frequency band.

    Parameters
    ----------
    data : 1d-array
        Input signal in the time-domain.
    sf : float
        Sampling frequency of the data.
    band : list
        Lower and upper frequencies of the band of interest.
    window_sec : float
        Length of each window in seconds.
        If None, window_sec = (1 / min(band)) * 2
    relative : boolean
        If True, return the relative power (= divided by the total power of the signal).
        If False (default), return the absolute power.

    Return
    ------
    bp : float
        Absolute or relative band power.
    """

    band = np.asarray(band)
    low, high = band

    # Define window length
    if window_sec is not None:
        nperseg = window_sec * sf
    else:
        nperseg = (2 / low) * sf

    # Compute the modified periodogram (Welch)
    freqs, psd = welch(data, sf, nperseg=nperseg)

    # Frequency resolution
    freq_res = freqs[1] - freqs[0]

    # Find closest indices of band in frequency vector
    idx_band = np.logical_and(freqs >= low, freqs <= high)

    # Potencia delta absoluta:
    bp = simps(psd[idx_band], dx=freq_res)

    #Potencia relativa:
    bpr = bp/simps(psd, dx=freq_res)
    #porcentaje = bpr*100
    return bp, bpr, psd

#%%
def rms(signal_):
  cuadrados = np.square(signal_)
  media_cuadrados = np.mean(cuadrados)
  rms = np.sqrt(media_cuadrados)
  return rms

#%%
def freq_domain(signal):
    time_step = 1/250
    signal_ft = np.abs(np.fft.fft(signal))
    frec = np.fft.fftfreq(signal_ft.size, time_step)
    frec =frec[0:len(signal_ft)//2]
    idx = np.argsort(frec)
    
    frecs = frec[idx]
    trans = signal_ft[idx]
    
    #Calcular caracteristicas:
    # Valor Mínimo
    min_trans = np.min(trans)
    
    # Valor máximo
    max_trans = np.max(trans)
    
    # Media
    mean_trans = np.mean(trans)
    
    # Mediana
    median_trans = np.median(trans)
    
      
    # Moda
    mode_trans = scipy.stats.mode(trans)
    mode_trans = float(mode_trans[0])
    # Varianza
    variance_trans = np.var(trans)
    
    # Desviación Estándar
    std_trans = np.std(trans)
    
    # Curtosis
    kurtosis_trans = scipy.stats.kurtosis(trans)
    
    # Skewness
    ske_trans = scipy.stats.skew(trans)
    
    # Energía
    pow_trans = np.sum(np.square(trans))
    
    # xxx
     
    centroid = np.sum(frecs * trans) / np.sum(trans)
    dispersion = np.sqrt(np.sum(((frecs - centroid) ** 2) * trans) / np.sum(trans))
    flatness = np.exp(np.mean(np.log(trans))) / (np.mean(trans))
    slope = np.polyfit(frecs, trans, 1)[0]
    crest_factor = max_trans / np.mean(trans)
    return  max_trans, min_trans,mean_trans, median_trans, mode_trans, variance_trans, std_trans, pow_trans, frecs[np.argmin(trans)],kurtosis_trans, ske_trans, frecs[np.argmax(trans)],  np.mean(frecs),  frecs[np.argmin(np.abs(np.cumsum(trans) - np.sum(trans) / 2))], centroid, dispersion, flatness, slope, crest_factor
             
#%%

from scipy.stats import entropy

def mutual_information(x, y, bins):
    # Calcular histogramas conjuntos y marginales
    bins=114
    hist_xy, _, _ = np.histogram2d(x, y, bins=bins)
    hist_x = np.histogram(x, bins=bins)[0]
    hist_y = np.histogram(y, bins=bins)[0]

    # Calcular probabilidades conjuntas y marginales
    p_xy = hist_xy / np.sum(hist_xy)
    p_x = hist_x / np.sum(hist_x)
    p_y = hist_y / np.sum(hist_y)

    # Calcular la entropía de las señales y la conjunta
    h_x = entropy(p_x)
    h_y = entropy(p_y)
    h_xy = entropy(p_xy.flatten())

    # Calcular la información mutua
    mi = h_x + h_y - h_xy
    return  mi


#%%
from scipy.spatial.distance import cdist

def calculate_nonlinear_interdependence(X, Y, m=8, tau=2, k=10, T=50):
    # Reconstrucción de las series temporales en el espacio de estados
    def reconstruct_vectors(series):
        N = len(series)
        return np.array([series[i:(i + m * tau):tau] for i in range(N - (m - 1) * tau)])

    # Cálculo del radio cuadrado promedio de los vectores reconstruidos
    def average_squared_radius(reconstructed_vectors):
        return np.mean(np.sum(reconstructed_vectors ** 2, axis=1))

    # Cálculo de la distancia euclidiana cuadrada promedio condicionada a Y hacia sus k vecinos
    def mean_squared_distance_to_neighbours(X_reconstructed, Y_reconstructed):
        distances = cdist(Y_reconstructed, X_reconstructed, metric='sqeuclidean')
        sorted_distances = np.sort(distances, axis=1)[:, :k]
        return np.mean(sorted_distances, axis=1)

    # Reconstrucción de las series temporales en el espacio de estados
    X_reconstructed = reconstruct_vectors(X)
    Y_reconstructed = reconstruct_vectors(Y)

    # Cálculo de los radios cuadrados promedios
    Rn_X = average_squared_radius(X_reconstructed)
    Rn_Y = average_squared_radius(Y_reconstructed)

    # Cálculo de la distancia euclidiana cuadrada promedio condicionada a Y hacia sus k vecinos
    Rn_k_XY = mean_squared_distance_to_neighbours(X_reconstructed, Y_reconstructed)

    # Cálculo del índice N
    N = len(X)
    N_XY = (1/N) * sum((Rn_X -Rn_k_XY)/Rn_X)
    return N_XY